{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\uddea Hyperparameter Experimentation (SageMaker) \u2014 Manual vs Tuner\n", "\n", "**Purpose:** Provide a *stable*, **reproducible**, and **config-driven** notebook to explore hyperparameters for a churn model (or any binary classifier) in a way that\u2019s safe to promote to production.\n", "\n", "> Two approaches:\n", ">\n", "> 1) **Manual Search** \u2014 launch individual SageMaker training jobs for deterministic control and to work around tuner constraints.\n", "> 2) **SageMaker Hyperparameter Tuning Job** \u2014 a dedicated HPO job with metric-based early stopping and parallel exploration.\n", "\n", "Both approaches assume you have a training script (e.g., `training/train_sagemaker.py` or `training/train_sagemaker_large.py`) that prints metrics with regex-friendly lines.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udce6 What You Get\n", "- **Config-first** setup for S3 paths, instance types, and metric regex\n", "- **Manual** launcher for many single training jobs in parallel (spot instances supported)\n", "- **Dedicated** Hyperparameter Tuning Job (HPO) via `HyperparameterTuner`\n", "- Recall-first **metrics extraction** (ROC-AUC, F1@target recall, churner recall/precision)\n", "- Optional **MLflow** logging\n", "- **Artifacts**: CSV of search results; best params printed\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddf0 Prerequisites\n", "- Python 3.9+\n", "- AWS credentials with SageMaker permissions (role, S3 access)\n", "- Packages: `sagemaker`, `boto3`, `pandas`, `numpy`, `pyyaml`, `scikit-learn` (for local utils), `mlflow` (optional)\n", "- A `config.yaml` in the project root (next to this notebook or parent) with:\n", "  ```yaml\n", "  data:\n", "    parquet_uri: s3://your-bucket/path/to/data/*.parquet\n", "  ```\n", "- Training scripts:\n", "  - `training/train_sagemaker.py` (for Tuner)\n", "  - `training/train_sagemaker_large.py` (for Manual jobs; can be the same if you prefer)\n", "\n", "```python\n", "# If running on a fresh environment, uncomment as needed\n", "# %pip install sagemaker boto3 pandas numpy pyyaml scikit-learn mlflow\n", "```\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u267b\ufe0f Reproducibility & Environment Capture\n", "- Fixed seeds\n", "- Unique run folder for artifacts\n", "- Package versions captured\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, sys, json, hashlib, platform, random\n", "from datetime import datetime\n", "import numpy as np\n", "import pandas as pd\n", "\n", "SEED = 42\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "\n", "RUN_TS = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n", "RUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n", "ARTIFACT_DIR = os.environ.get('ARTIFACT_DIR', f\"artifacts/hpo_{RUN_TS}_{RUN_ID}\")\n", "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n", "\n", "env_info = {\n", "    'python': sys.version,\n", "    'platform': platform.platform(),\n", "    'timestamp_utc': RUN_TS,\n", "    'seed': SEED,\n", "    'packages': { 'pandas': pd.__version__, 'numpy': np.__version__ },\n", "}\n", "with open(os.path.join(ARTIFACT_DIR, 'env_info.json'), 'w') as f:\n", "    json.dump(env_info, f, indent=2)\n", "env_info"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2699\ufe0f Configuration (Edit here)\n", "Single source of truth for inputs, outputs, and behavior. You can flip between manual and tuner flows.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from pathlib import Path\n", "import yaml\n", "\n", "CONFIG = {\n", "    'aws': {\n", "        'instance_type_manual': os.getenv('INSTANCE_TYPE_MANUAL', 'ml.m5.large'),\n", "        'instance_type_tuner': os.getenv('INSTANCE_TYPE_TUNER', 'ml.m5.xlarge'),\n", "        'use_spot': True,\n", "        'max_run_manual_sec': 2*60*60,   # 2 hours\n", "        'max_wait_manual_sec': 4*60*60,  # 4 hours\n", "        'max_run_tuner_sec': 60*60,      # 1 hour\n", "        'max_wait_tuner_sec': 2*60*60,   # 2 hours\n", "    },\n", "    'data': {\n", "        'parquet_uri': os.getenv('PARQUET_URI', 's3://your-bucket/path/to/data/*.parquet'),\n", "    },\n", "    'training': {\n", "        'entry_point_manual': 'train_sagemaker_large.py',\n", "        'entry_point_tuner': 'train_sagemaker.py',\n", "        'source_dir': 'training',\n", "        'requirements': 'requirements.txt',\n", "        'config_yaml': 'config.yaml',\n", "        'mlflow_mode': os.getenv('MLFLOW_MODE', 'disabled'),  # 'local' | 'sagemaker' | 'disabled'\n", "        'recall_target': float(os.getenv('RECALL_TARGET', '0.80')),\n", "        'chunk_size': int(os.getenv('CHUNK_SIZE', '50000')),\n", "        'sample_ratio': float(os.getenv('SAMPLE_RATIO', '1.0')),\n", "    },\n", "    'regex_metrics': [\n", "        { 'Name': 'roc_auc', 'Regex': r'Final ROC-AUC: ([0-9]+\\.[0-9]+)' },\n", "        { 'Name': 'f1_score', 'Regex': r'Final F1 @ target recall: ([0-9]+\\.[0-9]+)' },\n", "        { 'Name': 'churner_recall', 'Regex': r'Churner recall:\\s+([0-9]+\\.[0-9]+)' },\n", "        { 'Name': 'churner_precision', 'Regex': r'Churner precision:\\s+([0-9]+\\.[0-9]+)' },\n", "    ],\n", "    'manual_search': {\n", "        'strategy': 'focused',   # 'focused' | 'broad'\n", "        'max_parallel_jobs': 3,\n", "        'max_combinations': None,\n", "        # focused grid (edit as needed)\n", "        'grid_focused': {\n", "            'n_estimators': [2200, 2500, 3000],\n", "            'learning_rate': [0.08],\n", "            'depth': [6],\n", "            'l2_leaf_reg': [5]\n", "        },\n", "        # broad grid (edit as needed)\n", "        'grid_broad': {\n", "            'n_estimators': [1500, 2000, 2500, 3000, 3500],\n", "            'learning_rate': [0.05, 0.07, 0.09, 0.11, 0.13],\n", "            'depth': [4, 5, 6, 7, 8],\n", "            'l2_leaf_reg': [2.0, 4.0, 6.0, 8.0, 10.0]\n", "        }\n", "    },\n", "    'tuner': {\n", "        'max_jobs': 5,\n", "        'max_parallel_jobs': 2,\n", "        'objective_metric': 'roc_auc',  # Stable for HPO\n", "        'early_stopping_type': 'Auto',\n", "        'ranges': {\n", "            'n-estimators': ['IntegerParameter', 500, 3000],\n", "            'learning-rate': ['ContinuousParameter', 0.01, 0.2],\n", "            'depth': ['IntegerParameter', 4, 10],\n", "            'l2-leaf-reg': ['ContinuousParameter', 0.5, 10.0]\n", "        }\n", "    },\n", "    'output': {\n", "        'artifact_dir': ARTIFACT_DIR,\n", "        'results_csv': str(Path(ARTIFACT_DIR)/'manual_search_results.csv'),\n", "    },\n", "    'mlflow': {\n", "        'enabled': False,\n", "        'tracking_uri': os.getenv('MLFLOW_TRACKING_URI',''),\n", "        'experiment_name': 'manual-hyperparameter-search'\n", "    }\n", "}\n", "CONFIG"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udded Approach A \u2014 Manual Hyperparameter Search (many single jobs)\n", "Use when you want full control, need to work around tuner issues, or want to stage/batch jobs for very large data.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import boto3, sagemaker, itertools, time\n", "from sagemaker.pytorch import PyTorch\n", "from datetime import datetime\n", "from typing import List, Dict, Tuple\n", "\n", "class ManualHyperparameterSearch:\n", "    def __init__(self, config: dict):\n", "        self.cfg = config\n", "        self.sess = sagemaker.Session()\n", "        try:\n", "            self.role = sagemaker.get_execution_role()\n", "        except Exception:\n", "            # For local notebooks where get_execution_role isn't available, allow role via env\n", "            self.role = os.getenv('SAGEMAKER_ROLE_ARN', 'YOUR-ROLE-ARN')\n", "        self.region = boto3.Session().region_name\n", "        self.bucket = self.sess.default_bucket()\n", "\n", "        # Data channel\n", "        self.training_input = sagemaker.inputs.TrainingInput(\n", "            s3_data=self.cfg['data']['parquet_uri'],\n", "            content_type='application/x-parquet'\n", "        )\n", "\n", "        self.jobs = []\n", "        print('\ud83d\udd2c Manual Hyperparameter Search Initialized')\n", "        print('   Instance type:', self.cfg['aws']['instance_type_manual'])\n", "        print('   Sample ratio:', self.cfg['training']['sample_ratio'])\n", "        print('   Region:', self.region)\n", "\n", "    def _grid(self, strategy='focused'):\n", "        grid = self.cfg['manual_search']['grid_focused'] if strategy=='focused' else self.cfg['manual_search']['grid_broad']\n", "        names = list(grid.keys())\n", "        vals = list(grid.values())\n", "        combos = list(itertools.product(*vals))\n", "        combos = [dict(zip(names, c)) for c in combos]\n", "        return combos\n", "\n", "    def define_hyperparameter_grid(self, strategy='focused') -> List[Dict]:\n", "        combos = self._grid(strategy)\n", "        print(f\"\\n\ud83d\udcca Generated {len(combos)} combos | strategy: {strategy}\")\n", "        for i, p in enumerate(combos[:3]):\n", "            print(f\"   {i+1}: {p}\")\n", "        if len(combos)>3:\n", "            print(f\"   ... and {len(combos)-3} more\")\n", "        return combos\n", "\n", "    def _estimator(self, params: Dict, job_index: int) -> Tuple[PyTorch,str]:\n", "        ts = datetime.now().strftime('%Y%m%d-%H%M%S')\n", "        job_name = f\"manual-hpo-{job_index:02d}-{ts}\"\n", "        est = PyTorch(\n", "            entry_point=self.cfg['training']['entry_point_manual'],\n", "            source_dir=self.cfg['training']['source_dir'],\n", "            role=self.role,\n", "            instance_type=self.cfg['aws']['instance_type_manual'],\n", "            instance_count=1,\n", "            framework_version='2.0.0',\n", "            py_version='py310',\n", "            hyperparameters={\n", "                'mlflow-mode': self.cfg['training']['mlflow_mode'],\n", "                'sample-ratio': str(self.cfg['training']['sample_ratio']),\n", "                'chunk-size': str(self.cfg['training']['chunk_size']),\n", "                'config': self.cfg['training']['config_yaml'],\n", "                'n-estimators': str(params['n_estimators']),\n", "                'learning-rate': str(params['learning_rate']),\n", "                'depth': str(params['depth']),\n", "                'l2-leaf-reg': str(params['l2_leaf_reg'])\n", "            },\n", "            max_run=self.cfg['aws']['max_run_manual_sec'],\n", "            use_spot_instances=self.cfg['aws']['use_spot'],\n", "            max_wait=self.cfg['aws']['max_wait_manual_sec'],\n", "            base_job_name=f\"manual-hpo-{job_index:02d}\",\n", "            dependencies=[self.cfg['training']['config_yaml'], self.cfg['training']['requirements']],\n", "            metric_definitions=self.cfg['regex_metrics']\n", "        )\n", "        return est, job_name\n", "\n", "    def launch_search(self, param_combinations: List[Dict], max_parallel_jobs: int = 3):\n", "        running, completed, failed = [], [], []\n", "        total = len(param_combinations)\n", "        print(f\"\\n\ud83d\ude80 Manual search: total {total}, parallel {max_parallel_jobs}\")\n", "        for i, params in enumerate(param_combinations):\n", "            while len(running) >= max_parallel_jobs:\n", "                print('\u23f3 Waiting for capacity ...')\n", "                time.sleep(60)\n", "                running, completed, failed = self._check_status(running, completed, failed)\n", "            try:\n", "                est, job_name = self._estimator(params, i+1)\n", "                est.fit({'training': self.training_input}, wait=False)\n", "                info = { 'job_index': i+1, 'job_name': est.latest_training_job.name, 'estimator': est, 'parameters': params, 'status': 'InProgress', 'start_time': datetime.now() }\n", "                running.append(info); self.jobs.append(info)\n", "                print(f\"\u2705 Launched {info['job_name']} with {params}\")\n", "                time.sleep(10)\n", "            except Exception as e:\n", "                print('\u274c Launch failed:', e)\n", "                failed.append({'job_index': i+1, 'parameters': params, 'error': str(e)})\n", "        print('\\n\u23f3 Waiting for all jobs to finish ...')\n", "        while running:\n", "            time.sleep(120)\n", "            running, completed, failed = self._check_status(running, completed, failed)\n", "        print(f\"\\n\ud83c\udfaf Done. Completed: {len(completed)} | Failed: {len(failed)}\")\n", "        return completed, failed\n", "\n", "    def _check_status(self, running, completed, failed):\n", "        still = []\n", "        for j in running:\n", "            try:\n", "                st = j['estimator'].latest_training_job.describe()['TrainingJobStatus']\n", "                if st == 'Completed':\n", "                    j['status'] = 'Completed'; j['end_time'] = datetime.now(); completed.append(j)\n", "                    print('\u2705 Completed:', j['job_name'])\n", "                elif st in ['Failed','Stopped']:\n", "                    j['status'] = st; j['end_time'] = datetime.now(); failed.append(j)\n", "                    print(f\"\u274c {st}:\", j['job_name'])\n", "                else:\n", "                    still.append(j)\n", "            except Exception as e:\n", "                print('\u26a0\ufe0f  Status check error:', j.get('job_name'), e)\n", "                still.append(j)\n", "        if len(still) != len(running):\n", "            print(f\"\ud83d\udcca Update \u2014 completed: {len(completed)}, failed: {len(failed)}, running: {len(still)}\")\n", "        return still, completed, failed\n", "\n", "    def collect_results(self, completed_jobs: List[Dict]):\n", "        import pandas as pd\n", "        print(f\"\\n\ud83d\udcca Collecting results from {len(completed_jobs)} jobs ...\")\n", "        rows = []\n", "        for j in completed_jobs:\n", "            try:\n", "                desc = j['estimator'].latest_training_job.describe()\n", "                metrics = {m['MetricName']: m['Value'] for m in desc.get('FinalMetricDataList', [])}\n", "                rows.append({\n", "                    'job_name': j['job_name'],\n", "                    'job_index': j['job_index'],\n", "                    'training_time_minutes': (j['end_time'] - j['start_time']).total_seconds()/60,\n", "                    **j['parameters'],\n", "                    **metrics\n", "                })\n", "            except Exception as e:\n", "                print('\u26a0\ufe0f  Collect error for', j.get('job_name'), e)\n", "        df = pd.DataFrame(rows)\n", "        if not df.empty:\n", "            if 'roc_auc' in df.columns:\n", "                df = df.sort_values('roc_auc', ascending=False)\n", "            out = self.cfg['output']['results_csv']\n", "            Path(out).parent.mkdir(parents=True, exist_ok=True)\n", "            df.to_csv(out, index=False)\n", "            print('\ud83d\udcbe Saved to', out)\n", "        else:\n", "            print('\u26a0\ufe0f  No metrics collected.')\n", "        return df\n", "\n", "    def analyze_results(self, df: pd.DataFrame):\n", "        if df.empty:\n", "            print('\u274c No results to analyze')\n", "            return df\n", "        print('\\n\ud83d\udcc8 Analysis \u2014 top 5 by roc_auc')\n", "        keep = [c for c in ['job_index','roc_auc','f1_score','churner_recall','churner_precision','n_estimators','learning_rate','depth','l2_leaf_reg'] if c in df.columns]\n", "        print(df[keep].head(5).to_string(index=False))\n", "        if 'roc_auc' in df.columns:\n", "            print('\\nSummary:')\n", "            print('  best roc_auc:', df['roc_auc'].max())\n", "            print('  mean roc_auc:', df['roc_auc'].mean())\n", "            print('  std  roc_auc:', df['roc_auc'].std())\n", "        return df\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u25b6\ufe0f Run Manual Search\n", "Uncomment and execute to launch. **Note:** This creates multiple training jobs and incurs AWS cost.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# search = ManualHyperparameterSearch(CONFIG)\n", "# combos = search.define_hyperparameter_grid(strategy=CONFIG['manual_search']['strategy'])\n", "# if CONFIG['manual_search']['max_combinations']:\n", "#     combos = combos[:CONFIG['manual_search']['max_combinations']]\n", "# completed, failed = search.launch_search(combos, max_parallel_jobs=CONFIG['manual_search']['max_parallel_jobs'])\n", "# results_df = search.collect_results(completed)\n", "# search.analyze_results(results_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udd16 Approach B \u2014 SageMaker Hyperparameter Tuning Job (HPO)\n", "Create a dedicated tuning job that explores ranges and maximizes a chosen objective metric.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import boto3, sagemaker\n", "from sagemaker.pytorch import PyTorch\n", "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n", "\n", "def create_hyperparameter_tuning_job(config: dict):\n", "    sess = sagemaker.Session()\n", "    try:\n", "        role = sagemaker.get_execution_role()\n", "    except Exception:\n", "        role = os.getenv('SAGEMAKER_ROLE_ARN', 'YOUR-ROLE-ARN')\n", "    region = boto3.Session().region_name\n", "    print('SageMaker role:', role)\n", "    print('Region:', region)\n", "\n", "    # Optionally write a minimal requirements.txt the training script can use\n", "    req_txt = '''\\\n", "pandas>=1.5.0\n", "numpy>=1.24.0\n", "scikit-learn>=1.2.0\n", "catboost>=1.2.0\n", "mlflow>=2.8.0\n", "s3fs>=2023.1.0\n", "pyarrow>=10.0.0\n", "sqlalchemy>=2.0.0\n", "redshift-connector>=2.0.0\n", "sagemaker>=2.190.0\n", "boto3>=1.26.0\n", "awswrangler>=3.0.0\n", "pyyaml>=6.0\n", "'''\n", "    with open('requirements.txt','w') as f:\n", "        f.write(req_txt)\n", "\n", "    est = PyTorch(\n", "        entry_point=config['training']['entry_point_tuner'],\n", "        source_dir=config['training']['source_dir'],\n", "        role=role,\n", "        instance_type=config['aws']['instance_type_tuner'],\n", "        instance_count=1,\n", "        framework_version='2.0.0',\n", "        py_version='py310',\n", "        hyperparameters={ 'mlflow-mode': 'disabled', 'config': config['training']['config_yaml'] },\n", "        max_run=config['aws']['max_run_tuner_sec'],\n", "        use_spot_instances=True,\n", "        max_wait=config['aws']['max_wait_tuner_sec'],\n", "        dependencies=[config['training']['config_yaml'], 'requirements.txt'],\n", "        metric_definitions=config['regex_metrics']\n", "    )\n", "\n", "    # Build hyperparameter ranges from config\n", "    ranges_cfg = config['tuner']['ranges']\n", "    ranges = {}\n", "    for k, v in ranges_cfg.items():\n", "        kind, lo, hi = v\n", "        if kind == 'IntegerParameter':\n", "            ranges[k] = IntegerParameter(int(lo), int(hi))\n", "        else:\n", "            ranges[k] = ContinuousParameter(float(lo), float(hi))\n", "\n", "    tuner = HyperparameterTuner(\n", "        estimator=est,\n", "        objective_metric_name=config['tuner']['objective_metric'],\n", "        hyperparameter_ranges=ranges,\n", "        objective_type='Maximize',\n", "        max_jobs=config['tuner']['max_jobs'],\n", "        max_parallel_jobs=config['tuner']['max_parallel_jobs'],\n", "        base_tuning_job_name='churn-model-tuning',\n", "        early_stopping_type=config['tuner']['early_stopping_type']\n", "    )\n", "\n", "    print('Starting tuning job ...')\n", "    print('Objective metric:', config['tuner']['objective_metric'])\n", "    print('Max jobs:', config['tuner']['max_jobs'], '| Parallel:', config['tuner']['max_parallel_jobs'])\n", "\n", "    tuner.fit({\n", "        'training': sagemaker.inputs.TrainingInput(\n", "            s3_data=config['data']['parquet_uri'],\n", "            content_type='application/x-parquet'\n", "        )\n", "    })\n", "\n", "    print('\\nTuning job started:', tuner.latest_tuning_job.job_name)\n", "    return tuner\n", "\n", "def monitor_tuning_job(tuner: HyperparameterTuner):\n", "    import time\n", "    while True:\n", "        desc = tuner.describe()\n", "        status = desc['HyperParameterTuningJobStatus']\n", "        print('Status:', status)\n", "        if status in ['Completed','Failed','Stopped']:\n", "            break\n", "        try:\n", "            best = tuner.best_training_job()\n", "            print('Best so far:', best['TrainingJobName'], '| value:', best['FinalHyperParameterTuningJobObjectiveMetric']['Value'])\n", "        except Exception:\n", "            print('No completed jobs yet ...')\n", "        time.sleep(60)\n", "    print('Final status:', status)\n", "    if status=='Completed':\n", "        best = tuner.best_training_job()\n", "        print('\\nBest hyperparameters:')\n", "        for p,v in best['TunedHyperParameters'].items():\n", "            print(' ', p, ':', v)\n", "        print('\\nBest metric value:', best['FinalHyperParameterTuningJobObjectiveMetric']['Value'])\n", "        try:\n", "            print('\\nTop 5 jobs:')\n", "            print(tuner.analytics().dataframe().head())\n", "        except Exception as e:\n", "            print('Analytics unavailable:', e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u25b6\ufe0f Launch Tuning Job\n", "Uncomment and execute to start. You can later attach to a running job by name and monitor.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# tuner = create_hyperparameter_tuning_job(CONFIG)\n", "# monitor_tuning_job(tuner)\n", "\n", "# Or attach to an existing job by name:\n", "# from sagemaker.tuner import HyperparameterTuner\n", "# tuner = HyperparameterTuner.attach('churn-model-tuning-YYYY-MM-DD-HH-MM-SS-XYZ')\n", "# monitor_tuning_job(tuner)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udded Guidance \u2014 When to Use Which Approach?\n", "- **Manual Search**: you want explicit control of each job, different sample ratios/chunk sizes, or staged waves of experiments.\n", "- **Tuner (HPO)**: you want automatic exploration, early stopping, and a consolidated analytics view in SageMaker.\n", "\n", "**Metric of interest:** If your business goal is recall-first, ensure your training script reports the **recall at a target threshold** (and F1@target recall). You can still use ROC-AUC for tuner stability while analyzing recall externally.\n"]}]}