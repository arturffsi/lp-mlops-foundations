{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\uddea Student Lab: Hyperparameter Experiments (Manual vs Tuner)\n", "\n", "**Goal:** Reproduce a recall-first HPO workflow where **you choose the model and hyperparameters**. The notebook saves artifacts and can run on SageMaker or locally.\n", "\n", "> You\u2019ll complete the **TODO** slots marked with `# <- TODO \u270f\ufe0f` to plug your model and search spaces.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 What You\u2019ll Do\n", "1. Capture environment for reproducibility  \n", "2. Configure data location, instance types, training scripts, and **metrics regex**  \n", "3. **Choose your model** inside your training script (e.g., CatBoost, XGBoost, LightGBM, PyTorch, etc.)  \n", "4. Run **Manual Search** (many single jobs) or **Tuner HPO**  \n", "5. Collect & compare metrics (recall-first mindset)  \n", "6. Export results CSV and best params\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddf0 Prerequisites\n", "- AWS creds with SageMaker + S3 access\n", "- Packages: `sagemaker`, `boto3`, `pandas`, `numpy`, `pyyaml`, optionally `mlflow`\n", "- A `config.yaml` with your dataset pointer\n", "- A training script that prints metrics with regex-friendly lines (you choose the **model inside the script**)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u267b\ufe0f Reproducibility & Run Folder"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, sys, json, hashlib, platform, random\n", "from datetime import datetime\n", "import numpy as np\n", "import pandas as pd\n", "\n", "SEED = 42\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "\n", "RUN_TS = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n", "RUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n", "ARTIFACT_DIR = os.environ.get('ARTIFACT_DIR', f\"artifacts/hpo_lab_{RUN_TS}_{RUN_ID}\")\n", "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n", "\n", "env_info = {\n", "    'python': sys.version,\n", "    'platform': platform.platform(),\n", "    'timestamp_utc': RUN_TS,\n", "    'seed': SEED,\n", "    'packages': { 'pandas': pd.__version__, 'numpy': np.__version__ }\n", "}\n", "with open(os.path.join(ARTIFACT_DIR, 'env_info.json'), 'w') as f:\n", "    json.dump(env_info, f, indent=2)\n", "env_info"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2699\ufe0f CONFIG \u2014 \u270f\ufe0f Edit this cell"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from pathlib import Path\n", "import yaml\n", "\n", "CONFIG = {\n", "    'aws': {\n", "        'instance_type_manual': os.getenv('INSTANCE_TYPE_MANUAL', 'ml.m5.large'),   # <- TODO \u270f\ufe0f choose\n", "        'instance_type_tuner': os.getenv('INSTANCE_TYPE_TUNER', 'ml.m5.xlarge'),   # <- TODO \u270f\ufe0f choose\n", "        'use_spot': True,\n", "        'max_run_manual_sec': 2*60*60,\n", "        'max_wait_manual_sec': 4*60*60,\n", "        'max_run_tuner_sec': 60*60,\n", "        'max_wait_tuner_sec': 2*60*60,\n", "    },\n", "    'data': {\n", "        'parquet_uri': os.getenv('PARQUET_URI', 's3://your-bucket/path/*.parquet'),  # <- TODO \u270f\ufe0f your S3 path\n", "    },\n", "    'training': {\n", "        'entry_point_manual': 'train_sagemaker_large.py',  # <- TODO \u270f\ufe0f your script (you choose model *inside* it)\n", "        'entry_point_tuner': 'train_sagemaker.py',         # <- TODO \u270f\ufe0f your script (can be the same as manual)\n", "        'source_dir': 'training',                          # <- TODO \u270f\ufe0f folder with training code\n", "        'requirements': 'requirements.txt',                # <- TODO \u270f\ufe0f requirements for remote training\n", "        'config_yaml': 'config.yaml',                      # <- TODO \u270f\ufe0f config file passed to script\n", "        'mlflow_mode': os.getenv('MLFLOW_MODE', 'disabled'),\n", "        'recall_target': float(os.getenv('RECALL_TARGET', '0.80')),  # <- TODO \u270f\ufe0f if recall differs\n", "        'chunk_size': int(os.getenv('CHUNK_SIZE', '50000')),\n", "        'sample_ratio': float(os.getenv('SAMPLE_RATIO', '1.0')),\n", "    },\n", "    'regex_metrics': [  # <- TODO \u270f\ufe0f align with your script's printouts\n", "        { 'Name': 'roc_auc', 'Regex': r'Final ROC-AUC: ([0-9]+\\.[0-9]+)' },\n", "        { 'Name': 'f1_score', 'Regex': r'Final F1 @ target recall: ([0-9]+\\.[0-9]+)' },\n", "        { 'Name': 'churner_recall', 'Regex': r'Churner recall:\\s+([0-9]+\\.[0-9]+)' },\n", "        { 'Name': 'churner_precision', 'Regex': r'Churner precision:\\s+([0-9]+\\.[0-9]+)' },\n", "    ],\n", "    'manual_search': {\n", "        'strategy': 'focused',   # 'focused' | 'broad'  # <- TODO \u270f\ufe0f\n", "        'max_parallel_jobs': 3,  # <- TODO \u270f\ufe0f\n", "        'max_combinations': None,\n", "        # Define YOUR hyperparameter grid (keys must match your script\u2019s argparse/Hyperparameters)\n", "        'grid_focused': {        # <- TODO \u270f\ufe0f your focused grid\n", "            'n_estimators': [2200, 2500, 3000],\n", "            'learning_rate': [0.08],\n", "            'depth': [6],\n", "            'l2_leaf_reg': [5]\n", "        },\n", "        'grid_broad': {          # <- TODO \u270f\ufe0f your broad grid\n", "            'n_estimators': [1500, 2000, 2500, 3000, 3500],\n", "            'learning_rate': [0.05, 0.07, 0.09, 0.11, 0.13],\n", "            'depth': [4, 5, 6, 7, 8],\n", "            'l2_leaf_reg': [2.0, 4.0, 6.0, 8.0, 10.0]\n", "        }\n", "    },\n", "    'tuner': {\n", "        'max_jobs': 8,                # <- TODO \u270f\ufe0f\n", "        'max_parallel_jobs': 2,       # <- TODO \u270f\ufe0f\n", "        'objective_metric': 'roc_auc',# <- TODO \u270f\ufe0f consider stability\n", "        'early_stopping_type': 'Auto',\n", "        # Ranges MUST match the hyperparameter names your script accepts\n", "        'ranges': {                   # <- TODO \u270f\ufe0f your ranges\n", "            'n-estimators': ['IntegerParameter', 500, 3000],\n", "            'learning-rate': ['ContinuousParameter', 0.01, 0.2],\n", "            'depth': ['IntegerParameter', 4, 10],\n", "            'l2-leaf-reg': ['ContinuousParameter', 0.5, 10.0]\n", "        }\n", "    },\n", "    'output': {\n", "        'artifact_dir': ARTIFACT_DIR,\n", "        'results_csv': str(Path(ARTIFACT_DIR)/'manual_search_results.csv'),\n", "    }\n", "}\n", "CONFIG"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udded Approach A \u2014 Manual Hyperparameter Search (you choose hyperparams)\n", "The **keys** in the grid must match your training script hyperparameter names."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import boto3, sagemaker, itertools, time, os\n", "from sagemaker.pytorch import PyTorch  # <- TODO \u270f\ufe0f If you don't use PyTorch Estimator, import the right one\n", "from datetime import datetime\n", "from typing import List, Dict, Tuple\n", "\n", "class ManualHyperparameterSearch:\n", "    def __init__(self, config: dict):\n", "        self.cfg = config\n", "        self.sess = sagemaker.Session()\n", "        try:\n", "            self.role = sagemaker.get_execution_role()\n", "        except Exception:\n", "            self.role = os.getenv('SAGEMAKER_ROLE_ARN', 'YOUR-ROLE-ARN')  # <- TODO \u270f\ufe0f set for local\n", "        self.region = boto3.Session().region_name\n", "        self.training_input = sagemaker.inputs.TrainingInput(\n", "            s3_data=self.cfg['data']['parquet_uri'], content_type='application/x-parquet')\n", "        print('\ud83d\udd2c Manual HPO ready | role:', self.role, '| region:', self.region)\n", "\n", "    def _grid(self, strategy='focused'):\n", "        grid = self.cfg['manual_search']['grid_focused'] if strategy=='focused' else self.cfg['manual_search']['grid_broad']\n", "        names = list(grid.keys()); vals = list(grid.values())\n", "        return [dict(zip(names, c)) for c in itertools.product(*vals)]\n", "\n", "    def define_hyperparameter_grid(self, strategy='focused') -> List[Dict]:\n", "        combos = self._grid(strategy)\n", "        print(f\"\ud83d\udcca {len(combos)} combinations | strategy={strategy}\")\n", "        print('Examples:', combos[:3])\n", "        return combos\n", "\n", "    def _estimator(self, params: Dict, job_index: int):\n", "        ts = datetime.now().strftime('%Y%m%d-%H%M%S')\n", "        job_name = f\"student-manual-hpo-{job_index:02d}-{ts}\"\n", "        # NOTE: You can swap PyTorch for another Estimator if your script/framework differs\n", "        est = PyTorch(\n", "            entry_point=self.cfg['training']['entry_point_manual'],            # <- TODO \u270f\ufe0f your script\n", "            source_dir=self.cfg['training']['source_dir'],                     # <- TODO \u270f\ufe0f your folder\n", "            role=self.role,\n", "            instance_type=self.cfg['aws']['instance_type_manual'],\n", "            instance_count=1,\n", "            framework_version='2.0.0',    # <- TODO \u270f\ufe0f adapt if not PyTorch\n", "            py_version='py310',\n", "            hyperparameters={\n", "                'mlflow-mode': self.cfg['training']['mlflow_mode'],\n", "                'sample-ratio': str(self.cfg['training']['sample_ratio']),\n", "                'chunk-size': str(self.cfg['training']['chunk_size']),\n", "                'config': self.cfg['training']['config_yaml'],\n", "                # Map grid keys to your script arg names (edit if your names differ)\n", "                'n-estimators': str(params.get('n_estimators', '')),          # <- TODO \u270f\ufe0f map\n", "                'learning-rate': str(params.get('learning_rate', '')),        # <- TODO \u270f\ufe0f map\n", "                'depth': str(params.get('depth', '')),                        # <- TODO \u270f\ufe0f map\n", "                'l2-leaf-reg': str(params.get('l2_leaf_reg', ''))             # <- TODO \u270f\ufe0f map\n", "            },\n", "            max_run=self.cfg['aws']['max_run_manual_sec'],\n", "            use_spot_instances=self.cfg['aws']['use_spot'],\n", "            max_wait=self.cfg['aws']['max_wait_manual_sec'],\n", "            base_job_name=f\"student-manual-hpo-{job_index:02d}\",\n", "            dependencies=[self.cfg['training']['config_yaml'], self.cfg['training']['requirements']],\n", "            metric_definitions=self.cfg['regex_metrics']                       # <- TODO \u270f\ufe0f align regex with script prints\n", "        )\n", "        return est, job_name\n", "\n", "    def launch_search(self, param_combinations: List[Dict], max_parallel_jobs: int = 3):\n", "        running, completed, failed = [], [], []\n", "        for i, params in enumerate(param_combinations):\n", "            while len(running) >= max_parallel_jobs:\n", "                time.sleep(60)\n", "                running, completed, failed = self._check_status(running, completed, failed)\n", "            try:\n", "                est, job_name = self._estimator(params, i+1)\n", "                est.fit({'training': self.training_input}, wait=False)\n", "                info = { 'job_index': i+1, 'job_name': est.latest_training_job.name, 'estimator': est, 'parameters': params, 'status': 'InProgress', 'start_time': datetime.now() }\n", "                running.append(info)\n", "                print('\u2705 Launched', info['job_name'], params)\n", "                time.sleep(10)\n", "            except Exception as e:\n", "                print('\u274c Launch failed:', e)\n", "                failed.append({'job_index': i+1, 'parameters': params, 'error': str(e)})\n", "        print('\u23f3 Waiting for all jobs ...')\n", "        while running:\n", "            time.sleep(120)\n", "            running, completed, failed = self._check_status(running, completed, failed)\n", "        print('\ud83c\udfaf Done | completed:', len(completed), '| failed:', len(failed))\n", "        return completed, failed\n", "\n", "    def _check_status(self, running, completed, failed):\n", "        still = []\n", "        for j in running:\n", "            try:\n", "                st = j['estimator'].latest_training_job.describe()['TrainingJobStatus']\n", "                if st == 'Completed':\n", "                    j['status'] = 'Completed'; j['end_time'] = datetime.now(); completed.append(j)\n", "                    print('\u2705 Completed:', j['job_name'])\n", "                elif st in ['Failed','Stopped']:\n", "                    j['status'] = st; j['end_time'] = datetime.now(); failed.append(j)\n", "                    print(f\"\u274c {st}:\", j['job_name'])\n", "                else:\n", "                    still.append(j)\n", "            except Exception as e:\n", "                print('\u26a0\ufe0f  Status error:', j.get('job_name'), e)\n", "                still.append(j)\n", "        if len(still) != len(running):\n", "            print(f\"\ud83d\udcca Update | completed: {len(completed)} | failed: {len(failed)} | running: {len(still)}\")\n", "        return still, completed, failed\n", "\n", "    def collect_results(self, completed_jobs: List[Dict]):\n", "        import pandas as pd\n", "        rows = []\n", "        for j in completed_jobs:\n", "            try:\n", "                desc = j['estimator'].latest_training_job.describe()\n", "                m = {r['MetricName']: r['Value'] for r in desc.get('FinalMetricDataList', [])}\n", "                rows.append({ 'job_name': j['job_name'], 'job_index': j['job_index'], 'training_time_minutes': (j['end_time']-j['start_time']).total_seconds()/60, **j['parameters'], **m })\n", "            except Exception as e:\n", "                print('\u26a0\ufe0f  Collect error:', e)\n", "        import pandas as pd\n", "        df = pd.DataFrame(rows)\n", "        if not df.empty:\n", "            if 'roc_auc' in df.columns:\n", "                df = df.sort_values('roc_auc', ascending=False)\n", "            Path(self.cfg['output']['results_csv']).parent.mkdir(parents=True, exist_ok=True)\n", "            df.to_csv(self.cfg['output']['results_csv'], index=False)\n", "            print('\ud83d\udcbe Saved:', self.cfg['output']['results_csv'])\n", "        else:\n", "            print('\u26a0\ufe0f  No metrics collected.')\n", "        return df\n", "\n", "    def analyze_results(self, df):\n", "        if df.empty:\n", "            print('\u274c No results to analyze'); return df\n", "        keep = [c for c in ['job_index','roc_auc','f1_score','churner_recall','churner_precision','training_time_minutes'] + list(CONFIG['manual_search']['grid_focused'].keys()) if c in df.columns]\n", "        print('\\n\ud83c\udfc5 Top results:')\n", "        print(df[keep].head(10).to_string(index=False))\n", "        return df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u25b6\ufe0f Run Manual Search\n", "Uncomment and execute. **Costs real money** on AWS."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# search = ManualHyperparameterSearch(CONFIG)\n", "# combos = search.define_hyperparameter_grid(strategy=CONFIG['manual_search']['strategy'])\n", "# if CONFIG['manual_search']['max_combinations']: combos = combos[:CONFIG['manual_search']['max_combinations']]\n", "# completed, failed = search.launch_search(combos, max_parallel_jobs=CONFIG['manual_search']['max_parallel_jobs'])\n", "# results_df = search.collect_results(completed)\n", "# search.analyze_results(results_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udd16 Approach B \u2014 SageMaker Hyperparameter Tuning Job (HPO)\n", "Define **ranges** matching your script\u2019s hyperparameter names. The tuner will maximize your chosen objective."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import boto3, sagemaker, os\n", "from sagemaker.pytorch import PyTorch  # <- TODO \u270f\ufe0f swap Estimator class if not using PyTorch\n", "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n", "\n", "def create_hyperparameter_tuning_job(config: dict):\n", "    sess = sagemaker.Session()\n", "    try:\n", "        role = sagemaker.get_execution_role()\n", "    except Exception:\n", "        role = os.getenv('SAGEMAKER_ROLE_ARN', 'YOUR-ROLE-ARN')  # <- TODO \u270f\ufe0f for local\n", "    region = boto3.Session().region_name\n", "    print('SageMaker role:', role); print('Region:', region)\n", "\n", "    # Optional: emit requirements for remote installs used by your training script\n", "    with open('requirements.txt','w') as f:\n", "        f.write('pandas>=1.5.0\\nnumpy>=1.24.0\\nscikit-learn>=1.2.0\\ncatboost>=1.2.0\\nmlflow>=2.8.0\\ns3fs>=2023.1.0\\npyarrow>=10.0.0\\nsqlalchemy>=2.0.0\\nredshift-connector>=2.0.0\\nsagemaker>=2.190.0\\nboto3>=1.26.0\\nawswrangler>=3.0.0\\npyyaml>=6.0\\n')\n", "\n", "    est = PyTorch(\n", "        entry_point=config['training']['entry_point_tuner'],     # <- TODO \u270f\ufe0f script where YOU choose the model\n", "        source_dir=config['training']['source_dir'],\n", "        role=role,\n", "        instance_type=config['aws']['instance_type_tuner'],\n", "        instance_count=1,\n", "        framework_version='2.0.0',  # <- TODO \u270f\ufe0f adapt if not PyTorch\n", "        py_version='py310',\n", "        hyperparameters={'mlflow-mode': 'disabled', 'config': config['training']['config_yaml']},\n", "        max_run=config['aws']['max_run_tuner_sec'],\n", "        use_spot_instances=True,\n", "        max_wait=config['aws']['max_wait_tuner_sec'],\n", "        dependencies=[config['training']['config_yaml'], 'requirements.txt'],\n", "        metric_definitions=config['regex_metrics']               # <- TODO \u270f\ufe0f must match your script prints\n", "    )\n", "\n", "    # Build ranges from CONFIG (keys must match your script's hypargs)\n", "    rng_cfg = config['tuner']['ranges']\n", "    ranges = {}\n", "    for k, v in rng_cfg.items():\n", "        kind, lo, hi = v\n", "        if kind == 'IntegerParameter':\n", "            ranges[k] = IntegerParameter(int(lo), int(hi))\n", "        else:\n", "            ranges[k] = ContinuousParameter(float(lo), float(hi))\n", "\n", "    tuner = HyperparameterTuner(\n", "        estimator=est,\n", "        objective_metric_name=config['tuner']['objective_metric'],  # <- TODO \u270f\ufe0f choose objective\n", "        hyperparameter_ranges=ranges,\n", "        objective_type='Maximize',\n", "        max_jobs=config['tuner']['max_jobs'],\n", "        max_parallel_jobs=config['tuner']['max_parallel_jobs'],\n", "        base_tuning_job_name='student-hpo-tuning',\n", "        early_stopping_type=config['tuner']['early_stopping_type']\n", "    )\n", "\n", "    print('Starting tuning job ...')\n", "    tuner.fit({\n", "        'training': sagemaker.inputs.TrainingInput(\n", "            s3_data=config['data']['parquet_uri'], content_type='application/x-parquet')\n", "    })\n", "    print('Tuning job started:', tuner.latest_tuning_job.job_name)\n", "    return tuner\n", "\n", "def monitor_tuning_job(tuner: HyperparameterTuner):\n", "    import time\n", "    while True:\n", "        desc = tuner.describe(); status = desc['HyperParameterTuningJobStatus']\n", "        print('Status:', status)\n", "        if status in ['Completed','Failed','Stopped']:\n", "            break\n", "        try:\n", "            best = tuner.best_training_job()\n", "            print('Best so far:', best['TrainingJobName'], '| value:', best['FinalHyperParameterTuningJobObjectiveMetric']['Value'])\n", "        except Exception:\n", "            print('No completed jobs yet ...')\n", "        time.sleep(60)\n", "    print('Final status:', status)\n", "    if status=='Completed':\n", "        best = tuner.best_training_job()\n", "        print('\\nBest hyperparameters:')\n", "        for p,v in best['TunedHyperParameters'].items():\n", "            print(' ', p, ':', v)\n", "        print('\\nBest objective value:', best['FinalHyperParameterTuningJobObjectiveMetric']['Value'])\n", "        try:\n", "            print('\\nTop jobs:')\n", "            print(tuner.analytics().dataframe().head())\n", "        except Exception as e:\n", "            print('Analytics unavailable:', e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u25b6\ufe0f Launch Tuning Job or Attach"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# tuner = create_hyperparameter_tuning_job(CONFIG)   # <- TODO \u270f\ufe0f run when ready\n", "# monitor_tuning_job(tuner)\n", "\n", "# from sagemaker.tuner import HyperparameterTuner\n", "# tuner = HyperparameterTuner.attach('student-hpo-tuning-YYYY-MM-DD-HH-MM-SS-XYZ')  # <- TODO \u270f\ufe0f job name\n", "# monitor_tuning_job(tuner)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udde0 Tips\n", "- Put **model choice** inside your training script; here we only pass hyperparameters and collect metrics.\n", "- For **recall-first** selection, make sure your script prints recall (and F1 @ target recall). Use ROC-AUC as stable tuner objective, then review recall in your results.\n", "- Ensure the names in `manual_search.grid_*` **match** the arg names your script expects (and the tuner ranges too)."]}]}