{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Model Selection (Choose Your Data & Models)\n",
    "\n",
    "**Goal:** Build a *reproducible*, **config‚Äëdriven** model selection pipeline. You will choose the **data source**, define the **target**, and select **models** to compare. The notebook will pick a winner by **Recall at target** (default 0.80), using PR‚ÄëAUC and ROC‚ÄëAUC as tie‚Äëbreakers.\n",
    "\n",
    "> This template runs on **SageMaker** or locally. You can load data from **CSV/Parquet (local/S3)**, **Postgres**, **Redshift**, or a custom `data_io.load_data()` you provide. The code is defensive: if a library/model isn't installed, that model is skipped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ What You‚Äôll Do\n",
    "1) Set up reproducibility and a run folder for artifacts  \n",
    "2) Choose a data source (CSV/Parquet/S3, Postgres, Redshift, or `data_io.py`)  \n",
    "3) Define target column and (optional) time column  \n",
    "4) Perform **deterministic** train/valid split (time‚Äëbased preferred)  \n",
    "5) Train a **model zoo** you choose (e.g., CatBoost, LightGBM, XGBoost, Logistic Regression)  \n",
    "6) Tune thresholds for **metric target** and compare models  \n",
    "7) Export artifacts: `metrics.json`, `thresholds.json`, and the winning model  \n",
    "8) (Optional) Log into **MLflow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Student Checklist\n",
    "- [ ] Pick a data source and update the **CONFIG** cell\n",
    "- [ ] Set the **target column** (classification: 0/1)\n",
    "- [ ] (Optional) Set a **time column** for a robust time‚Äëbased split\n",
    "- [ ] Choose which **models** to enable in `CONFIG['models']['enabled']`\n",
    "- [ ] (Optional) Add your own model to the registry block\n",
    "- [ ] Run the notebook end‚Äëto‚Äëend and inspect metrics\n",
    "- [ ] Explain which model won and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß∞ Prerequisites\n",
    "- Python 3.9+\n",
    "- Packages: `pandas`, `numpy`, `pyarrow`, `scikit-learn`, `sqlalchemy`, `redshift_connector`, `s3fs`,\n",
    "  and optionally `catboost`, `lightgbm`, `xgboost`, `mlflow`\n",
    "\n",
    "```python\n",
    "# If running on a fresh environment (uncomment as needed):\n",
    "# %pip install pandas numpy pyarrow scikit-learn sqlalchemy redshift_connector s3fs\n",
    "# %pip install catboost lightgbm xgboost mlflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôªÔ∏è Reproducibility & Run Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, hashlib, platform, random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n",
    "ARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/run_{RUN_TS}_{RUN_ID}\")\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "env_info = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"timestamp_utc\": RUN_TS,\n",
    "    \"seed\": SEED,\n",
    "    \"packages\": {\"pandas\": pd.__version__, \"numpy\": np.__version__},\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"env_info.json\"), \"w\") as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "env_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è CONFIG ‚Äî ‚úèÔ∏è Edit this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        # Choose one: 'csv' | 'parquet' | 's3-parquet' | 'postgres' | 'redshift' | 'data_io' | 'synthetic'\n",
    "        \"source\": os.environ.get(\"SOURCE\", \"synthetic\"),\n",
    "        # Local paths\n",
    "        \"csv_path\": \"./your_data.csv\",     # used if source == 'csv'\n",
    "        \"parquet_path\": \"./your_data.parquet\",  # used if source == 'parquet'\n",
    "        # S3 (requires s3fs creds)\n",
    "        \"s3_parquet_uri\": \"s3://bucket/path/to/*.parquet\",\n",
    "        # SQL query and connection details\n",
    "        \"sql\": \"SELECT * FROM public.final_feature_snapshot\",\n",
    "        \"pg\": {\"user\": os.getenv(\"PGUSER\",\"postgres\"), \"password\": os.getenv(\"PGPASSWORD\",\"postgres\"), \"host\": os.getenv(\"PGHOST\",\"localhost\"), \"port\": os.getenv(\"PGPORT\",\"5432\"), \"db\": os.getenv(\"PGDATABASE\",\"testdb\")},\n",
    "        \"redshift\": {\"host\": os.getenv(\"REDSHIFT_HOST\",\"example.redshift.amazonaws.com\"), \"database\": os.getenv(\"REDSHIFT_DB\",\"dev\"), \"user\": os.getenv(\"REDSHIFT_USER\",\"user\"), \"password\": os.getenv(\"REDSHIFT_PASSWORD\",\"password\"), \"port\": int(os.getenv(\"REDSHIFT_PORT\",\"5439\"))},\n",
    "        \"row_limit\": int(os.environ.get(\"ROW_LIMIT\",\"0\")) or None,\n",
    "    },\n",
    "    \"columns\": {\n",
    "        \"target\": \"churn\",                   # <- ‚úèÔ∏è set your binary target name (0/1)\n",
    "        \"primary_key\": \"codigocontaservico\",  # optional but recommended\n",
    "        \"time_col\": \"iddim_date_inicio\",      # optional, for time-based split\n",
    "        # known id/leakage cols (edit/remove as needed)\n",
    "        \"drop_cols\": [\n",
    "            \"idconsumo\",\"id_contaservico\",\"codigocontaservico\",\"idconta\",\"iddim_cliente\",\n",
    "            \"idcliente\",\"codigocliente\",\"iddim_conta\",\"codigoconta\",\n",
    "            \"idgrupo_dim_contadimensao\",\"iddim_contaservico_dth\",\"idcontaservico\"\n",
    "        ],\n",
    "    },\n",
    "    \"split\": {\n",
    "        \"test_size\": 0.2,\n",
    "        \"recall_target\": 0.80,  # <- ‚úèÔ∏è recall goal for selection\n",
    "        \"prefer_time_split\": True,\n",
    "    },\n",
    "    \"models\": {\n",
    "        # Enable/disable models here by name\n",
    "        \"enabled\": [\"catboost\", \"lightgbm\", \"xgboost\", \"logreg\"],\n",
    "        \"catboost\": {\"iterations\": 2000, \"early_stopping_rounds\": 200, \"depth\": 6, \"learning_rate\": 0.08, \"l2_leaf_reg\": 3.0, \"task_type\": os.environ.get(\"CAT_TASK_TYPE\",\"CPU\"), \"auto_class_weights\": \"Balanced\", \"verbose\": 200},\n",
    "        \"lightgbm\": {\"n_estimators\": 1500, \"learning_rate\": 0.05, \"num_leaves\": 64, \"min_child_samples\": 40, \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0, \"random_state\": 42},\n",
    "        \"xgboost\":  {\"n_estimators\": 1500, \"learning_rate\": 0.05, \"max_depth\": 6, \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0, \"random_state\": 42, \"eval_metric\": \"auc\"},\n",
    "        \"logreg\":   {\"max_iter\": 1000, \"class_weight\": \"balanced\", \"solver\": \"liblinear\", \"random_state\": 42},\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"artifact_dir\": ARTIFACT_DIR,\n",
    "        \"metrics_path\": str(Path(ARTIFACT_DIR)/\"metrics.json\"),\n",
    "        \"thresholds_path\": str(Path(ARTIFACT_DIR)/\"thresholds.json\"),\n",
    "        \"model_dir\": str(Path(ARTIFACT_DIR)/\"models\"),\n",
    "    },\n",
    "    \"mlflow\": {\"enabled\": False, \"tracking_uri\": os.getenv(\"MLFLOW_TRACKING_URI\",\"\"), \"experiment_name\": \"student-model-selection\"}\n",
    "}\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = None\n",
    "try:\n",
    "    from data_io import load_data  # optional, if you provide one\n",
    "except Exception as e:\n",
    "    print(\"‚ÑπÔ∏è No data_io.load_data found (that's fine):\", repr(e))\n",
    "\n",
    "def _demo_dataset(n=12000, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"codigocontaservico\": np.arange(1, n+1),\n",
    "        \"iddim_date_inicio\": pd.to_datetime(\"2023-01-01\") + pd.to_timedelta(rng.integers(0, 650, size=n), unit=\"D\"),\n",
    "        \"tenure_days\": rng.integers(30, 800, size=n),\n",
    "        \"expiry_month\": rng.integers(1, 13, size=n),\n",
    "        \"expiry_dow\": rng.integers(0, 7, size=n),\n",
    "        \"tipo_produto_atual\": rng.choice([\"normal\",\"premium\"], size=n, p=[0.85,0.15]),\n",
    "        \"topup_total_value\": rng.gamma(2.0, 30.0, size=n).round(2),\n",
    "        \"municipio\": rng.choice([\"luanda\",\"lubango\",\"viana\"], size=n),\n",
    "        \"past_churns\": rng.poisson(0.2, size=n),\n",
    "        \"n_prev_contracts\": rng.integers(0, 5, size=n),\n",
    "    })\n",
    "    logits = -1.1 + 0.0025*df[\"tenure_days\"] + 0.5*(df[\"tipo_produto_atual\"]==\"premium\").astype(int) - 0.0007*df[\"topup_total_value\"] + 0.35*df[\"past_churns\"]\n",
    "    p = 1/(1+np.exp(-logits))\n",
    "    df[\"churn\"] = (rng.random(size=n) < p).astype(int)\n",
    "    return df\n",
    "\n",
    "source = CONFIG['data']['source']\n",
    "df_raw = None\n",
    "\n",
    "try:\n",
    "    if source == 'data_io' and load_data is not None:\n",
    "        df_raw = load_data(source='parquet', uri=CONFIG['data'].get('parquet_path') or CONFIG['data'].get('s3_parquet_uri'), sql=CONFIG['data']['sql'], redshift_kwargs=CONFIG['data']['redshift'])\n",
    "    elif source == 'csv':\n",
    "        df_raw = pd.read_csv(CONFIG['data']['csv_path'])\n",
    "    elif source == 'parquet':\n",
    "        df_raw = pd.read_parquet(CONFIG['data']['parquet_path'])\n",
    "    elif source == 's3-parquet':\n",
    "        df_raw = pd.read_parquet(CONFIG['data']['s3_parquet_uri'])\n",
    "    elif source == 'postgres':\n",
    "        from sqlalchemy import create_engine, text\n",
    "        pg = CONFIG['data']['pg']\n",
    "        url = f\"postgresql+psycopg://{pg['user']}:{pg['password']}@{pg['host']}:{pg['port']}/{pg['db']}\"\n",
    "        engine = create_engine(url)\n",
    "        with engine.begin() as conn:\n",
    "            df_raw = pd.read_sql_query(text(CONFIG['data']['sql']), conn)\n",
    "    elif source == 'redshift':\n",
    "        import redshift_connector, sqlalchemy\n",
    "        from sqlalchemy import create_engine\n",
    "        rs = CONFIG['data']['redshift']\n",
    "        url = f\"redshift+redshift_connector://{rs['user']}:{rs['password']}@{rs['host']}:{rs['port']}/{rs['database']}\"\n",
    "        engine = create_engine(url)\n",
    "        with engine.begin() as conn:\n",
    "            from sqlalchemy import text\n",
    "            df_raw = pd.read_sql_query(text(CONFIG['data']['sql']), conn)\n",
    "    elif source == 'synthetic':\n",
    "        df_raw = _demo_dataset()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported source. Edit CONFIG.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Failed to load from configured source:\", repr(e))\n",
    "    print(\"Using synthetic demo dataset ‚Ä¶\")\n",
    "    df_raw = _demo_dataset()\n",
    "\n",
    "row_limit = CONFIG['data']['row_limit']\n",
    "if row_limit and len(df_raw) > row_limit:\n",
    "    pk = CONFIG['columns']['primary_key']\n",
    "    if pk in df_raw.columns:\n",
    "        df_raw = df_raw.sort_values(pk).head(row_limit).reset_index(drop=True)\n",
    "    else:\n",
    "        df_raw = df_raw.sample(n=row_limit, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "df_raw.head(3), df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Quick Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"column\": df_raw.columns,\n",
    "    \"dtype\": df_raw.dtypes.astype(str).values,\n",
    "    \"nulls\": [df_raw[c].isna().sum() for c in df_raw.columns],\n",
    "    \"non_nulls\": [df_raw[c].notna().sum() for c in df_raw.columns],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßº Minimal Cleaning (extend as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = CONFIG['columns']['target']\n",
    "primary_key = CONFIG['columns']['primary_key']\n",
    "time_col = CONFIG['columns']['time_col']\n",
    "drop_cols = list(set(CONFIG['columns']['drop_cols'] + [target_col]))\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Convert timedeltas to days if any\n",
    "for c in df.columns:\n",
    "    if str(df[c].dtype).startswith('timedelta'):\n",
    "        df[c] = df[c].dt.total_seconds() / 86400\n",
    "\n",
    "# Ensure datetime for time split\n",
    "if time_col in df.columns:\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "\n",
    "# Fill missing categoricals with a token\n",
    "cat_cols = [c for c in df.columns if df[c].dtype=='object' or str(df[c].dtype)=='category']\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(object).where(~pd.isna(df[c]), '<MISSING>')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Deterministic Split (Time‚Äëbased preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "y = df[target_col].astype(int).values\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "def build_split_masks(df, y, time_col, prefer_time=True):\n",
    "    if prefer_time and time_col in df.columns:\n",
    "        dates = pd.to_datetime(df[time_col], errors='coerce')\n",
    "        if dates.notna().mean() > 0.8:\n",
    "            cutoff = dates.quantile(0.8)\n",
    "            print('cutoff_date:', cutoff)\n",
    "            train_mask = dates < cutoff\n",
    "            valid_mask = ~train_mask\n",
    "            return train_mask, valid_mask\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['split']['test_size'], random_state=SEED)\n",
    "    train_idx, valid_idx = next(splitter.split(X, y))\n",
    "    train_mask = pd.Series(False, index=X.index); train_mask.iloc[train_idx] = True\n",
    "    valid_mask = ~train_mask\n",
    "    return train_mask, valid_mask\n",
    "\n",
    "train_mask, valid_mask = build_split_masks(df, y, time_col, CONFIG['split']['prefer_time_split'])\n",
    "sum(train_mask), sum(valid_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model Zoo (You Choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def best_f1_threshold(y_true, y_scores):\n",
    "    p, r, t = precision_recall_curve(y_true, y_scores)\n",
    "    f1 = (2*p[:-1]*r[:-1])/(p[:-1]+r[:-1]+1e-12)\n",
    "    i = int(np.argmax(f1))\n",
    "    return float(t[i]), float(f1[i]), float(p[i]), float(r[i])\n",
    "\n",
    "def recall_target_threshold(y_true, y_scores, target=0.80):\n",
    "    p, r, t = precision_recall_curve(y_true, y_scores)\n",
    "    idx = np.where(r[:-1] >= target)[0]\n",
    "    if len(idx):\n",
    "        i = int(idx[-1])\n",
    "        return float(t[i]), float(p[i]), float(r[i])\n",
    "    return 0.0, float(p[0]), float(r[0])\n",
    "\n",
    "def prep_tree_inputs(X):\n",
    "    X2 = X.copy()\n",
    "    # drop datetime for tree libs\n",
    "    dt_cols = [c for c in X2.columns if np.issubdtype(X2[c].dtype, np.datetime64)]\n",
    "    if dt_cols: X2 = X2.drop(columns=dt_cols)\n",
    "    for c in X2.columns:\n",
    "        if X2[c].dtype=='object' or str(X2[c].dtype)=='category':\n",
    "            X2[c] = X2[c].astype(str)\n",
    "    return X2\n",
    "\n",
    "X2 = prep_tree_inputs(X)\n",
    "X_train, X_valid = X2.loc[train_mask], X2.loc[valid_mask]\n",
    "y_train, y_valid = y[train_mask], y[valid_mask]\n",
    "\n",
    "enabled = set(CONFIG['models']['enabled'])\n",
    "results, thresholds, models = [], {}, {}\n",
    "\n",
    "# --- CatBoost ---\n",
    "if 'catboost' in enabled:\n",
    "    try:\n",
    "        from catboost import CatBoostClassifier, Pool\n",
    "        cat_cols = [c for c in X_train.columns if X_train[c].dtype=='object']\n",
    "        cat_idx = X_train.columns.get_indexer(cat_cols).tolist()\n",
    "        tr_pool = Pool(X_train, y_train, cat_features=cat_idx)\n",
    "        va_pool = Pool(X_valid, y_valid, cat_features=cat_idx)\n",
    "        p = CONFIG['models']['catboost']\n",
    "        m = CatBoostClassifier(iterations=p['iterations'], learning_rate=p['learning_rate'], depth=p['depth'], l2_leaf_reg=p['l2_leaf_reg'], loss_function='Logloss', eval_metric='AUC', random_seed=SEED, auto_class_weights=p.get('auto_class_weights','Balanced'), task_type=p.get('task_type','CPU'), verbose=p.get('verbose',200))\n",
    "        m.fit(tr_pool, eval_set=va_pool, use_best_model=True, early_stopping_rounds=p['early_stopping_rounds'])\n",
    "        proba = m.predict_proba(va_pool)[:,1]\n",
    "        roc = roc_auc_score(y_valid, proba); pr = average_precision_score(y_valid, proba)\n",
    "        thr_f1, best_f1, p_f1, r_f1 = best_f1_threshold(y_valid, proba)\n",
    "        thr_rec, p80, r80 = recall_target_threshold(y_valid, proba, CONFIG['split']['recall_target'])\n",
    "        results.append({\"model\":\"catboost\",\"roc_auc\":roc,\"pr_auc\":pr,\"best_f1\":best_f1,\"precision_at_best_f1\":p_f1,\"recall_at_best_f1\":r_f1,\"precision_at_recall_target\":p80,\"recall_at_recall_target\":r80})\n",
    "        thresholds['catboost'] = {\"best_f1_threshold\":thr_f1, \"recall_target_threshold\":thr_rec}\n",
    "        models['catboost'] = m\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Skipping CatBoost:\", repr(e))\n",
    "\n",
    "# --- LightGBM ---\n",
    "if 'lightgbm' in enabled:\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        p = CONFIG['models']['lightgbm']\n",
    "        m = lgb.LGBMClassifier(**p)\n",
    "        m.fit(X_train, y_train, eval_set=[(X_valid,y_valid)], eval_metric='auc', verbose=False)\n",
    "        proba = m.predict_proba(X_valid)[:,1]\n",
    "        roc = roc_auc_score(y_valid, proba); pr = average_precision_score(y_valid, proba)\n",
    "        thr_f1, best_f1, p_f1, r_f1 = best_f1_threshold(y_valid, proba)\n",
    "        thr_rec, p80, r80 = recall_target_threshold(y_valid, proba, CONFIG['split']['recall_target'])\n",
    "        results.append({\"model\":\"lightgbm\",\"roc_auc\":roc,\"pr_auc\":pr,\"best_f1\":best_f1,\"precision_at_best_f1\":p_f1,\"recall_at_best_f1\":r_f1,\"precision_at_recall_target\":p80,\"recall_at_recall_target\":r80})\n",
    "        thresholds['lightgbm'] = {\"best_f1_threshold\":thr_f1, \"recall_target_threshold\":thr_rec}\n",
    "        models['lightgbm'] = m\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Skipping LightGBM:\", repr(e))\n",
    "\n",
    "# --- XGBoost ---\n",
    "if 'xgboost' in enabled:\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        p = CONFIG['models']['xgboost']\n",
    "        m = XGBClassifier(**p)\n",
    "        m.fit(X_train, y_train, eval_set=[(X_valid,y_valid)], verbose=False)\n",
    "        proba = m.predict_proba(X_valid)[:,1]\n",
    "        roc = roc_auc_score(y_valid, proba); pr = average_precision_score(y_valid, proba)\n",
    "        thr_f1, best_f1, p_f1, r_f1 = best_f1_threshold(y_valid, proba)\n",
    "        thr_rec, p80, r80 = recall_target_threshold(y_valid, proba, CONFIG['split']['recall_target'])\n",
    "        results.append({\"model\":\"xgboost\",\"roc_auc\":roc,\"pr_auc\":pr,\"best_f1\":best_f1,\"precision_at_best_f1\":p_f1,\"recall_at_best_f1\":r_f1,\"precision_at_recall_target\":p80,\"recall_at_recall_target\":r80})\n",
    "        thresholds['xgboost'] = {\"best_f1_threshold\":thr_f1, \"recall_target_threshold\":thr_rec}\n",
    "        models['xgboost'] = m\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Skipping XGBoost:\", repr(e))\n",
    "\n",
    "# --- Logistic Regression ---\n",
    "if 'logreg' in enabled:\n",
    "    try:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        num_cols = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\n",
    "        cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "        pre = ColumnTransformer([(\"cat\", OneHotEncoder(handle_unknown='ignore'), cat_cols)], remainder='passthrough')\n",
    "        p = CONFIG['models']['logreg']\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        m = Pipeline([('pre', pre), ('clf', LogisticRegression(**p))])\n",
    "        m.fit(X_train, y_train)\n",
    "        proba = m.predict_proba(X_valid)[:,1]\n",
    "        roc = roc_auc_score(y_valid, proba); pr = average_precision_score(y_valid, proba)\n",
    "        thr_f1, best_f1, p_f1, r_f1 = best_f1_threshold(y_valid, proba)\n",
    "        thr_rec, p80, r80 = recall_target_threshold(y_valid, proba, CONFIG['split']['recall_target'])\n",
    "        results.append({\"model\":\"logreg\",\"roc_auc\":roc,\"pr_auc\":pr,\"best_f1\":best_f1,\"precision_at_best_f1\":p_f1,\"recall_at_best_f1\":r_f1,\"precision_at_recall_target\":p80,\"recall_at_recall_target\":r80})\n",
    "        thresholds['logreg'] = {\"best_f1_threshold\":thr_f1, \"recall_target_threshold\":thr_rec}\n",
    "        models['logreg'] = m\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Skipping Logistic Regression:\", repr(e))\n",
    "\n",
    "pd.DataFrame(results).sort_values([\"recall_at_recall_target\",\"pr_auc\",\"roc_auc\"], ascending=[False, False, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÖ Pick Winner by Recall (with tie‚Äëbreakers) & Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(CONFIG['output']['model_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "if res_df.empty:\n",
    "    raise RuntimeError(\"No models trained. Ensure packages are installed and enabled in CONFIG.\")\n",
    "\n",
    "res_sorted = res_df.sort_values([\"recall_at_recall_target\",\"pr_auc\",\"roc_auc\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "best_name = res_sorted.loc[0, 'model']\n",
    "best_model = models[best_name]\n",
    "best_thr = thresholds[best_name]\n",
    "\n",
    "display(res_sorted)\n",
    "print(\"Best model:\", best_name)\n",
    "\n",
    "with open(CONFIG['output']['metrics_path'], 'w') as f:\n",
    "    json.dump(res_sorted.to_dict(orient='records'), f, indent=2)\n",
    "with open(CONFIG['output']['thresholds_path'], 'w') as f:\n",
    "    json.dump(thresholds, f, indent=2)\n",
    "\n",
    "model_path = None\n",
    "try:\n",
    "    if best_name == 'catboost':\n",
    "        model_path = str(Path(CONFIG['output']['model_dir'])/\"catboost_best.cbm\")\n",
    "        best_model.save_model(model_path)\n",
    "    else:\n",
    "        import joblib\n",
    "        model_path = str(Path(CONFIG['output']['model_dir'])/f\"{best_name}_best.joblib\")\n",
    "        joblib.dump(best_model, model_path)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not persist model:\", repr(e))\n",
    "\n",
    "{\n",
    "  'best_model': best_name,\n",
    "  'model_path': model_path,\n",
    "  'metrics_json': CONFIG['output']['metrics_path'],\n",
    "  'thresholds_json': CONFIG['output']['thresholds_path']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà (Optional) MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['mlflow']['enabled']:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(CONFIG['mlflow']['tracking_uri'] or 'file://' + str(Path(ARTIFACT_DIR).absolute()))\n",
    "    mlflow.set_experiment(CONFIG['mlflow']['experiment_name'])\n",
    "    with mlflow.start_run(run_name=f\"student-model-selection-{RUN_TS}\") as run:\n",
    "        mlflow.log_params({\n",
    "            'seed': SEED,\n",
    "            'source': CONFIG['data']['source'],\n",
    "            'time_col': CONFIG['columns']['time_col'],\n",
    "            'recall_target': CONFIG['split']['recall_target']\n",
    "        })\n",
    "        mlflow.log_artifact(CONFIG['output']['metrics_path'])\n",
    "        mlflow.log_artifact(CONFIG['output']['thresholds_path'])\n",
    "        if 'model_path' in locals() and model_path:\n",
    "            mlflow.log_artifact(model_path)\n",
    "        print(\"MLflow run:\", run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Add Your Own Model\n",
    "1. Install the library (if needed)\n",
    "2. Create it inside the **Model Zoo** cell:\n",
    "```python\n",
    "if 'my_model' in enabled:\n",
    "    try:\n",
    "        from mylib import MyModel\n",
    "        m = MyModel(**your_params)\n",
    "        m.fit(X_train, y_train)\n",
    "        proba = m.predict_proba(X_valid)[:,1]\n",
    "        # compute metrics as above, append to results, thresholds, models\n",
    "    except Exception as e:\n",
    "        print('‚ö†Ô∏è Skipping my_model:', repr(e))\n",
    "```\n",
    "3. Add `'my_model'` to `CONFIG['models']['enabled']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Reflection (Short Answer)\n",
    "- Which model won by **Recall at target**? Provide the metrics table and explain any trade‚Äëoffs in **precision**.\n",
    "- Did time‚Äëbased splitting change your results vs stratified? Why might that happen?\n",
    "- What would you try next to improve recall (feature engineering, thresholds, class weights, cost‚Äësensitive training)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
