{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\ude80 Data Preparation Exercise (SageMaker + Redshift)\n\n**Purpose:** Build a *stable*, **reproducible**, and **config-driven** data prep flow you can promote to production.\n\n> This is an **exercise** version of the example. **You choose** the data source (Redshift or S3 Parquet) and the **transformations**. Update the cells marked with `# <- TODO \u270f\ufe0f`.\n\n## \ud83d\udce6 What You\u2019ll Deliver\n- A config-first **data loader** using `load_data()` from `data_io.py` *(or demo data fallback)*  \n- Your own **cleaning & feature engineering** rules (deterministic)  \n- **Leakage-aware** train/val/test splits \u2192 `splits.json`  \n- Export the **processed dataset** \u2192 Parquet  \n\n## \ud83e\uddf0 Prerequisites\n- Python 3.9+\n- Packages: `pandas`, `numpy`, `pyarrow`, `scikit-learn`, `sqlalchemy`, `redshift_connector`, `s3fs` (and optionally `mlflow`)\n- A `data_io.py` next to this notebook containing a `load_data(source, uri, sql, redshift_kwargs)` function.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# If running on a fresh environment (SageMaker usually has these), uncomment as needed\n# %pip install pandas numpy pyarrow scikit-learn sqlalchemy redshift_connector s3fs mlflow"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, sys, json, time, hashlib, platform, random\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\n\n# \u267b\ufe0f Reproducibility & Environment Capture\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nRUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\nRUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n\nARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/run_{RUN_TS}_{RUN_ID}\")\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\nenv_info = {\n    \"python\": sys.version,\n    \"platform\": platform.platform(),\n    \"timestamp_utc\": RUN_TS,\n    \"seed\": SEED,\n    \"packages\": {\n        \"pandas\": pd.__version__,\n        \"numpy\": np.__version__,\n    },\n}\nwith open(os.path.join(ARTIFACT_DIR, \"env_info.json\"), \"w\") as f:\n    json.dump(env_info, f, indent=2)\n\nenv_info"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2699\ufe0f Configuration\nSingle source of truth for inputs, outputs, and behavior. Switch **source** between `\"redshift\"` and `\"parquet\"` here. Fill in your S3 path or SQL.\n\n> Edit the **TODO** fields to point to your data and tweak processing choices.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from pathlib import Path\n\nCONFIG = {\n    \"data\": {\n        # choose 'redshift' or 'parquet'\n        \"source\": os.environ.get(\"SOURCE\", \"parquet\"),  # <- TODO \u270f\ufe0f 'parquet' or 'redshift'\n        # parquet URI if using S3 parquet (supports wildcards)\n        \"parquet_uri\": os.environ.get(\"PARQUET_URI\", \"s3://your-bucket/path/to/data/*.parquet\"),  # <- TODO \u270f\ufe0f your S3 path\n        # SQL for Redshift (must be deterministic if you sample! include ORDER BY)\n        \"sql\": os.environ.get(\"SQL\", \"SELECT * FROM your_schema.your_table ORDER BY id\"),  # <- TODO \u270f\ufe0f your query\n        # Redshift connection kwargs (fill via env vars or IAM role inside SageMaker)\n        \"redshift_kwargs\": {\n            \"host\": os.environ.get(\"REDSHIFT_HOST\", \"example.your-cluster.redshift.amazonaws.com\"),  # <- TODO \u270f\ufe0f\n            \"database\": os.environ.get(\"REDSHIFT_DB\", \"dev\"),  # <- TODO \u270f\ufe0f\n            \"user\": os.environ.get(\"REDSHIFT_USER\", \"username\"),  # <- TODO \u270f\ufe0f\n            \"password\": os.environ.get(\"REDSHIFT_PASSWORD\", \"password\"),  # <- TODO \u270f\ufe0f (or use IAM)\n            \"port\": int(os.environ.get(\"REDSHIFT_PORT\", \"5439\")),\n        },\n        # Optional: limit rows deterministically for dev runs (None = no limit)\n        \"row_limit\": int(os.environ.get(\"ROW_LIMIT\", \"50000\")),  # <- TODO \u270f\ufe0f adjust for quick iteration\n    },\n    \"columns\": {\n        # Define your target column name (classification or regression)\n        \"target\": os.environ.get(\"TARGET\", \"churned\"),  # <- TODO \u270f\ufe0f\n        # Optional primary key for splits and deterministic joins\n        \"primary_key\": os.environ.get(\"PRIMARY_KEY\", \"customer_id\"),  # <- TODO \u270f\ufe0f\n        # Optional timestamp column for time-based split\n        \"timestamp\": os.environ.get(\"TS_COL\", \"\"),  # <- TODO \u270f\ufe0f e.g., event_time\n    },\n    \"processing\": {\n        \"stratify_splits\": True,       # set False for regression  # <- TODO \u270f\ufe0f\n        \"test_size\": 0.2,              # <- TODO \u270f\ufe0f\n        \"val_size\": 0.1,               # <- TODO \u270f\ufe0f\n        \"dropna_threshold_ratio\": 0.95,  # drop columns with >5% missing if needed  # <- TODO \u270f\ufe0f\n        \"cap_outliers_iqr\": True,        # <- TODO \u270f\ufe0f\n        \"normalize_categoricals\": True,  # <- TODO \u270f\ufe0f\n    },\n    \"output\": {\n        \"artifact_dir\": ARTIFACT_DIR,\n        \"processed_parquet_path\": str(Path(ARTIFACT_DIR) / \"processed\" / \"dataset.parquet\"),\n        \"feature_schema_path\": str(Path(ARTIFACT_DIR) / \"feature_schema.json\"),\n        \"splits_path\": str(Path(ARTIFACT_DIR) / \"splits.json\"),\n    },\n    \"mlflow\": {\n        \"enabled\": False,  # <- TODO \u270f\ufe0f enable if you want lineage\n        \"tracking_uri\": os.environ.get(\"MLFLOW_TRACKING_URI\", \"\"),\n        \"experiment_name\": os.environ.get(\"MLFLOW_EXPERIMENT\", \"data-prep-exercise\"),\n    }\n}\n\nCONFIG"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce5 Load Data (Redshift or S3 Parquet)\n\nWe will try to import `load_data()` from `data_io.py`. If not found, we fall back to a **synthetic demo dataset** so the rest of the exercise remains runnable.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Try to import the provided load_data function from data_io.py\nload_data = None\ntry:\n    from data_io import load_data  # expects file next to this notebook\nexcept Exception as e:\n    print(\"\u26a0\ufe0f Could not import `load_data` from data_io.py. Using synthetic demo data. Error:\", repr(e))\n\ndef _demo_dataset(n=3000, seed=SEED):\n    rng = np.random.default_rng(seed)\n    df = pd.DataFrame({\n        \"customer_id\": np.arange(1, n+1),\n        \"age\": rng.integers(18, 90, size=n),\n        \"tenure_months\": rng.integers(0, 120, size=n),\n        \"monthly_charges\": rng.normal(45, 15, size=n).round(2),\n        \"contract_type\": rng.choice([\"month-to-month\", \"one-year\", \"two-year\"], size=n, p=[0.6, 0.25, 0.15]),\n        \"country\": rng.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n, p=[0.5,0.2,0.2,0.1]),\n        \"signup_ts\": pd.to_datetime(\"2022-01-01\") + pd.to_timedelta(rng.integers(0, 900, size=n), unit=\"D\"),\n        \"churned\": rng.choice([0,1], size=n, p=[0.78, 0.22]).astype(int),\n    })\n    # anomalies to clean\n    df.loc[rng.choice(df.index, 15, replace=False), \"monthly_charges\"] = -1.0\n    df.loc[rng.choice(df.index, 25, replace=False), \"age\"] = None\n    return df\n\nif load_data:\n    source = CONFIG[\"data\"][\"source\"]\n    uri = CONFIG[\"data\"][\"parquet_uri\"]\n    sql = CONFIG[\"data\"][\"sql\"]\n    rs_kwargs = CONFIG[\"data\"][\"redshift_kwargs\"]\n    print(f\"Loading data via data_io.load_data(source={source!r}) ...\")\n    df_raw = load_data(source=source, uri=uri, sql=sql, redshift_kwargs=rs_kwargs)\nelse:\n    print(\"Using synthetic dataset for demonstration.\")\n    df_raw = _demo_dataset(n=CONFIG[\"data\"][\"row_limit\"] or 3000)\n\n# Optional row limit for dev runs\nrow_limit = CONFIG[\"data\"][\"row_limit\"]\nif row_limit and len(df_raw) > row_limit:\n    pk = CONFIG[\"columns\"][\"primary_key\"]\n    if pk in df_raw.columns:\n        df_raw = df_raw.sort_values(pk).head(row_limit).reset_index(drop=True)\n    else:\n        df_raw = df_raw.sample(n=row_limit, random_state=SEED).reset_index(drop=True)\n\ndf_raw.head(), df_raw.shape"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udd0e Quick Profile\nLightweight overview\u2014scan types, nulls, and rough completeness.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "pd.DataFrame({\n    \"column\": df_raw.columns,\n    \"dtype\": df_raw.dtypes.astype(str).values,\n    \"nulls\": [df_raw[c].isna().sum() for c in df_raw.columns],\n    \"non_nulls\": [df_raw[c].notna().sum() for c in df_raw.columns],\n}).head(40)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfe Feature Schema (Draft)\nDefine **types, nullability, min/max for numerics**, and sample categories for strings. Export to JSON.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import json\nfrom typing import Any, Dict\n\ntarget_col = CONFIG[\"columns\"][\"target\"]\nprimary_key = CONFIG[\"columns\"][\"primary_key\"]\n\ndef infer_basic_schema(df: pd.DataFrame) -> Dict[str, Any]:\n    schema = {}\n    for c in df.columns:\n        col_dtype = str(df[c].dtype)\n        col = { \"dtype\": col_dtype, \"nullable\": bool(df[c].isna().any()) }\n        if pd.api.types.is_numeric_dtype(df[c]):\n            finite_vals = pd.to_numeric(df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n            if len(finite_vals):\n                col[\"min\"] = float(finite_vals.min())\n                col[\"max\"] = float(finite_vals.max())\n        else:\n            col[\"example_values\"] = df[c].dropna().astype(str).unique()[:20].tolist()\n        schema[c] = col\n    schema_meta = {\n        \"_meta\": {\n            \"target\": target_col if target_col in df.columns else None,\n            \"primary_key\": primary_key if primary_key in df.columns else None,\n            \"generated_at\": RUN_TS,\n            \"seed\": SEED,\n        },\n        \"columns\": schema\n    }\n    return schema_meta\n\nfeature_schema = infer_basic_schema(df_raw)\n\nschema_path = CONFIG[\"output\"][\"feature_schema_path\"]\nos.makedirs(Path(schema_path).parent, exist_ok=True)\nwith open(schema_path, \"w\") as f:\n    json.dump(feature_schema, f, indent=2)\n\nschema_path"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfc Cleaning (YOU decide)\nUpdate the block below to **codify** your rules. Keep them deterministic and versioned.\n\nSuggestions (edit as needed):\n- Type coercions for IDs\n- Invalid value fixes (e.g., negative price \u2192 NaN)\n- Missing value strategies (numeric/categorical)\n- Optional outlier capping (IQR)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "df = df_raw.copy()\n\n# IDs as strings (preserve leading zeros)\nif primary_key in df.columns:\n    df[primary_key] = df[primary_key].astype(str)  # <- TODO \u270f\ufe0f confirm your PK name\n\n# Example rule: fix invalid negatives in 'monthly_charges'\nif \"monthly_charges\" in df.columns:\n    df.loc[df[\"monthly_charges\"] < 0, \"monthly_charges\"] = np.nan  # <- TODO \u270f\ufe0f adapt to your domain\n\n# Build column type lists (excluding target)\nnum_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != target_col]\ncat_cols = [c for c in df.columns if (not pd.api.types.is_numeric_dtype(df[c])) and c not in [target_col]]\n\n# Missing values \u2014 simple, deterministic strategy (EDIT if you need smarter imputers)\nfor c in num_cols:\n    median_val = df[c].median()\n    df[c] = df[c].fillna(median_val)  # <- TODO \u270f\ufe0f try mean/constant or KNN later\n\nfor c in cat_cols:\n    df[c] = df[c].fillna(\"__MISSING__\")  # <- TODO \u270f\ufe0f choose a sentinel\n\n# Normalize categoricals (lowercase/trim) for consistency\nif bool(CONFIG[\"processing\"][\"normalize_categoricals\"]):\n    for c in cat_cols:\n        df[c] = df[c].astype(str).str.strip().str.lower()\n\n# Optional IQR capping for outliers\nif bool(CONFIG[\"processing\"][\"cap_outliers_iqr\"]):\n    for c in num_cols:\n        q1, q3 = np.percentile(df[c], [25, 75])\n        iqr = q3 - q1\n        lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n        df[c] = np.clip(df[c], lower, upper)  # <- TODO \u270f\ufe0f choose columns to cap\n\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\udde9 Feature Engineering (YOU decide)\nAdd domain features and lightweight encodings here. Defer heavy encoders to training.\n\nExamples:\n- Ratios/interactions\n- Date parts from timestamps\n- Frequency or target-aware encodings (deterministic)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Example engineered features (EDIT/ADD/REMOVE)\nts_col = CONFIG[\"columns\"].get(\"timestamp\")  # <- TODO \u270f\ufe0f set in config if you have time\nif ts_col and ts_col in df.columns:\n    ts = pd.to_datetime(df[ts_col], errors=\"coerce\")\n    df[\"ts_year\"] = ts.dt.year\n    df[\"ts_month\"] = ts.dt.month\n    df[\"ts_dow\"] = ts.dt.dayofweek\n\n# Example: lifetime value proxy\nif set([\"tenure_months\",\"monthly_charges\"]).issubset(df.columns):\n    df[\"est_lifetime_value\"] = (df[\"tenure_months\"] * df[\"monthly_charges\"]).round(2)  # <- TODO \u270f\ufe0f your formula\n\n# Simple frequency encoding for categoricals (kept numeric)\nfor c in [c for c in df.columns if (c not in [target_col]) and (df[c].dtype == object or isinstance(df[c].dtype, pd.StringDtype))]:\n    freq = df[c].value_counts(normalize=True)\n    df[f\"{c}__freq\"] = df[c].map(freq).astype(float)\n\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2705 Lightweight Validation\nAdd simple checks before export. For production, consider Great Expectations or pandera.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "checks = []\n\n# 1) Target checks\nif target_col in df.columns and CONFIG[\"processing\"][\"stratify_splits\"]:\n    uniq = pd.Series(df[target_col]).dropna().unique()\n    is_binary = set(uniq) <= {0,1}\n    checks.append({\"check\": \"target_binary_if_classif\", \"passed\": bool(is_binary), \"unique\": uniq.tolist()})\n\n# 2) Primary key not null\nif primary_key in df.columns:\n    pk_nulls = int(df[primary_key].isna().sum())\n    checks.append({\"check\": \"no_null_primary_key\", \"passed\": pk_nulls == 0, \"null_count\": pk_nulls})\n\n# 3) Optional: drop columns with too many nulls (ratio threshold)\nthr = float(CONFIG[\"processing\"][\"dropna_threshold_ratio\"])\nnull_ratio = df.isna().mean().to_dict()\nchecks.append({\"check\": \"null_ratio_snapshot\", \"passed\": True, \"details\": {k: round(v,4) for k,v in null_ratio.items()}})\n\npd.DataFrame(checks)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2702\ufe0f Train / Validation / Test Split\n- Deterministic with fixed `random_state`\n- Stratified if classification\n- Optionally **time-based** using your timestamp column\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.model_selection import train_test_split\n\nrandom_state = SEED\nstratify = None\nif CONFIG[\"processing\"][\"stratify_splits\"] and (target_col in df.columns):\n    stratify = df[target_col]\n\nts_col = CONFIG[\"columns\"].get(\"timestamp\", \"\")\nif ts_col and ts_col in df.columns:\n    ts = pd.to_datetime(df[ts_col], errors=\"coerce\")\n    # 80/20 split by time, then split 20% into val/test by ratio\n    cutoff = ts.quantile(1.0 - CONFIG[\"processing\"][\"test_size\"])\n    train_val_df = df[ts < cutoff].copy()\n    test_df = df[ts >= cutoff].copy()\nelse:\n    train_val_df, test_df = train_test_split(\n        df,\n        test_size=CONFIG[\"processing\"][\"test_size\"],\n        random_state=random_state,\n        stratify=stratify\n    )\n\n# Second split: train vs val\nstratify_train_val = train_val_df[target_col] if (stratify is not None and target_col in train_val_df.columns) else None\nval_ratio = CONFIG[\"processing\"][\"val_size\"] / (1.0 - CONFIG[\"processing\"][\"test_size\"])\ntrain_df, val_df = train_test_split(\n    train_val_df,\n    test_size=val_ratio,\n    random_state=random_state,\n    stratify=stratify_train_val\n)\n\nlen(train_df), len(val_df), len(test_df)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udcbe Write Artifacts\n- **Processed dataset** (Parquet)\n- **Feature schema** (`feature_schema.json`)\n- **Splits** (`splits.json`) with IDs for deterministic reuse\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from pathlib import Path\nout_path = Path(CONFIG[\"output\"][\"processed_parquet_path\"])\nout_path.parent.mkdir(parents=True, exist_ok=True)\n\n# Save full processed dataset\ndf.to_parquet(out_path, index=False)\n\n# Save split IDs by primary key (preferred) or DataFrame indices\nsplits = {}\npk = primary_key if primary_key in df.columns else None\ndef ids_of(subdf):\n    if pk:\n        return subdf[pk].tolist()\n    else:\n        return subdf.index.tolist()\n\nsplits = {\n    \"meta\": {\n        \"seed\": SEED,\n        \"created_at\": RUN_TS,\n        \"primary_key\": pk,\n        \"target\": target_col if target_col in df.columns else None,\n        \"source\": CONFIG[\"data\"][\"source\"],\n    },\n    \"train_ids\": ids_of(train_df),\n    \"val_ids\": ids_of(val_df),\n    \"test_ids\": ids_of(test_df),\n}\n\nwith open(CONFIG[\"output\"][\"splits_path\"], \"w\") as f:\n    json.dump(splits, f, indent=2)\n\n{\n    \"processed_parquet\": str(out_path),\n    \"feature_schema\": CONFIG[\"output\"][\"feature_schema_path\"],\n    \"splits\": CONFIG[\"output\"][\"splits_path\"],\n}"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udcc8 (Optional) MLflow Trace\nEnable by setting `CONFIG[\"mlflow\"][\"enabled\"] = True`.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "if CONFIG[\"mlflow\"][\"enabled\"]:\n    import mlflow\n    mlflow.set_tracking_uri(CONFIG[\"mlflow\"][\"tracking_uri\"] or \"file://\" + str(Path(ARTIFACT_DIR).absolute()))\n    mlflow.set_experiment(CONFIG[\"mlflow\"][\"experiment_name\"])\n\n    with mlflow.start_run(run_name=f\"data-prep-exercise-{RUN_TS}\") as run:\n        mlflow.log_params({\n            \"seed\": SEED,\n            \"source\": CONFIG[\"data\"][\"source\"],\n            \"row_limit\": CONFIG[\"data\"][\"row_limit\"],\n            \"stratify\": CONFIG[\"processing\"][\"stratify_splits\"],\n        })\n        mlflow.log_artifact(CONFIG[\"output\"][\"feature_schema_path\"])\n        mlflow.log_artifact(CONFIG[\"output\"][\"splits_path\"])\n        # logging the whole parquet can be large; consider sampling or schema-only\n        # mlflow.log_artifact(CONFIG[\"output\"][\"processed_parquet_path\"])\n        print(\"MLflow run:\", run.info.run_id)"}]}