{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b263b2b5",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Data Preparation (SageMaker + Redshift)\n",
    "\n",
    "**Purpose:** Create a *stable*, **reproducible**, and **config-driven** data preparation process that is safe to promote toward production deployment.\n",
    "\n",
    "> This notebook is designed to run on SageMaker or locally. It loads data from **Redshift** or **S3 Parquet** via the provided `load_data` function in `data_io.py`, applies deterministic preprocessing & feature engineering, validates schema, and writes versioned artifacts for downstream training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db0eae",
   "metadata": {},
   "source": [
    "\n",
    "## üì¶ What You Get\n",
    "- Config-first **data loader** (Redshift or S3 Parquet) using `load_data()` from `data_io.py`  \n",
    "- **Cleaning & feature engineering** examples\n",
    "- **Leakage-aware** train/val/test splits ‚Üí `splits.json`  \n",
    "- Export **processed dataset** ‚Üí Parquet (optionally partitioned)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68593dbc",
   "metadata": {},
   "source": [
    "\n",
    "## üß∞ Prerequisites\n",
    "- Python 3.9+\n",
    "- Packages: `pandas`, `numpy`, `pyarrow`, `scikit-learn`, `mlflow` (optional), `sqlalchemy`, `redshift_connector`, `s3fs`\n",
    "- A `data_io.py` next to this notebook containing the provided `load_data(...)` implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054e1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a fresh environment, uncomment as needed (SageMaker kernels usually have most of these)\n",
    "# %pip install pandas numpy pyarrow scikit-learn mlflow sqlalchemy redshift_connector s3fs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55458e",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ôªÔ∏è Reproducibility & Environment Capture\n",
    "- **Fixed seeds** for deterministic results.\n",
    "- Capture **package versions** for traceability.\n",
    "- All artifacts are written under a **run folder** with a unique timestamp/hash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0759c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/dh5fxyvd55sbf5r8pv3bfclw0000gq/T/ipykernel_88142/1607273166.py:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  RUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts/run_20251016T180850Z_0e3e59e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, json, platform, random, hashlib\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Run folder\n",
    "RUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:8]\n",
    "ARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/run_{RUN_TS}_{RUN_ID}\")\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# Minimal environment capture\n",
    "ENV_INFO = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"timestamp_utc\": RUN_TS,\n",
    "    \"seed\": SEED,\n",
    "    \"packages\": {\"pandas\": pd.__version__, \"numpy\": np.__version__}\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"env_info.json\"), \"w\") as f:\n",
    "    json.dump(ENV_INFO, f, indent=2)\n",
    "\n",
    "ARTIFACT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f9f1b",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è Configuration\n",
    "Single source of truth for inputs, outputs, and behavior. Switch **source** between `\"redshift\"` and `\"parquet\"` using this cell only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6006483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: churn\n",
      "Test size: 20%\n"
     ]
    }
   ],
   "source": [
    "# Configuration - similar to production but simplified for training\n",
    "CONFIG = {\n",
    "    \"target_col\": \"churn\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_seed\": SEED,\n",
    "    \n",
    "    # Feature lists (identify which columns are what type)\n",
    "    \"id_features\": [\"idconsumo\", \"codigocontaservico\", \"idconta\", \"iddim_cliente\"],\n",
    "    \"int_features\": [\"codigocontaservico\", \"codigocliente\", \"n_dias_subscricao\"],\n",
    "    \"datetime_features\": [\"iddim_date_inicio\", \"iddim_date_fim\"]\n",
    "}\n",
    "\n",
    "print(f\"Target: {CONFIG['target_col']}\")\n",
    "print(f\"Test size: {CONFIG['test_size']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fda91a",
   "metadata": {},
   "source": [
    "\n",
    "## üì• Load Data (Redshift or S3 Parquet)\n",
    "Uses `load_data()` defined in `data_io.py`:\n",
    "\n",
    "```python\n",
    "df = load_data(\n",
    "    source=CONFIG[\"data\"][\"source\"],\n",
    "    uri=CONFIG[\"data\"][\"parquet_uri\"],\n",
    "    sql=CONFIG[\"data\"][\"sql\"],\n",
    "    redshift_kwargs=CONFIG[\"data\"][\"redshift_kwargs\"]\n",
    ")\n",
    "```\n",
    "If `data_io.py` is not found, we fall back to a **synthetic demo dataset** so the rest of the pipeline remains testable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006a0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../sample_data_from_redshift/sample.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caa318",
   "metadata": {},
   "source": [
    "\n",
    "## üîé Quick Profile\n",
    "Lightweight overview to understand data types, nulls, and basic distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91d31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 100,000 rows x 58 columns\n",
      "\n",
      "Target distribution:\n",
      "churn\n",
      "0    70329\n",
      "1    29671\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Churn rate: 29.7%\n",
      "\n",
      "Total missing values: 297,424\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idconsumo</th>\n",
       "      <th>id_contaservico</th>\n",
       "      <th>codigocontaservico</th>\n",
       "      <th>idconta</th>\n",
       "      <th>iddim_date_inicio</th>\n",
       "      <th>iddim_date_fim</th>\n",
       "      <th>id_produto_actual</th>\n",
       "      <th>tipo_produto_actual</th>\n",
       "      <th>tipo_subscricao</th>\n",
       "      <th>tipo_stb</th>\n",
       "      <th>...</th>\n",
       "      <th>was_contacted</th>\n",
       "      <th>topup_count</th>\n",
       "      <th>topup_total_value</th>\n",
       "      <th>topup_avg_value</th>\n",
       "      <th>topup_std_value</th>\n",
       "      <th>topup_cv_value</th>\n",
       "      <th>topup_days_since_last</th>\n",
       "      <th>used_selfcare</th>\n",
       "      <th>topup_type_nunique</th>\n",
       "      <th>topup_channel_nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>511039064</td>\n",
       "      <td>3594153</td>\n",
       "      <td>110614850201</td>\n",
       "      <td>3539750</td>\n",
       "      <td>2025-05-17</td>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>24</td>\n",
       "      <td>tafacil7</td>\n",
       "      <td>7</td>\n",
       "      <td>HD</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12280.70</td>\n",
       "      <td>383.77</td>\n",
       "      <td>578.333869</td>\n",
       "      <td>1.506980</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>412255701</td>\n",
       "      <td>1157023</td>\n",
       "      <td>133299940101</td>\n",
       "      <td>1120996</td>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>24</td>\n",
       "      <td>normal</td>\n",
       "      <td>7</td>\n",
       "      <td>HD</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10657.89</td>\n",
       "      <td>1522.55</td>\n",
       "      <td>1898.960612</td>\n",
       "      <td>1.247224</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>534347088</td>\n",
       "      <td>2987119</td>\n",
       "      <td>111385530401</td>\n",
       "      <td>2930755</td>\n",
       "      <td>2025-08-04</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>24</td>\n",
       "      <td>normal</td>\n",
       "      <td>7</td>\n",
       "      <td>HD</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>26570.18</td>\n",
       "      <td>369.03</td>\n",
       "      <td>653.186315</td>\n",
       "      <td>1.770009</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   idconsumo  id_contaservico codigocontaservico  idconta iddim_date_inicio  \\\n",
       "0  511039064          3594153       110614850201  3539750        2025-05-17   \n",
       "1  412255701          1157023       133299940101  1120996        2024-05-30   \n",
       "2  534347088          2987119       111385530401  2930755        2025-08-04   \n",
       "\n",
       "  iddim_date_fim  id_produto_actual tipo_produto_actual  tipo_subscricao  \\\n",
       "0     2025-05-19                 24            tafacil7                7   \n",
       "1     2024-06-25                 24              normal                7   \n",
       "2     2025-08-06                 24              normal                7   \n",
       "\n",
       "  tipo_stb  ...  was_contacted  topup_count  topup_total_value  \\\n",
       "0       HD  ...              0           32           12280.70   \n",
       "1       HD  ...              0            7           10657.89   \n",
       "2       HD  ...              0           72           26570.18   \n",
       "\n",
       "   topup_avg_value  topup_std_value  topup_cv_value  topup_days_since_last  \\\n",
       "0           383.77       578.333869        1.506980                      2   \n",
       "1          1522.55      1898.960612        1.247224                     26   \n",
       "2           369.03       653.186315        1.770009                      2   \n",
       "\n",
       "   used_selfcare  topup_type_nunique  topup_channel_nunique  \n",
       "0              0                   4                      1  \n",
       "1              0                   3                      0  \n",
       "2              0                   4                      1  \n",
       "\n",
       "[3 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick look at the data\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows x {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Check target distribution\n",
    "print(\"Target distribution:\")\n",
    "print(df[CONFIG['target_col']].value_counts())\n",
    "print(f\"\\nChurn rate: {df[CONFIG['target_col']].mean():.1%}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum():,}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacb588",
   "metadata": {},
   "source": [
    "\n",
    "## üßæ Feature Schema (Draft)\n",
    "Define **types, nullability, and basic constraints**. This schema will be exported to JSON and should be reviewed by ML + DevOps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20010af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types in the dataset:\n",
      "\n",
      "ID features: 4\n",
      "Integer features: 3\n",
      "Datetime features: 2\n",
      "Other numeric: 38\n",
      "Categorical: 10\n",
      "Target: 1\n",
      "\n",
      "Total features: 58\n"
     ]
    }
   ],
   "source": [
    "# Identify feature types using config (like production)\n",
    "print(\"Feature types in the dataset:\\n\")\n",
    "\n",
    "# Get feature lists from config\n",
    "id_cols = [c for c in CONFIG['id_features'] if c in df.columns]\n",
    "int_cols = [c for c in CONFIG['int_features'] if c in df.columns]\n",
    "datetime_cols = [c for c in CONFIG['datetime_features'] if c in df.columns]\n",
    "\n",
    "# Everything else is either numeric or categorical\n",
    "all_special = set(id_cols + int_cols + datetime_cols + [CONFIG['target_col']])\n",
    "remaining = [c for c in df.columns if c not in all_special]\n",
    "\n",
    "# Split remaining into numeric vs categorical\n",
    "numeric_cols = df[remaining].select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df[remaining].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"ID features: {len(id_cols)}\")\n",
    "print(f\"Integer features: {len(int_cols)}\")\n",
    "print(f\"Datetime features: {len(datetime_cols)}\")\n",
    "print(f\"Other numeric: {len(numeric_cols)}\")\n",
    "print(f\"Categorical: {len(categorical_cols)}\")\n",
    "print(f\"Target: 1\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77e22d",
   "metadata": {},
   "source": [
    "\n",
    "## üßº Cleaning\n",
    "- Handle missing values\n",
    "- Normalize categoricals\n",
    "- Fix invalid values (e.g., negative prices)\n",
    "- Optional: Outlier capping via IQR\n",
    "> **Stable rule sets** are critical‚Äîavoid ad-hoc fixes. All transformations must be deterministic and versioned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0425fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "\n",
      "Converted iddim_date_inicio to datetime\n",
      "Converted iddim_date_fim to datetime\n",
      "Filled 80 missing in 'codigocliente' with -1\n",
      "Filled 3,751 missing in 'gap_since_prev_expiry' with 0.0\n",
      "Filled 7,229 missing in 'mean_len_prev' with 0.0\n",
      "Filled 10,721 missing in 'std_len_prev' with 0.0\n",
      "Filled 80 missing in 'idcliente' with 0.0\n",
      "Filled 66,524 missing in 'age' with 0.0\n",
      "Filled 80 missing in 'age_missing' with 0.0\n",
      "Filled 80 missing in 'account_age_d_cliente' with 0.0\n",
      "Filled 6,102 missing in 'days_since_last_update_cliente' with 0.0\n",
      "Filled 80 missing in 'iddim_conta' with 0.0\n",
      "Filled 848 missing in 'idgrupo_dim_contadimensao' with 0.0\n",
      "Filled 80 missing in 'account_age_d_conta' with 0.0\n",
      "Filled 80 missing in 'iddim_contaservico_dth' with 0.0\n",
      "Filled 80 missing in 'account_age_d_contaservico' with 0.0\n",
      "Filled 99,373 missing in 'days_since_last_contact' with 0.0\n",
      "Filled 196 missing in 'topup_avg_value' with 0.0\n",
      "Filled 1,012 missing in 'topup_std_value' with 0.0\n",
      "Filled 778 missing in 'topup_cv_value' with 0.0\n",
      "Filled 196 missing in 'topup_days_since_last' with 0.0\n",
      "Filled 21 missing in 'tipo_stb' with '<MISSING>'\n",
      "Filled 80 missing in 'provincia' with '<MISSING>'\n",
      "Filled 80 missing in 'municipio' with '<MISSING>'\n",
      "Filled 90 missing in 'sexo' with '<MISSING>'\n",
      "Filled 90 missing in 'estado_civil' with '<MISSING>'\n",
      "Filled 80 missing in 'codigoconta' with '<MISSING>'\n",
      "Filled 80 missing in 'tipoconta' with '<MISSING>'\n",
      "Filled 80 missing in 'tecnologia' with '<MISSING>'\n",
      "\n",
      "‚úì Cleaning complete: 100,000 rows, 99453 missing values\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning - based on production preprocess_data() function\n",
    "print(\"Starting data cleaning...\\n\")\n",
    "\n",
    "# 1. Convert datetime features to datetime type\n",
    "for col in datetime_cols:\n",
    "    if 'date' in col.lower():\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"Converted {col} to datetime\")\n",
    "\n",
    "# 2. Fill integer features with -1 (production approach)\n",
    "for col in int_cols:\n",
    "    missing = df[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        df[col] = df[col].fillna(-1).astype(int)\n",
    "        print(f\"Filled {missing:,} missing in '{col}' with -1\")\n",
    "    else:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# 3. Fill other numeric features with 0.0\n",
    "for col in numeric_cols:\n",
    "    missing = df[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "        print(f\"Filled {missing:,} missing in '{col}' with 0.0\")\n",
    "\n",
    "# 4. Fill categorical features with '<MISSING>' (production approach)\n",
    "for col in categorical_cols:\n",
    "    missing = df[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        df[col] = df[col].fillna('<MISSING>')\n",
    "        print(f\"Filled {missing:,} missing in '{col}' with '<MISSING>'\")\n",
    "\n",
    "# 5. Drop rows where target is missing\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=[CONFIG['target_col']])\n",
    "dropped = initial_rows - len(df)\n",
    "if dropped > 0:\n",
    "    print(f\"Dropped {dropped:,} rows with missing target\")\n",
    "\n",
    "# 6. Ensure target is integer\n",
    "df[CONFIG['target_col']] = df[CONFIG['target_col']].astype(int)\n",
    "\n",
    "print(f\"\\n‚úì Cleaning complete: {len(df):,} rows, {df.isnull().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf23786",
   "metadata": {},
   "source": [
    "\n",
    "## üß© Feature Engineering (Examples)\n",
    "- Ratios and interactions\n",
    "- Encoding categoricals for model training (defer heavy encoders to training pipeline)\n",
    "- Date/time features (if present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8691f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering step...\n",
      "Converted tenure_bucket to string\n",
      "\n",
      "Total features: 58\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering (optional - keep it minimal for now)\n",
    "# Production code doesn't create new features here, just prepares existing ones\n",
    "# You can add domain-specific features if needed\n",
    "\n",
    "print(\"Feature engineering step...\")\n",
    "\n",
    "# Example: Convert tenure_bucket to string if it exists (production does this)\n",
    "if 'tenure_bucket' in df.columns:\n",
    "    df['tenure_bucket'] = df['tenure_bucket'].astype(str)\n",
    "    print(\"Converted tenure_bucket to string\")\n",
    "\n",
    "# Add your own feature engineering here if needed\n",
    "# Examples:\n",
    "# - Ratios: df['ratio'] = df['col1'] / df['col2']  \n",
    "# - Bins: df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100])\n",
    "# - Flags: df['is_active'] = (df['days_since_last'] < 30).astype(int)\n",
    "\n",
    "print(f\"\\nTotal features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a2a8d",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Lightweight Validation\n",
    "Simple checks before export. For production, consider a formal framework (e.g., Great Expectations or pandera).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaacdfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic checks...\n",
      "\n",
      "‚úì No missing targets\n",
      "‚úì Target is binary\n",
      "‚úì Sufficient data: 100,000 rows\n",
      "‚úì Churn rate: 29.7% (29671 churners)\n",
      "\n",
      "All checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Quick validation checks\n",
    "print(\"Running basic checks...\\n\")\n",
    "\n",
    "# Check 1: No missing targets\n",
    "target_col = CONFIG['target_col']\n",
    "assert df[target_col].isnull().sum() == 0, \"Target has missing values!\"\n",
    "print(\"‚úì No missing targets\")\n",
    "\n",
    "# Check 2: Target is binary (0 and 1)\n",
    "unique_targets = df[target_col].unique()\n",
    "assert set(unique_targets).issubset({0, 1}), f\"Target should be 0/1, found: {unique_targets}\"\n",
    "print(\"‚úì Target is binary\")\n",
    "\n",
    "# Check 3: Enough data\n",
    "assert len(df) > 1000, f\"Not enough data: {len(df)} rows\"\n",
    "print(f\"‚úì Sufficient data: {len(df):,} rows\")\n",
    "\n",
    "# Check 4: Check class balance\n",
    "churn_rate = df[target_col].mean()\n",
    "print(f\"‚úì Churn rate: {churn_rate:.1%} ({churn_rate*len(df):.0f} churners)\")\n",
    "\n",
    "print(\"\\nAll checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40d147",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÇÔ∏è Train / Validation / Test Split\n",
    "- Deterministic with fixed `random_state`\n",
    "- Stratified if classification (`stratify=True`)\n",
    "- Optionally **time-based** if a timestamp column is provided (edit here if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df886cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data by time...\n",
      "\n",
      "Train: 80,000 rows (80%)\n",
      "Val:   10,000 rows (10%)\n",
      "Test:  10,000 rows (10%)\n",
      "\n",
      "Date ranges:\n",
      "Train: 2017-12-22 00:00:00 to 2025-06-23 00:00:00\n",
      "Val:   2025-06-23 00:00:00 to 2025-08-07 00:00:00\n",
      "Test:  2025-08-07 00:00:00 to 2025-09-29 00:00:00\n",
      "\n",
      "Churn rates:\n",
      "Train: 34.5%\n",
      "Val:   11.6%\n",
      "Test:  9.0%\n"
     ]
    }
   ],
   "source": [
    "# Time-based split (simplified from prepare_data_for_training in production)\n",
    "# This is better for time-series data - train on older data, test on newer data\n",
    "\n",
    "print(\"Splitting data by time...\\n\")\n",
    "\n",
    "# Sort by date column\n",
    "date_col = 'iddim_date_inicio'\n",
    "if date_col in df.columns:\n",
    "    df = df.sort_values(date_col)\n",
    "    \n",
    "    # Define cutoff points (80% train, 10% val, 10% test)\n",
    "    train_size = 0.8\n",
    "    val_size = 0.1\n",
    "    \n",
    "    n = len(df)\n",
    "    train_end = int(n * train_size)\n",
    "    val_end = int(n * (train_size + val_size))\n",
    "    \n",
    "    # Split by position (time-ordered)\n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"Train: {len(train_df):,} rows ({len(train_df)/len(df):.0%})\")\n",
    "    print(f\"Val:   {len(val_df):,} rows ({len(val_df)/len(df):.0%})\")\n",
    "    print(f\"Test:  {len(test_df):,} rows ({len(test_df)/len(df):.0%})\")\n",
    "    \n",
    "    # Check date ranges\n",
    "    print(f\"\\nDate ranges:\")\n",
    "    print(f\"Train: {train_df[date_col].min()} to {train_df[date_col].max()}\")\n",
    "    print(f\"Val:   {val_df[date_col].min()} to {val_df[date_col].max()}\")\n",
    "    print(f\"Test:  {test_df[date_col].min()} to {test_df[date_col].max()}\")\n",
    "    \n",
    "    # Check churn rates\n",
    "    target_col = CONFIG['target_col']\n",
    "    print(f\"\\nChurn rates:\")\n",
    "    print(f\"Train: {train_df[target_col].mean():.1%}\")\n",
    "    print(f\"Val:   {val_df[target_col].mean():.1%}\")\n",
    "    print(f\"Test:  {test_df[target_col].mean():.1%}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Date column '{date_col}' not found, falling back to random split\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=CONFIG['test_size'], \n",
    "        random_state=CONFIG['random_seed'],\n",
    "        stratify=df[CONFIG['target_col']]\n",
    "    )\n",
    "    val_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446157fa",
   "metadata": {},
   "source": [
    "\n",
    "## üíæ Write Artifacts\n",
    "- **Processed dataset** (Parquet)\n",
    "- **Feature schema** (`feature_schema.json`)\n",
    "- **Splits** (`splits.json`) with IDs for deterministic reuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b8f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "\n",
      "‚úì Train data saved: artifacts/run_20251016T180850Z_0e3e59e5/train.parquet\n",
      "‚úì Val data saved:   artifacts/run_20251016T180850Z_0e3e59e5/val.parquet\n",
      "‚úì Test data saved:  artifacts/run_20251016T180850Z_0e3e59e5/test.parquet\n",
      "‚úì Summary saved: artifacts/run_20251016T180850Z_0e3e59e5/summary.json\n",
      "\n",
      "All files saved to: artifacts/run_20251016T180850Z_0e3e59e5\n"
     ]
    }
   ],
   "source": [
    "# Save the processed datasets\n",
    "print(\"Saving data...\\n\")\n",
    "\n",
    "# Save as Parquet (efficient format for ML)\n",
    "train_path = os.path.join(ARTIFACT_DIR, \"train.parquet\")\n",
    "val_path = os.path.join(ARTIFACT_DIR, \"val.parquet\")\n",
    "test_path = os.path.join(ARTIFACT_DIR, \"test.parquet\")\n",
    "\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "val_df.to_parquet(val_path, index=False)\n",
    "test_df.to_parquet(test_path, index=False)\n",
    "\n",
    "print(f\"‚úì Train data saved: {train_path}\")\n",
    "print(f\"‚úì Val data saved:   {val_path}\")\n",
    "print(f\"‚úì Test data saved:  {test_path}\")\n",
    "\n",
    "# Save a simple summary\n",
    "target_col = CONFIG['target_col']\n",
    "summary = {\n",
    "    \"timestamp\": RUN_TS,\n",
    "    \"train_rows\": len(train_df),\n",
    "    \"val_rows\": len(val_df),\n",
    "    \"test_rows\": len(test_df),\n",
    "    \"features\": len(train_df.columns),\n",
    "    \"train_churn_rate\": float(train_df[target_col].mean()),\n",
    "    \"val_churn_rate\": float(val_df[target_col].mean()),\n",
    "    \"test_churn_rate\": float(test_df[target_col].mean()),\n",
    "    \"split_type\": \"time_based\"\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(ARTIFACT_DIR, \"summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Summary saved: {summary_path}\")\n",
    "print(f\"\\nAll files saved to: {ARTIFACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418ba8e",
   "metadata": {},
   "source": [
    "\n",
    "## üìà (Optional) MLflow Trace\n",
    "Record data prep parameters and artifacts for lineage. Enable by setting `CONFIG[\"mlflow\"][\"enabled\"] = True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc16d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking disabled (set USE_MLFLOW=True to enable)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Track with MLflow (skip this if you don't have MLflow set up)\n",
    "USE_MLFLOW = False  # Set to True to enable\n",
    "\n",
    "if USE_MLFLOW:\n",
    "    try:\n",
    "        import mlflow\n",
    "        \n",
    "        mlflow.set_experiment(\"data-preparation\")\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            # Log basic info\n",
    "            mlflow.log_params({\n",
    "                \"train_rows\": len(train_df),\n",
    "                \"val_rows\": len(val_df),\n",
    "                \"test_rows\": len(test_df),\n",
    "                \"split_type\": \"time_based\",\n",
    "                \"churn_rate\": summary['train_churn_rate']\n",
    "            })\n",
    "            \n",
    "            # Log the data files\n",
    "            mlflow.log_artifact(train_path)\n",
    "            mlflow.log_artifact(val_path)\n",
    "            mlflow.log_artifact(test_path)\n",
    "            \n",
    "            print(\"‚úì Logged to MLflow\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è MLflow not available - skipping\")\n",
    "else:\n",
    "    print(\"MLflow tracking disabled (set USE_MLFLOW=True to enable)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
