{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b263b2b5",
   "metadata": {},
   "source": [
    "\n",
    "# 🚀 Data Preparation (SageMaker + Redshift)\n",
    "\n",
    "**Purpose:** Create a *stable*, **reproducible**, and **config-driven** data preparation process that is safe to promote toward production deployment.\n",
    "\n",
    "> This notebook is designed to run on SageMaker or locally. It loads data from **Redshift** or **S3 Parquet** via the provided `load_data` function in `data_io.py`, applies deterministic preprocessing & feature engineering, validates schema, and writes versioned artifacts for downstream training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db0eae",
   "metadata": {},
   "source": [
    "\n",
    "## 📦 What You Get\n",
    "- Config-first **data loader** (Redshift or S3 Parquet) using `load_data()` from `data_io.py`  \n",
    "- **Cleaning & feature engineering** examples\n",
    "- **Leakage-aware** train/val/test splits → `splits.json`  \n",
    "- Export **processed dataset** → Parquet (optionally partitioned)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68593dbc",
   "metadata": {},
   "source": [
    "\n",
    "## 🧰 Prerequisites\n",
    "- Python 3.9+\n",
    "- Packages: `pandas`, `numpy`, `pyarrow`, `scikit-learn`, `mlflow` (optional), `sqlalchemy`, `redshift_connector`, `s3fs`\n",
    "- A `data_io.py` next to this notebook containing the provided `load_data(...)` implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a fresh environment, uncomment as needed (SageMaker kernels usually have most of these)\n",
    "# %pip install pandas numpy pyarrow scikit-learn mlflow sqlalchemy redshift_connector s3fs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55458e",
   "metadata": {},
   "source": [
    "\n",
    "## ♻️ Reproducibility & Environment Capture\n",
    "- **Fixed seeds** for deterministic results.\n",
    "- Capture **package versions** for traceability.\n",
    "- All artifacts are written under a **run folder** with a unique timestamp/hash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0759c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, json, time, hashlib, platform, random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fixed seeds for full determinism\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n",
    "\n",
    "ARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/run_{RUN_TS}_{RUN_ID}\")\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "env_info = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"timestamp_utc\": RUN_TS,\n",
    "    \"seed\": SEED,\n",
    "    \"packages\": {\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"env_info.json\"), \"w\") as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "env_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f9f1b",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ Configuration\n",
    "Single source of truth for inputs, outputs, and behavior. Switch **source** between `\"redshift\"` and `\"parquet\"` using this cell only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        # choose 'redshift' or 'parquet'\n",
    "        \"source\": os.environ.get(\"SOURCE\", \"redshift\"),\n",
    "        # parquet URI if using S3 parquet (supports wildcards)\n",
    "        \"parquet_uri\": os.environ.get(\"PARQUET_URI\", \"s3://your-bucket/path/to/data/*.parquet\"),\n",
    "        # SQL for Redshift (must be deterministic if you sample!)\n",
    "        \"sql\": os.environ.get(\"SQL\", \"SELECT * FROM your_schema.your_table ORDER BY id\"),\n",
    "        # Redshift connection kwargs (fill via env vars or IAM role inside SageMaker)\n",
    "        \"redshift_kwargs\": {\n",
    "            \"host\": os.environ.get(\"REDSHIFT_HOST\", \"example.your-cluster.redshift.amazonaws.com\"),\n",
    "            \"database\": os.environ.get(\"REDSHIFT_DB\", \"dev\"),\n",
    "            \"user\": os.environ.get(\"REDSHIFT_USER\", \"username\"),\n",
    "            \"password\": os.environ.get(\"REDSHIFT_PASSWORD\", \"password\"),\n",
    "            \"port\": int(os.environ.get(\"REDSHIFT_PORT\", \"5439\")),\n",
    "        },\n",
    "        # Optional: limit rows deterministically for dev runs (None = no limit)\n",
    "        \"row_limit\": int(os.environ.get(\"ROW_LIMIT\", \"50000\")),\n",
    "    },\n",
    "    \"columns\": {\n",
    "        # Define a target column if already known (can be None and set later)\n",
    "        \"target\": os.environ.get(\"TARGET\", \"churned\"),\n",
    "\n",
    "        # Optional: primary key for splits and deterministic joins\n",
    "        \"primary_key\": os.environ.get(\"PRIMARY_KEY\", \"customer_id\"),\n",
    "    },\n",
    "    \"processing\": {\n",
    "        \"stratify_splits\": True,       # set to False for regression\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.1,\n",
    "        \"dropna_threshold_ratio\": 0.95,  # drop columns with >5% missing if needed\n",
    "        \"cap_outliers_iqr\": True,\n",
    "        \"normalize_categoricals\": True,\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"artifact_dir\": ARTIFACT_DIR,\n",
    "        \"processed_parquet_path\": str(Path(ARTIFACT_DIR) / \"processed\" / \"dataset.parquet\"),\n",
    "        \"feature_schema_path\": str(Path(ARTIFACT_DIR) / \"feature_schema.json\"),\n",
    "        \"splits_path\": str(Path(ARTIFACT_DIR) / \"splits.json\"),\n",
    "    },\n",
    "    \"mlflow\": {\n",
    "        \"enabled\": False,\n",
    "        \"tracking_uri\": os.environ.get(\"MLFLOW_TRACKING_URI\", \"\"),\n",
    "        \"experiment_name\": os.environ.get(\"MLFLOW_EXPERIMENT\", \"data-prep\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fda91a",
   "metadata": {},
   "source": [
    "\n",
    "## 📥 Load Data (Redshift or S3 Parquet)\n",
    "Uses `load_data()` defined in `data_io.py`:\n",
    "\n",
    "```python\n",
    "df = load_data(\n",
    "    source=CONFIG[\"data\"][\"source\"],\n",
    "    uri=CONFIG[\"data\"][\"parquet_uri\"],\n",
    "    sql=CONFIG[\"data\"][\"sql\"],\n",
    "    redshift_kwargs=CONFIG[\"data\"][\"redshift_kwargs\"]\n",
    ")\n",
    "```\n",
    "If `data_io.py` is not found, we fall back to a **synthetic demo dataset** so the rest of the pipeline remains testable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to import the provided load_data function from data_io.py\n",
    "load_data = None\n",
    "try:\n",
    "    from data_io import load_data  # expects file next to this notebook\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not import `load_data` from data_io.py. Using synthetic demo data. Error:\", repr(e))\n",
    "\n",
    "def _demo_dataset(n=5000, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"customer_id\": np.arange(1, n+1),\n",
    "        \"age\": rng.integers(18, 90, size=n),\n",
    "        \"tenure_months\": rng.integers(0, 120, size=n),\n",
    "        \"monthly_charges\": rng.normal(45, 15, size=n).round(2),\n",
    "        \"contract_type\": rng.choice([\"month-to-month\", \"one-year\", \"two-year\"], size=n, p=[0.6, 0.25, 0.15]),\n",
    "        \"payment_method\": rng.choice([\"card\", \"bank\", \"paypal\"], size=n),\n",
    "        \"country\": rng.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n, p=[0.5,0.2,0.2,0.1]),\n",
    "        \"churned\": rng.choice([0,1], size=n, p=[0.78, 0.22]).astype(int),\n",
    "    })\n",
    "    # inject a few anomalies\n",
    "    df.loc[rng.choice(df.index, 20, replace=False), \"monthly_charges\"] = -1.0  # invalid negative price\n",
    "    df.loc[rng.choice(df.index, 30, replace=False), \"age\"] = None             # missing age\n",
    "    return df\n",
    "\n",
    "if load_data:\n",
    "    source = CONFIG[\"data\"][\"source\"]\n",
    "    uri = CONFIG[\"data\"][\"parquet_uri\"]\n",
    "    sql = CONFIG[\"data\"][\"sql\"]\n",
    "    rs_kwargs = CONFIG[\"data\"][\"redshift_kwargs\"]\n",
    "    print(f\"Loading data via data_io.load_data(source={source!r}) ...\")\n",
    "    df_raw = load_data(source=source, uri=uri, sql=sql, redshift_kwargs=rs_kwargs)\n",
    "else:\n",
    "    print(\"Using synthetic dataset for demonstration.\")\n",
    "    df_raw = _demo_dataset(n=CONFIG[\"data\"][\"row_limit\"] or 5000)\n",
    "\n",
    "# Optional row limit for dev runs (deterministic by ORDER + head or sample with fixed seed)\n",
    "row_limit = CONFIG[\"data\"][\"row_limit\"]\n",
    "if row_limit and len(df_raw) > row_limit:\n",
    "    # deterministic subset if primary key exists and ordering is stable\n",
    "    pk = CONFIG[\"columns\"][\"primary_key\"]\n",
    "    if pk in df_raw.columns:\n",
    "        df_raw = df_raw.sort_values(pk).head(row_limit).reset_index(drop=True)\n",
    "    else:\n",
    "        df_raw = df_raw.sample(n=row_limit, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "df_raw.head(), df_raw.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caa318",
   "metadata": {},
   "source": [
    "\n",
    "## 🔎 Quick Profile\n",
    "Lightweight overview to understand data types, nulls, and basic distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    \"shape\": df_raw.shape,\n",
    "    \"dtypes\": df_raw.dtypes.astype(str).to_dict(),\n",
    "    \"null_counts\": df_raw.isna().sum().to_dict(),\n",
    "    \"sample_rows\": 5,\n",
    "}\n",
    "pd.DataFrame({\n",
    "    \"column\": df_raw.columns,\n",
    "    \"dtype\": df_raw.dtypes.astype(str).values,\n",
    "    \"nulls\": [df_raw[c].isna().sum() for c in df_raw.columns],\n",
    "    \"non_nulls\": [df_raw[c].notna().sum() for c in df_raw.columns],\n",
    "}).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacb588",
   "metadata": {},
   "source": [
    "\n",
    "## 🧾 Feature Schema (Draft)\n",
    "Define **types, nullability, and basic constraints**. This schema will be exported to JSON and should be reviewed by ML + DevOps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20010af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "target_col = CONFIG[\"columns\"][\"target\"]\n",
    "primary_key = CONFIG[\"columns\"][\"primary_key\"]\n",
    "\n",
    "def infer_basic_schema(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    schema = {}\n",
    "    for c in df.columns:\n",
    "        col_dtype = str(df[c].dtype)\n",
    "        col = {\n",
    "            \"dtype\": col_dtype,\n",
    "            \"nullable\": bool(df[c].isna().any()),\n",
    "        }\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            # robust min/max ignoring NaNs and infs\n",
    "            finite_vals = pd.to_numeric(df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if len(finite_vals):\n",
    "                col[\"min\"] = float(finite_vals.min())\n",
    "                col[\"max\"] = float(finite_vals.max())\n",
    "        else:\n",
    "            # capture a sample of categories for non-numeric cols\n",
    "            col[\"example_values\"] = df[c].dropna().astype(str).unique()[:20].tolist()\n",
    "        schema[c] = col\n",
    "    schema_meta = {\n",
    "        \"_meta\": {\n",
    "            \"target\": target_col if target_col in df.columns else None,\n",
    "            \"primary_key\": primary_key if primary_key in df.columns else None,\n",
    "            \"generated_at\": RUN_TS,\n",
    "            \"seed\": SEED,\n",
    "        },\n",
    "        \"columns\": schema\n",
    "    }\n",
    "    return schema_meta\n",
    "\n",
    "feature_schema = infer_basic_schema(df_raw)\n",
    "\n",
    "# write to artifacts\n",
    "schema_path = CONFIG[\"output\"][\"feature_schema_path\"]\n",
    "os.makedirs(Path(schema_path).parent, exist_ok=True)\n",
    "with open(schema_path, \"w\") as f:\n",
    "    json.dump(feature_schema, f, indent=2)\n",
    "\n",
    "schema_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77e22d",
   "metadata": {},
   "source": [
    "\n",
    "## 🧼 Cleaning\n",
    "- Handle missing values\n",
    "- Normalize categoricals\n",
    "- Fix invalid values (e.g., negative prices)\n",
    "- Optional: Outlier capping via IQR\n",
    "> **Stable rule sets** are critical—avoid ad-hoc fixes. All transformations must be deterministic and versioned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0425fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Example: type coercions (id as string to preserve leading zeros, etc.)\n",
    "if primary_key in df.columns:\n",
    "    df[primary_key] = df[primary_key].astype(str)\n",
    "\n",
    "# Example: fix invalid negatives in 'monthly_charges' if present\n",
    "if \"monthly_charges\" in df.columns:\n",
    "    df.loc[df[\"monthly_charges\"] < 0, \"monthly_charges\"] = np.nan\n",
    "\n",
    "# Missing value strategy (simple example; in practice consider smarter imputers)\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != target_col]\n",
    "cat_cols = [c for c in df.columns if (not pd.api.types.is_numeric_dtype(df[c])) and c not in [target_col]]\n",
    "\n",
    "for c in num_cols:\n",
    "    median_val = df[c].median()\n",
    "    df[c] = df[c].fillna(median_val)\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna(\"__MISSING__\")\n",
    "\n",
    "# Normalize categoricals (lowercase/trim) for consistency\n",
    "if bool(CONFIG[\"processing\"][\"normalize_categoricals\"]):\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Optional IQR capping for outliers on numeric cols\n",
    "if bool(CONFIG[\"processing\"][\"cap_outliers_iqr\"]):\n",
    "    for c in num_cols:\n",
    "        q1, q3 = np.percentile(df[c], [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        df[c] = np.clip(df[c], lower, upper)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf23786",
   "metadata": {},
   "source": [
    "\n",
    "## 🧩 Feature Engineering (Examples)\n",
    "- Ratios and interactions\n",
    "- Encoding categoricals for model training (defer heavy encoders to training pipeline)\n",
    "- Date/time features (if present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example engineered features\n",
    "if \"tenure_months\" in df.columns and \"monthly_charges\" in df.columns:\n",
    "    df[\"est_lifetime_value\"] = (df[\"tenure_months\"] * df[\"monthly_charges\"]).round(2)\n",
    "\n",
    "# Simple frequency encoding for categoricals (kept numeric but reproducible)\n",
    "for c in cat_cols:\n",
    "    freq = df[c].value_counts(normalize=True)\n",
    "    df[f\"{c}__freq\"] = df[c].map(freq).astype(float)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a2a8d",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Lightweight Validation\n",
    "Simple checks before export. For production, consider a formal framework (e.g., Great Expectations or pandera).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checks = []\n",
    "\n",
    "# Example: target present and binary (if classification)\n",
    "if (target_col in df.columns) and CONFIG[\"processing\"][\"stratify_splits\"]:\n",
    "    is_binary = set(pd.Series(df[target_col]).dropna().unique()) <= {0,1}\n",
    "    checks.append({\"check\": \"target_binary\", \"passed\": bool(is_binary)})\n",
    "\n",
    "# Example: no nulls in primary key\n",
    "if primary_key in df.columns:\n",
    "    pk_nulls = int(df[primary_key].isna().sum())\n",
    "    checks.append({\"check\": \"no_null_primary_key\", \"passed\": pk_nulls == 0, \"null_count\": pk_nulls})\n",
    "\n",
    "pd.DataFrame(checks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40d147",
   "metadata": {},
   "source": [
    "\n",
    "## ✂️ Train / Validation / Test Split\n",
    "- Deterministic with fixed `random_state`\n",
    "- Stratified if classification (`stratify=True`)\n",
    "- Optionally **time-based** if a timestamp column is provided (edit here if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df886cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = SEED\n",
    "stratify = None\n",
    "if CONFIG[\"processing\"][\"stratify_splits\"] and target_col in df.columns:\n",
    "    stratify = df[target_col]\n",
    "\n",
    "# First split: train+val vs test\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=CONFIG[\"processing\"][\"test_size\"],\n",
    "    random_state=random_state,\n",
    "    stratify=stratify\n",
    ")\n",
    "\n",
    "# Second split: train vs val\n",
    "stratify_train_val = train_val_df[target_col] if stratify is not None else None\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=CONFIG[\"processing\"][\"val_size\"] / (1.0 - CONFIG[\"processing\"][\"test_size\"]),\n",
    "    random_state=random_state,\n",
    "    stratify=stratify_train_val\n",
    ")\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446157fa",
   "metadata": {},
   "source": [
    "\n",
    "## 💾 Write Artifacts\n",
    "- **Processed dataset** (Parquet)\n",
    "- **Feature schema** (`feature_schema.json`)\n",
    "- **Splits** (`splits.json`) with IDs for deterministic reuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "out_path = Path(CONFIG[\"output\"][\"processed_parquet_path\"])\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save full processed dataset (you can also save splits separately if preferred)\n",
    "df.to_parquet(out_path, index=False)\n",
    "\n",
    "# Save split indices by primary key (preferred) or DataFrame indices\n",
    "splits = {}\n",
    "pk = primary_key if primary_key in df.columns else None\n",
    "def ids_of(subdf):\n",
    "    if pk:\n",
    "        return subdf[pk].tolist()\n",
    "    else:\n",
    "        return subdf.index.tolist()\n",
    "\n",
    "splits = {\n",
    "    \"meta\": {\n",
    "        \"seed\": SEED,\n",
    "        \"created_at\": RUN_TS,\n",
    "        \"primary_key\": pk,\n",
    "        \"target\": target_col if target_col in df.columns else None,\n",
    "        \"source\": CONFIG[\"data\"][\"source\"],\n",
    "    },\n",
    "    \"train_ids\": ids_of(train_df),\n",
    "    \"val_ids\": ids_of(val_df),\n",
    "    \"test_ids\": ids_of(test_df),\n",
    "}\n",
    "\n",
    "with open(CONFIG[\"output\"][\"splits_path\"], \"w\") as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "\n",
    "{\n",
    "    \"processed_parquet\": str(out_path),\n",
    "    \"feature_schema\": CONFIG[\"output\"][\"feature_schema_path\"],\n",
    "    \"splits\": CONFIG[\"output\"][\"splits_path\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418ba8e",
   "metadata": {},
   "source": [
    "\n",
    "## 📈 (Optional) MLflow Trace\n",
    "Record data prep parameters and artifacts for lineage. Enable by setting `CONFIG[\"mlflow\"][\"enabled\"] = True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc16d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if CONFIG[\"mlflow\"][\"enabled\"]:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(CONFIG[\"mlflow\"][\"tracking_uri\"] or \"file://\" + str(Path(ARTIFACT_DIR).absolute()))\n",
    "    mlflow.set_experiment(CONFIG[\"mlflow\"][\"experiment_name\"])\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"data-prep-{RUN_TS}\") as run:\n",
    "        mlflow.log_params({\n",
    "            \"seed\": SEED,\n",
    "            \"source\": CONFIG[\"data\"][\"source\"],\n",
    "            \"row_limit\": CONFIG[\"data\"][\"row_limit\"],\n",
    "            \"stratify\": CONFIG[\"processing\"][\"stratify_splits\"],\n",
    "        })\n",
    "        mlflow.log_artifact(CONFIG[\"output\"][\"feature_schema_path\"])\n",
    "        mlflow.log_artifact(CONFIG[\"output\"][\"splits_path\"])\n",
    "        # logging the whole parquet can be large; consider sampling or schema-only\n",
    "        # mlflow.log_artifact(CONFIG[\"output\"][\"processed_parquet_path\"])\n",
    "\n",
    "        print(\"MLflow run:\", run.info.run_id)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
