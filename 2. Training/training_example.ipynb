{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa54fcc",
   "metadata": {},
   "source": [
    "# Week 2 — Pipelines & ETL with Naming Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48930379",
   "metadata": {},
   "source": [
    "**Learning Objectives (Week 2 – Pipelines & ETL)**  \n",
    "- Build a reproducible ETL pipeline from Redshift to feature tables.  \n",
    "- Externalize configuration and adopt consistent naming conventions (ZAP).  \n",
    "- Package pipeline steps for CI and future automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b40e14",
   "metadata": {},
   "source": [
    "## Company Naming Conventions (Propose & Refer Importance)\n",
    "- Pipelines: `zap-ml-etl-<domain>-<purpose>`  \n",
    "- Jobs: `zap-ci-<pipeline>-<step>`  \n",
    "- S3 paths: `s3://zap-ml/<env>/<pipeline>/<artifact>`  \n",
    "- Feature tables: `zap_feature_<entity>_<version>`  \n",
    "> Consistent naming makes CI/CD traceable and eases auditing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61385eb",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Parameterize a pipeline with `config.yaml` (env, data sources, output path).  \n",
    "2. Implement steps: extract (Redshift), transform (clean, encode), load (persist to S3/local).  \n",
    "3. Add simple data validation checks (row counts, schema conformity).  \n",
    "4. Package as a CLI: `python pipelines/etl.py --config config.yaml`.  \n",
    "5. Emit artifacts to `artifacts/week2/` and logs to `logs/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ae9a5",
   "metadata": {},
   "source": [
    "## Peer Validation\n",
    "- **Peer Review Checklist:**  \n",
    "  - Clear naming / paths.  \n",
    "  - Configurable environments.  \n",
    "  - Validation checks implemented.  \n",
    "  - Pipeline script runnable end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas pyyaml boto3 --quiet\n",
    "import os, json, yaml, pandas as pd, numpy as np\n",
    "\n",
    "os.makedirs(\"artifacts/week2\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG_PATH = \"config.yaml\"\n",
    "\n",
    "default_cfg = {\n",
    "  \"env\": \"dev\",\n",
    "  \"sources\": {\n",
    "    \"redshift\": {\n",
    "      \"enabled\": False,\n",
    "      \"query\": \"SELECT * FROM zap_sandbox.transactions LIMIT 10000;\",\n",
    "      \"host\": \"YOUR-REDSHIFT-ENDPOINT\",\n",
    "      \"port\": 5439,\n",
    "      \"database\": \"dev\",\n",
    "      \"user\": \"YOUR-USER\",\n",
    "      \"password\": \"YOUR-PASSWORD\"\n",
    "    },\n",
    "    \"synthetic\": {\n",
    "      \"enabled\": True,\n",
    "      \"n_rows\": 5000\n",
    "    }\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"local_csv\": \"artifacts/week2/features.csv\",\n",
    "    \"s3_path\": \"s3://zap-ml/dev/zap-ml-etl-core/features.csv\"\n",
    "  }\n",
    "}\n",
    "\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH,\"w\") as f:\n",
    "        yaml.safe_dump(default_cfg, f, sort_keys=False)\n",
    "\n",
    "print(f\"Wrote default config to {CONFIG_PATH}. Edit values and re-run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0455286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- ETL Implementation (extract -> transform -> load) ----\n",
    "def extract(cfg):\n",
    "    if cfg[\"sources\"][\"redshift\"][\"enabled\"]:\n",
    "        # %pip install redshift_connector --quiet\n",
    "        import redshift_connector, pandas as pd\n",
    "        conn = redshift_connector.connect(\n",
    "            host=cfg[\"sources\"][\"redshift\"][\"host\"],\n",
    "            port=cfg[\"sources\"][\"redshift\"][\"port\"],\n",
    "            database=cfg[\"sources\"][\"redshift\"][\"database\"],\n",
    "            user=cfg[\"sources\"][\"redshift\"][\"user\"],\n",
    "            password=cfg[\"sources\"][\"redshift\"][\"password\"],\n",
    "        )\n",
    "        df = pd.read_sql(cfg[\"sources\"][\"redshift\"][\"query\"], conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    else:\n",
    "        n = cfg[\"sources\"][\"synthetic\"][\"n_rows\"]\n",
    "        df = pd.DataFrame({\n",
    "            \"amount\": np.random.gamma(2.0, 50.0, n),\n",
    "            \"country\": np.random.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n),\n",
    "            \"channel\": np.random.choice([\"web\",\"store\",\"mobile\"], size=n),\n",
    "            \"label\": np.random.choice([0,1], size=n, p=[0.7,0.3])\n",
    "        })\n",
    "        return df\n",
    "\n",
    "def transform(df):\n",
    "    df = df.copy()\n",
    "    # Simple cleaning\n",
    "    df = df.dropna()\n",
    "    # Example encoding\n",
    "    df = pd.get_dummies(df, columns=[\"country\",\"channel\"], drop_first=True)\n",
    "    return df\n",
    "\n",
    "def validate(df):\n",
    "    assert len(df) > 0, \"Empty dataframe after transform\"\n",
    "    assert \"label\" in df.columns, \"Missing target column 'label'\"\n",
    "    return True\n",
    "\n",
    "def load(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "with open(\"config.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "df_raw = extract(cfg)\n",
    "df_feat = transform(df_raw)\n",
    "validate(df_feat)\n",
    "out_path = cfg[\"outputs\"][\"local_csv\"]\n",
    "load(df_feat, out_path)\n",
    "print(f\"Saved features to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
