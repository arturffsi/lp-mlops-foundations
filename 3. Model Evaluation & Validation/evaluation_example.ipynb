{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\udcca Model Evaluation \u2014 MLOps Pipeline (Example)\n\n**Goal:** provide a *production-ready* evaluation workflow you can run locally or in SageMaker to validate model quality **before** promotion and **after** deployment.\n\n### Why evaluation matters (theory, briefly)\n- **Ensures Production Readiness:** confirms predictive performance meets SLAs before and after deployment.\n- **Drives Model Improvement:** highlights where performance drops (e.g., low recall on minority cohorts) \u2192 informs retraining or feature work.\n- **Maintains Model Health:** periodic checks catch **data drift**, **concept drift**, and **bias** that can emerge post-deployment.\n- **Traceability & Governance:** metrics and artifacts are captured with **MLflow** and can be associated with **SageMaker Model Registry** entries.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddf0 Prereqs\n- Python 3.9+\n- Packages: `pandas`, `numpy`, `scikit-learn`, `mlflow` (optional), `catboost` (or fallback to `sklearn`), `sagemaker`, `boto3`\n- A `data_io.py` (for loading from Redshift or S3 Parquet). If not present, a **synthetic dataset** will be used.\n\n> If your environment is fresh, uncomment the cell below.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# %pip install pandas numpy scikit-learn mlflow catboost boto3 sagemaker s3fs pyarrow sqlalchemy redshift_connector"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# \u267b\ufe0f Reproducibility & environment capture\nimport os, sys, json, hashlib, random, platform\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\n\nRUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\nRUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n\nARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/eval_{RUN_TS}_{RUN_ID}\")\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\nenv_info = {\n    \"python\": sys.version,\n    \"platform\": platform.platform(),\n    \"timestamp_utc\": RUN_TS,\n    \"seed\": SEED,\n}\nwith open(os.path.join(ARTIFACT_DIR, \"env_eval_info.json\"), \"w\") as f:\n    json.dump(env_info, f, indent=2)\n\nenv_info"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2699\ufe0f Configuration\nSet all evaluation knobs here (data source, target column, model params, CV strategy, thresholds, etc.)."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "CONFIG = {\n    \"data\": {\n        # choose: \"parquet\" or \"redshift\"\n        \"source\": os.environ.get(\"SOURCE\", \"parquet\"),\n        \"parquet_uri\": os.environ.get(\"PARQUET_URI\", \"s3://your-bucket/path/to/processed/*.parquet\"),  # edit if using S3\n        \"redshift_sql\": os.environ.get(\"SQL\", \"SELECT * FROM your_schema.your_table ORDER BY id\"),\n        \"redshift_kwargs\": {\n            \"host\": os.environ.get(\"REDSHIFT_HOST\", \"example.redshift.amazonaws.com\"),\n            \"database\": os.environ.get(\"REDSHIFT_DB\", \"dev\"),\n            \"user\": os.environ.get(\"REDSHIFT_USER\", \"username\"),\n            \"password\": os.environ.get(\"REDSHIFT_PASSWORD\", \"password\"),\n            \"port\": int(os.environ.get(\"REDSHIFT_PORT\", \"5439\")),\n        },\n    },\n    \"features\": {\n        \"id_features\": [\n            # Put identifiers or known leakage columns here to drop before training/eval\n            \"customer_id\", \"contract_id\", \"account_id\"\n        ]\n    },\n    \"model\": {\n        \"target_col\": os.environ.get(\"TARGET\", \"churned\"),\n        \"random_seed\": 42,\n        # CatBoost-style params (used if CatBoost available; otherwise a sklearn fallback is used)\n        \"n_estimators\": 1200,\n        \"learning_rate\": 0.08,\n        \"depth\": 6,\n        \"l2_leaf_reg\": 3.0,\n        \"auto_class_weights\": \"Balanced\",\n    },\n    \"evaluation\": {\n        \"cv_folds\": 5,\n        \"cv_strategy\": \"stratified\",  # stratified | timeseries\n        \"opt_target_recall\": 0.80,    # threshold selection target\n        \"use_mlflow\": False,          # toggle MLflow logging on/off\n    },\n    \"output\": {\n        \"artifact_dir\": ARTIFACT_DIR,\n        \"cv_summary_path\": os.path.join(ARTIFACT_DIR, \"cv_summary.json\"),\n        \"holdout_report_path\": os.path.join(ARTIFACT_DIR, \"holdout_report.json\"),\n    }\n}\n\nCONFIG"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce5 Load Data\nUses `load_data` from `data_io.py` if available; otherwise a **synthetic churn** dataset is generated so this notebook remains runnable."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Try loading utility\nload_data = None\ntry:\n    from data_io import load_data  # expects a function load_data(source, uri, sql, redshift_kwargs)\nexcept Exception as e:\n    print(\"\u26a0\ufe0f data_io.load_data not found. Falling back to synthetic demo. Error:\", repr(e))\n\ndef _demo_dataset(n=12000, seed=SEED):\n    rng = np.random.default_rng(seed)\n    df = pd.DataFrame({\n        \"customer_id\": np.arange(1, n+1),\n        \"age\": rng.integers(18, 85, size=n),\n        \"tenure_months\": rng.integers(0, 120, size=n),\n        \"monthly_charges\": rng.normal(45, 15, size=n).round(2),\n        \"contract_type\": rng.choice([\"month-to-month\",\"one-year\",\"two-year\"], size=n, p=[0.6,0.25,0.15]),\n        \"country\": rng.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n, p=[0.5,0.2,0.2,0.1]),\n        \"signup_ts\": pd.to_datetime(\"2022-01-01\") + pd.to_timedelta(rng.integers(0, 900, size=n), unit=\"D\"),\n        \"churned\": rng.choice([0,1], size=n, p=[0.78,0.22]).astype(int),\n    })\n    # add a few anomalies\n    df.loc[rng.choice(df.index, 40, replace=False), \"monthly_charges\"] = -5.0\n    df.loc[rng.choice(df.index, 60, replace=False), \"age\"] = None\n    return df\n\nif load_data:\n    if CONFIG[\"data\"][\"source\"] == \"parquet\":\n        df = load_data(source=\"parquet\", uri=CONFIG[\"data\"][\"parquet_uri\"], sql=None, redshift_kwargs=None)\n    else:\n        df = load_data(source=\"redshift\", uri=None, sql=CONFIG[\"data\"][\"redshift_sql\"], redshift_kwargs=CONFIG[\"data\"][\"redshift_kwargs\"])\nelse:\n    df = _demo_dataset()\n\nprint(df.shape)\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddea Training Utils (fallbacks)\nWe try to import `training_utils` (your project helpers). If not present, we define minimal fallbacks here to keep the notebook runnable."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Attempt to import project utilities; else define minimal versions\ntry:\n    from training_utils import (\n        setup_mlflow_tracking, preprocess_data, create_catboost_model, train_and_evaluate_model\n    )\n    HAVE_TRAINING_UTILS = True\nexcept Exception as e:\n    print(\"\u2139\ufe0f training_utils not found; using minimal fallbacks. Error:\", repr(e))\n    HAVE_TRAINING_UTILS = False\n\nimport warnings\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score, precision_recall_curve,\n    classification_report, confusion_matrix, precision_score, recall_score, f1_score\n)\n\ndef _simple_preprocess(df, config):\n    # Clean simple issues; preserve determinism\n    target = config[\"model\"][\"target_col\"]\n    df = df.copy()\n    # replace invalid negatives in monthly_charges\n    if \"monthly_charges\" in df.columns:\n        df.loc[df[\"monthly_charges\"] < 0, \"monthly_charges\"] = np.nan\n        df[\"monthly_charges\"] = df[\"monthly_charges\"].fillna(df[\"monthly_charges\"].median())\n    # fill simple categoricals\n    for c in df.columns:\n        if c == target: continue\n        if df[c].dtype == object:\n            df[c] = df[c].fillna(\"__MISSING__\").astype(str)\n        elif pd.api.types.is_numeric_dtype(df[c]):\n            df[c] = df[c].fillna(df[c].median())\n        elif hasattr(df[c], \"dtype\") and str(df[c].dtype).startswith(\"datetime\"):\n            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n    return df\n\ndef _make_model(config):\n    # Try CatBoost; else fallback to sklearn RandomForest\n    try:\n        from catboost import CatBoostClassifier\n        model = CatBoostClassifier(\n            iterations=config[\"model\"][\"n_estimators\"],\n            learning_rate=config[\"model\"][\"learning_rate\"],\n            depth=config[\"model\"][\"depth\"],\n            l2_leaf_reg=config[\"model\"][\"l2_leaf_reg\"],\n            loss_function=\"Logloss\",\n            eval_metric=\"AUC\",\n            random_seed=config[\"model\"][\"random_seed\"],\n            auto_class_weights=config[\"model\"][\"auto_class_weights\"],\n            verbose=False\n        )\n        return model, \"catboost\"\n    except Exception:\n        from sklearn.ensemble import RandomForestClassifier\n        model = RandomForestClassifier(\n            n_estimators=300, random_state=config[\"model\"][\"random_seed\"], class_weight=\"balanced\"\n        )\n        return model, \"sklearn-rf\"\n\ndef _train_eval(model, X_train, y_train, X_val, y_val, target_recall=0.80, cat_idx=None):\n    # Fit and evaluate\n    import numpy as np\n    import pandas as pd\n    try:\n        # Encode object columns deterministically\n        if any(getattr(X_train[c], \"dtype\", None) == object for c in X_train.columns):\n            X_train_enc = X_train.copy()\n            X_val_enc = X_val.copy()\n            for c in X_train_enc.columns:\n                if X_train_enc[c].dtype == object:\n                    vals = pd.concat([X_train_enc[c], X_val_enc[c]], axis=0).astype(str)\n                    mapping = {v:i for i, v in enumerate(pd.Series(vals).astype(str).unique())}\n                    X_train_enc[c] = X_train_enc[c].map(mapping).fillna(-1).astype(int)\n                    X_val_enc[c] = X_val_enc[c].map(mapping).fillna(-1).astype(int)\n        else:\n            X_train_enc, X_val_enc = X_train, X_val\n        model.fit(X_train_enc, y_train)\n        if hasattr(model, \"predict_proba\"):\n            proba = model.predict_proba(X_val_enc)[:,1]\n        else:\n            scores = model.decision_function(X_val_enc)\n            proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n    except Exception as e:\n        import warnings\n        warnings.warn(f\"Training failed: {e}\")\n        raise\n\n    from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, precision_score, recall_score\n    roc = roc_auc_score(y_val, proba)\n    pr_auc = average_precision_score(y_val, proba)\n    prec, rec, thr = precision_recall_curve(y_val, proba)  # len(thr)=len(prec)-1\n\n    # best F1\n    f1s = (2*prec[:-1]*rec[:-1])/(prec[:-1]+rec[:-1]+1e-12)\n    i_best = int(np.argmax(f1s))\n    thr_best = float(thr[i_best])\n    y_hat_best = (proba >= thr_best).astype(int)\n\n    # threshold at target recall (choose highest threshold reaching recall >= target)\n    idx = np.where(rec[:-1] >= target_recall)[0]\n    if len(idx):\n        i_t = int(idx[-1])\n        thr_rec = float(thr[i_t])\n    else:\n        thr_rec = 0.0\n        i_t = 0\n    y_hat_rec = (proba >= thr_rec).astype(int)\n\n    metrics = {\n        \"roc_auc\": float(roc),\n        \"pr_auc\": float(pr_auc),\n        \"best_f1\": float(f1s[i_best]),\n        \"best_f1_threshold\": thr_best,\n        \"precision_at_best_f1\": float(precision_score(y_val, y_hat_best, zero_division=0)),\n        \"recall_at_best_f1\": float(recall_score(y_val, y_hat_best, zero_division=0)),\n        \"threshold_at_target_recall\": thr_rec,\n        \"precision_at_target_recall\": float(prec[i_t]),\n        \"recall_at_target_recall\": float(rec[i_t]),\n    }\n    return metrics, proba, {\"thr_best\": thr_best, \"thr_rec\": thr_rec}\n\n# Bind fallbacks if needed\nif not HAVE_TRAINING_UTILS:\n    setup_mlflow_tracking = lambda cfg: None\n    preprocess_data = _simple_preprocess\n    create_catboost_model = lambda cfg: _make_model(cfg)[0]\n    def train_and_evaluate_model(model, train_pool, val_pool, config, X_train, X_val, y_train, y_val):\n        return _train_eval(model, X_train, y_train, X_val, y_val, target_recall=CONFIG[\"evaluation\"][\"opt_target_recall\"])\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udd01 Cross-Validation\nEvaluate generalization with **StratifiedKFold** (classification) or **TimeSeriesSplit** (temporal). Thresholds are chosen at **target recall** to reflect production priorities (e.g., catching churners)."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\nimport numpy as np\n\ndef cross_validate_model(df, config, cv_folds=5, cv_strategy='stratified', use_mlflow=False):\n    TARGET_COL = config['model']['target_col']\n    results = {\"fold_results\": []}\n\n    id_features = config['features']['id_features']\n    cols_to_drop = [c for c in id_features if c in df.columns]\n    df_clean = df.drop(columns=cols_to_drop).copy()\n\n    # Remove datetime columns (CatBoost Pools support them only via special handling)\n    dt_cols = [c for c in df_clean.columns if np.issubdtype(df_clean[c].dtype, np.datetime64)]\n    if dt_cols:\n        df_clean = df_clean.drop(columns=dt_cols)\n\n    df_clean = preprocess_data(df_clean, config)\n\n    y = df_clean[TARGET_COL].astype(int).values\n    X = df_clean.drop(columns=[TARGET_COL])\n\n    if cv_strategy == \"timeseries\":\n        splitter = TimeSeriesSplit(n_splits=cv_folds)\n    else:\n        splitter = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=config['model']['random_seed'])\n\n    print(f\"Starting {cv_folds}-fold CV with strategy: {cv_strategy}\")\n    for fold, (tr_idx, va_idx) in enumerate(splitter.split(X, y), 1):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y[tr_idx], y[va_idx]\n\n        model = create_catboost_model(config)\n        fold_metrics, proba, thr = train_and_evaluate_model(model, None, None, config, X_tr, X_va, y_tr, y_va)\n        results[\"fold_results\"].append({\n            \"fold\": fold,\n            **fold_metrics,\n            \"train_samples\": int(len(X_tr)),\n            \"val_samples\": int(len(X_va)),\n        })\n\n        print(f\"Fold {fold}: ROC-AUC={fold_metrics['roc_auc']:.4f}  F1={fold_metrics['best_f1']:.4f}  \"\n              f\"Recall@target={fold_metrics['recall_at_target_recall']:.4f}  \"\n              f\"Prec@target={fold_metrics['precision_at_target_recall']:.4f}\")\n\n    # Aggregate\n    agg = {}\n    for key in [\"roc_auc\",\"pr_auc\",\"best_f1\",\"precision_at_target_recall\",\"recall_at_target_recall\"]:\n        vals = [r[key] for r in results[\"fold_results\"]]\n        agg[f\"mean_{key}\"] = float(np.mean(vals))\n        agg[f\"std_{key}\"]  = float(np.std(vals))\n    results.update(agg)\n    return results, df_clean\n\ncv_results, df_clean = cross_validate_model(\n    df, CONFIG, cv_folds=CONFIG[\"evaluation\"][\"cv_folds\"],\n    cv_strategy=CONFIG[\"evaluation\"][\"cv_strategy\"],\n    use_mlflow=CONFIG[\"evaluation\"][\"use_mlflow\"]\n)\ncv_results"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udcdc CV Summary"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import json, pprint, os\npprint.pprint(cv_results)\nwith open(CONFIG[\"output\"][\"cv_summary_path\"], \"w\") as f:\n    json.dump(cv_results, f, indent=2)\nCONFIG[\"output\"][\"cv_summary_path\"]"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddea Extra: Holdout / Temporal Backtest (Optional)\nSimulate **pre-prod** evaluation with a strict temporal split to approximate future data."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n\nTARGET_COL = CONFIG[\"model\"][\"target_col\"]\n\n# Use signup_ts if available; else random split (deterministic)\nif \"signup_ts\" in df_clean.columns:\n    ts = pd.to_datetime(df_clean[\"signup_ts\"], errors=\"coerce\")\n    cutoff = ts.quantile(0.8)\n    tr = df_clean[ts < cutoff].copy()\n    te = df_clean[ts >= cutoff].copy()\nelse:\n    tr, te = train_test_split(df_clean, test_size=0.2, random_state=CONFIG[\"model\"][\"random_seed\"], stratify=df_clean[TARGET_COL])\n\nX_tr, y_tr = tr.drop(columns=[TARGET_COL]), tr[TARGET_COL].values\nX_te, y_te = te.drop(columns=[TARGET_COL]), te[TARGET_COL].values\n\nmdl = create_catboost_model(CONFIG)\nmetrics_tr, _, thr = train_and_evaluate_model(mdl, None, None, CONFIG, X_tr, X_te, y_tr, y_te)\n\nholdout_report = {\n    \"n_train\": int(len(tr)),\n    \"n_test\": int(len(te)),\n    **metrics_tr\n}\n\nimport json, os\nwith open(CONFIG[\"output\"][\"holdout_report_path\"], \"w\") as f:\n    json.dump(holdout_report, f, indent=2)\n\nholdout_report"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfe SageMaker Model Registry & Experiments (Browse)\nUse this section to **discover past experiments, tuning jobs, and registered models** to compare against your current run.\n\n> You may need AWS credentials & permissions. These cells are **safe** to run locally (they will no-op without creds)."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, datetime as dt\ntry:\n    import boto3, sagemaker\n    from sagemaker.analytics import ExperimentAnalytics\n    sm = boto3.client(\"sagemaker\")\n    sess = sagemaker.Session()\n    print(\"AWS Region:\", boto3.Session().region_name)\n\n    # --- Experiments (adjust filters as needed) ---\n    print(\"\\nRecent Experiments (last 30 days):\")\n    res = sm.list_experiments(\n        SortBy=\"CreationTime\", SortOrder=\"Descending\",\n        MaxResults=10\n    )\n    for e in res.get(\"ExperimentSummaries\", []):\n        print(\" -\", e[\"ExperimentName\"], \"| Created:\", e[\"CreationTime\"])\n\n    # --- Models in Registry ---\n    print(\"\\nModel Packages in Registry (latest 10):\")\n    mres = sm.list_model_packages(\n        ModelPackageGroupNameContains=\"\",\n        SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=10\n    )\n    for mp in mres.get(\"ModelPackageSummaryList\", []):\n        print(\" - Group:\", mp.get(\"ModelPackageGroupName\"), \"| Status:\", mp.get(\"ModelApprovalStatus\"),\n              \"| Created:\", mp.get(\"CreationTime\"))\n\n    # --- Training jobs (latest) ---\n    print(\"\\nRecent Training Jobs (latest 10):\")\n    tres = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=10)\n    for tj in tres.get(\"TrainingJobSummaries\", []):\n        print(\" -\", tj[\"TrainingJobName\"], \"| Status:\", tj[\"TrainingJobStatus\"], \"| Created:\", tj[\"CreationTime\"])\n\nexcept Exception as e:\n    print(\"\u2139\ufe0f Skipping registry/experiments browse (likely missing AWS creds):\", e)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2705 What to do with these results\n- **Gate for promotion:** enforce thresholds (e.g., mean Recall \u2265 0.80 at chosen threshold, CI bounds acceptable).\n- **Compare to registry:** if current run underperforms the **approved** model, block promotion.\n- **Open issues:** if drift/bias suspected, trigger data pipeline review and schedule retraining.\n- **Log everything:** ship `cv_summary.json` and `holdout_report.json` to your artifact store/MLflow and link it to Model Registry entries.\n"}]}