{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\udee1\ufe0f Model Validation \u2014 **User Exercise** (SageMaker-ready)\n\n**Goal:** decide if a candidate model is promotable by enforcing **quality gates**, comparing to the **Approved champion** in **SageMaker Model Registry**, and verifying **artifacts** are complete.\n\n> Every place you should edit is marked with **`# <- TODO \u270f\ufe0f`**.\n\n### Why validation?\n- **Ensures Production Readiness** \u2014 confirm predictive performance meets SLAs **before & after** deployment.\n- **Drives Model Improvement** \u2014 identify failure modes that steer **feature engineering** & **retraining**.\n- **Maintains Model Health** \u2014 catch **bias**, **data drift**, and **concept drift**; block risky promotions.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddf0 Prerequisites\nUncomment if your kernel is missing packages (Studio often has most already):"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# %pip install pandas numpy scikit-learn boto3 sagemaker mlflow s3fs pyarrow catboost xgboost lightgbm sqlalchemy redshift_connector"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udeaa SageMaker Studio Bootstrap (safe locally)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, boto3\ntry:\n    import sagemaker\n    sm_sess = sagemaker.Session()\n    _region = boto3.Session().region_name\n    try:\n        _role = sagemaker.get_execution_role()\n    except Exception:\n        _role = \"unknown-role\"\n    _bucket = sm_sess.default_bucket()\n    print(\"\u2705 SageMaker context\")\n    print(\" Region:\", _region)\n    print(\" Role:  \", _role)\n    print(\" Bucket:\", _bucket)\n    os.environ.setdefault(\"AWS_REGION\", _region or \"\")\n    os.environ.setdefault(\"SM_DEFAULT_BUCKET\", _bucket or \"\")\nexcept Exception as e:\n    print(\"\u2139\ufe0f Running without SageMaker context. Reason:\", e)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u267b\ufe0f Reproducibility & Environment Capture"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import sys, json, hashlib, random, platform\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nRUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\nRUN_ID = hashlib.sha1(f\"val-{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n\nARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/validation_{RUN_TS}_{RUN_ID}\")\nPath(ARTIFACT_DIR).mkdir(parents=True, exist_ok=True)\n\nenv_info = {\"python\": sys.version, \"platform\": platform.platform(), \"timestamp_utc\": RUN_TS, \"seed\": SEED}\nwith open(Path(ARTIFACT_DIR)/\"env_validation_info.json\", \"w\") as f:\n    json.dump(env_info, f, indent=2)\n\nenv_info"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2699\ufe0f Configuration \u2014 **edit here**\nAll knobs in one place. Look for **`# <- TODO \u270f\ufe0f`** to customize metrics, artifacts, and promotion criteria."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from pathlib import Path\n\nCONFIG = {\n    \"data\": {\n        \"source\": os.environ.get(\"SOURCE\", \"parquet\"),  # \"parquet\" | \"redshift\"\n        \"parquet_uri\": os.environ.get(\"PARQUET_URI\", \"s3://YOUR-BUCKET/path/*.parquet\"),   # <- TODO \u270f\ufe0f set your S3 path\n        \"redshift_sql\": os.environ.get(\"SQL\", \"SELECT * FROM your_schema.your_table ORDER BY id\"),  # <- TODO \u270f\ufe0f if using Redshift\n        \"redshift_kwargs\": {\n            \"host\": os.environ.get(\"REDSHIFT_HOST\", \"example.redshift.amazonaws.com\"),\n            \"database\": os.environ.get(\"REDSHIFT_DB\", \"dev\"),\n            \"user\": os.environ.get(\"REDSHIFT_USER\", \"username\"),\n            \"password\": os.environ.get(\"REDSHIFT_PASSWORD\", \"password\"),\n            \"port\": int(os.environ.get(\"REDSHIFT_PORT\", \"5439\")),\n        },\n        \"target_col\": os.environ.get(\"TARGET\", \"churned\"),                                   # <- TODO \u270f\ufe0f set your target\n        \"id_features\": [\"customer_id\",\"contract_id\",\"account_id\"],                         # <- TODO \u270f\ufe0f drop IDs/leakage cols\n        \"time_col\": os.environ.get(\"TIME_COL\", \"signup_ts\"),                                  # <- TODO \u270f\ufe0f None if N/A\n    },\n    \"modeling\": {\n        \"choice\": os.environ.get(\"MODEL_CHOICE\",\"random_forest\"),  # <- TODO \u270f\ufe0f 'random_forest' | 'catboost' | 'xgboost'\n        # Example hyperparameters for CatBoost (edit if you pick catboost)\n        \"catboost\": {\n            \"iterations\": 800,             # <- TODO \u270f\ufe0f\n            \"learning_rate\": 0.08,         # <- TODO \u270f\ufe0f\n            \"depth\": 6,                    # <- TODO \u270f\ufe0f\n            \"l2_leaf_reg\": 3.0,            # <- TODO \u270f\ufe0f\n            \"loss_function\": \"Logloss\",\n            \"eval_metric\": \"AUC\",\n            \"auto_class_weights\": \"Balanced\",\n            \"verbose\": False\n        },\n        # Example hyperparameters for XGBoost (edit if you pick xgboost)\n        \"xgboost\": {\n            \"n_estimators\": 700,           # <- TODO \u270f\ufe0f\n            \"learning_rate\": 0.08,         # <- TODO \u270f\ufe0f\n            \"max_depth\": 6,                # <- TODO \u270f\ufe0f\n            \"subsample\": 0.8,              # <- TODO \u270f\ufe0f\n            \"colsample_bytree\": 0.8,       # <- TODO \u270f\ufe0f\n            \"reg_lambda\": 1.0,             # <- TODO \u270f\ufe0f\n            \"objective\": \"binary:logistic\",\n            \"eval_metric\": \"auc\",\n            \"tree_method\": \"hist\",\n            \"random_state\": SEED\n        }\n    },\n    \"evaluation\": {\n        # ---- QUALITY GATES (edit thresholds) ----\n        \"cv_folds\": int(os.environ.get(\"CV_FOLDS\",\"5\")),\n        \"cv_strategy\": os.environ.get(\"CV_STRATEGY\",\"stratified\"),  # \"stratified\" | \"timeseries\"\n        \"target_recall\": float(os.environ.get(\"TARGET_RECALL\",\"0.80\")),               # <- TODO \u270f\ufe0f SLA\n        \"stability_std_max\": float(os.environ.get(\"STABILITY_STD_MAX\",\"0.03\")),       # <- TODO \u270f\ufe0f max std(recall)\n        \"min_fold_recall\": float(os.environ.get(\"MIN_FOLD_RECALL\",\"0.75\")),           # <- TODO \u270f\ufe0f per-fold floor\n        \"min_pr_auc\": float(os.environ.get(\"MIN_PR_AUC\",\"0.45\")),                      # <- TODO \u270f\ufe0f optional extra gate\n        # ---- METRICS YOU WANT TO EVALUATE ----\n        \"metrics_to_compute\": [                                                          # <- TODO \u270f\ufe0f add/remove metrics\n            \"roc_auc\", \"pr_auc\", \"recall_at_target\", \"precision_at_target\"\n        ],\n        # ---- PROMOTION CRITERIA vs CHAMPION ----\n        \"champion_metric_key\": os.environ.get(\"CHAMPION_METRIC\",\"mean_recall_at_target\"),  # <- TODO \u270f\ufe0f which metric to compare\n        \"better_than_champion_margin\": float(os.environ.get(\"BETTER_MARGIN\",\"0.00\")),       # <- TODO \u270f\ufe0f require this margin\n        \"read_cv_from\": os.environ.get(\"CV_JSON\",\"\"),   # optional: path to existing cv_summary.json\n    },\n    \"registry\": {\n        \"package_group\": os.environ.get(\"SM_MODEL_PACKAGE_GROUP\",\"churn-model-group\"),      # <- TODO \u270f\ufe0f your Model Package Group\n        \"candidate\": {\n            \"model_tar_path\": os.environ.get(\"MODEL_TAR\",\"model.tar.gz\"),                  # <- TODO \u270f\ufe0f ensure file exists\n            \"inference_script\": os.environ.get(\"INFERENCE_SCRIPT\",\"inference.py\"),         # <- TODO \u270f\ufe0f entrypoint inside tar\n            \"requirements\": os.environ.get(\"REQUIREMENTS\",\"requirements.txt\"),             # <- TODO \u270f\ufe0f runtime deps\n            \"schema_json\": os.environ.get(\"SCHEMA_JSON\", str(Path(ARTIFACT_DIR)/\"feature_schema.json\")),  # <- TODO \u270f\ufe0f\n            \"validation_report\": str(Path(ARTIFACT_DIR)/\"validation_report.json\"),\n        },\n        \"s3_prefix\": os.environ.get(\"ARTIFACTS_S3_PREFIX\", f\"s3://{os.environ.get('SM_DEFAULT_BUCKET','')}/model-validation/{RUN_TS}_{RUN_ID}\"),\n        \"register_if_pass\": os.environ.get(\"REGISTER_IF_PASS\",\"false\").lower() == \"true\",   # <- TODO \u270f\ufe0f set true in CI\n        \"model_approval_status\": os.environ.get(\"APPROVAL_STATUS\",\"PendingManualApproval\"),\n        \"container_image_uri\": os.environ.get(\"CONTAINER_IMAGE\",\"\"),  # <- TODO \u270f\ufe0f custom inference image if needed\n    },\n    \"paths\": {\n        \"artifact_dir\": ARTIFACT_DIR,\n        \"cv_summary_path\": str(Path(ARTIFACT_DIR)/\"cv_summary.json\"),\n        \"validation_report_path\": str(Path(ARTIFACT_DIR)/\"validation_report.json\"),\n    }\n}\n\nprint('Model choice:', CONFIG['modeling']['choice'])\nCONFIG"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce5 Load Data (Redshift or S3 Parquet)\nUses `data_io.load_data()` if available; otherwise a **synthetic dataset** so you can run end-to-end."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "load_data = None\ntry:\n    from data_io import load_data  # expects load_data(source, uri, sql, redshift_kwargs)\nexcept Exception as e:\n    print(\"\u2139\ufe0f data_io.load_data not found. Using synthetic demo. Error:\", repr(e))\n\ndef _demo_dataset(n=12000, seed=SEED):\n    rng = np.random.default_rng(seed)\n    df = pd.DataFrame({\n        \"customer_id\": np.arange(1, n+1),\n        \"age\": rng.integers(18, 85, size=n),\n        \"tenure_months\": rng.integers(0, 120, size=n),\n        \"monthly_charges\": rng.normal(45, 15, size=n).round(2),\n        \"contract_type\": rng.choice([\"month-to-month\",\"one-year\",\"two-year\"], size=n, p=[0.6,0.25,0.15]),\n        \"country\": rng.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n, p=[0.5,0.2,0.2,0.1]),\n        \"signup_ts\": pd.to_datetime(\"2022-01-01\") + pd.to_timedelta(rng.integers(0, 900, size=n), unit=\"D\"),\n        \"churned\": rng.choice([0,1], size=n, p=[0.78,0.22]).astype(int),\n    })\n    df.loc[rng.choice(df.index, 40, replace=False), \"monthly_charges\"] = -5.0\n    df.loc[rng.choice(df.index, 60, replace=False), \"age\"] = None\n    return df\n\nif load_data:\n    if CONFIG[\"data\"][\"source\"] == \"parquet\":\n        df = load_data(source=\"parquet\", uri=CONFIG[\"data\"][\"parquet_uri\"], sql=None, redshift_kwargs=None)\n    else:\n        df = load_data(source=\"redshift\", uri=None, sql=CONFIG[\"data\"][\"redshift_sql\"], redshift_kwargs=CONFIG[\"data\"][\"redshift_kwargs\"])\nelse:\n    df = _demo_dataset()\n\nprint(\"Shape:\", df.shape)\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfc Minimal Deterministic Preprocessing\nYou can replace this with your project preprocessor. Add feature engineering as needed. **Edit freely.**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.metrics import (\n    roc_auc_score, average_precision_score, precision_recall_curve,\n    precision_score, recall_score, brier_score_loss\n)\n\ndef preprocess_minimal(df, target):\n    df = df.copy()\n    if \"monthly_charges\" in df.columns:\n        df.loc[df[\"monthly_charges\"] < 0, \"monthly_charges\"] = np.nan\n    for c in df.columns:\n        if c == target: \n            continue\n        if df[c].dtype == object:\n            df[c] = df[c].fillna(\"__MISSING__\").astype(str)\n        elif pd.api.types.is_numeric_dtype(df[c]):\n            df[c] = df[c].fillna(df[c].median())\n        elif str(df[c].dtype).startswith(\"datetime\"):\n            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n    if {\"tenure_months\",\"monthly_charges\"}.issubset(df.columns):\n        df[\"est_ltv\"] = (df[\"tenure_months\"] * df[\"monthly_charges\"]).round(2)\n    return df"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udccf Custom Metrics \u2014 **add/edit here**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def compute_metrics(y_true, proba, target_recall, metrics_wanted):\n    prec, rec, thr = precision_recall_curve(y_true, proba)\n    idx = np.where(rec[:-1] >= target_recall)[0]\n    i = int(idx[-1]) if len(idx) else 0\n    thr_rec = float(thr[i]) if len(idx) else 0.0\n    yhat_rec = (proba >= thr_rec).astype(int)\n\n    out = {}\n    if \"roc_auc\" in metrics_wanted:\n        out[\"roc_auc\"] = float(roc_auc_score(y_true, proba))\n    if \"pr_auc\" in metrics_wanted:\n        out[\"pr_auc\"] = float(average_precision_score(y_true, proba))\n    if \"recall_at_target\" in metrics_wanted:\n        out[\"recall_at_target\"] = float(recall_score(y_true, yhat_rec, zero_division=0))\n    if \"precision_at_target\" in metrics_wanted:\n        out[\"precision_at_target\"] = float(precision_score(y_true, yhat_rec, zero_division=0))\n    if \"brier\" in metrics_wanted:  # <- TODO \u270f\ufe0f add more metrics you care about\n        out[\"brier\"] = float(brier_score_loss(y_true, proba))\n    out[\"threshold_at_target\"] = thr_rec\n    return out"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udd01 Cross-Validation \u2014 **choose your model**\nSet `CONFIG['modeling']['choice']` to `'random_forest'`, `'catboost'`, or `'xgboost'`.\nBelow are example configurations and training logic for each.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef encode_objects_joint(Xtr, Xva):\n    Xt, Xv = Xtr.copy(), Xva.copy()\n    for c in Xt.columns:\n        if Xt[c].dtype == object:\n            vals = pd.concat([Xt[c], Xv[c]], axis=0).astype(str)\n            mapping = {v:i for i,v in enumerate(pd.Series(vals).unique())}\n            Xt[c] = Xt[c].map(mapping).fillna(-1).astype(int)\n            Xv[c] = Xv[c].map(mapping).fillna(-1).astype(int)\n    return Xt, Xv\n\ndef evaluate_candidate_via_cv(df, cfg):\n    target = cfg[\"data\"][\"target_col\"]\n    ids = cfg[\"data\"][\"id_features\"]\n    time_col = cfg[\"data\"][\"time_col\"]\n    target_recall = cfg[\"evaluation\"][\"target_recall\"]\n    folds = cfg[\"evaluation\"][\"cv_folds\"]\n    strategy = cfg[\"evaluation\"][\"cv_strategy\"]\n    metrics_wanted = cfg[\"evaluation\"][\"metrics_to_compute\"]\n    model_choice = cfg[\"modeling\"][\"choice\"].lower()\n\n    df = df.drop(columns=[c for c in ids if c in df.columns]).copy()\n    df = preprocess_minimal(df, target)\n\n    dt_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n    if dt_cols:\n        df = df.drop(columns=dt_cols)\n\n    y = df[target].astype(int).values\n    X = df.drop(columns=[target])\n    cat_cols = [c for c in X.columns if X[c].dtype == object]\n\n    if strategy == \"timeseries\" and time_col in df.columns:\n        splitter = TimeSeriesSplit(n_splits=folds)\n        split_iter = splitter.split(X, y)\n    else:\n        splitter = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(X, y)\n\n    fold_rows = []\n    for k, (tr_idx, va_idx) in enumerate(split_iter, 1):\n        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n        ytr, yva = y[tr_idx], y[va_idx]\n\n        if model_choice == \"catboost\":\n            try:\n                from catboost import CatBoostClassifier, Pool\n            except Exception as e:\n                print(\"\u26a0\ufe0f CatBoost not available, falling back to RandomForest.\", e)\n                model_choice = \"random_forest\"\n\n        if model_choice == \"catboost\":\n            # Keep object dtypes; CatBoost handles them via cat_features\n            cat_idx = X.columns.get_indexer(cat_cols).tolist()\n            train_pool = Pool(Xtr, ytr, cat_features=cat_idx)\n            valid_pool = Pool(Xva, yva, cat_features=cat_idx)\n            params = cfg[\"modeling\"][\"catboost\"]\n            model = CatBoostClassifier(**params)\n            model.fit(train_pool, eval_set=valid_pool, use_best_model=False, verbose=params.get(\"verbose\", False))\n            proba = model.predict_proba(valid_pool)[:,1]\n        elif model_choice == \"xgboost\":\n            try:\n                from xgboost import XGBClassifier\n            except Exception as e:\n                print(\"\u26a0\ufe0f XGBoost not available, falling back to RandomForest.\", e)\n                model_choice = \"random_forest\"\n                XGBClassifier = None  # to appease linters\n            if model_choice == \"xgboost\":\n                Xtr_enc, Xva_enc = encode_objects_joint(Xtr, Xva)\n                params = cfg[\"modeling\"][\"xgboost\"]\n                model = XGBClassifier(**params)\n                model.fit(Xtr_enc, ytr, eval_set=[(Xva_enc, yva)], verbose=False)\n                proba = model.predict_proba(Xva_enc)[:,1]\n            else:\n                # fall through to RF\n                Xtr_enc, Xva_enc = encode_objects_joint(Xtr, Xva)\n                model = RandomForestClassifier(n_estimators=400, random_state=SEED, class_weight=\"balanced\")\n                model.fit(Xtr_enc, ytr)\n                proba = model.predict_proba(Xva_enc)[:,1]\n        else:\n            # random_forest (default)\n            Xtr_enc, Xva_enc = encode_objects_joint(Xtr, Xva)\n            model = RandomForestClassifier(n_estimators=400, random_state=SEED, class_weight=\"balanced\")  # MODEL CHOICE <- TODO \u270f\ufe0f\n            model.fit(Xtr_enc, ytr)\n            proba = model.predict_proba(Xva_enc)[:,1]\n\n        m = compute_metrics(yva, proba, target_recall, metrics_wanted)\n        row = {\"fold\": k, **m, \"n_train\": int(len(tr_idx)), \"n_val\": int(len(va_idx))}\n        fold_rows.append(row)\n\n        print(f\"Fold {k} [{model_choice}]:\", \" \".join([f\"{kk}:{row.get(kk):.4f}\" for kk in [\"roc_auc\",\"pr_auc\",\"recall_at_target\",\"precision_at_target\"] if kk in row]))\n\n    df_cv = pd.DataFrame(fold_rows)\n    agg = {}\n    if \"recall_at_target\" in df_cv.columns:\n        agg.update({\n            \"mean_recall_at_target\": float(df_cv[\"recall_at_target\"].mean()),\n            \"std_recall_at_target\": float(df_cv[\"recall_at_target\"].std())\n        })\n    if \"pr_auc\" in df_cv.columns:\n        agg[\"mean_pr_auc\"] = float(df_cv[\"pr_auc\"].mean())\n    if \"roc_auc\" in df_cv.columns:\n        agg[\"mean_roc_auc\"] = float(df_cv[\"roc_auc\"].mean())\n\n    results = {\"folds\": fold_rows, **agg}\n    return results\n\n# Compute or read CV summary\nif CONFIG[\"evaluation\"][\"read_cv_from\"] and Path(CONFIG[\"evaluation\"][\"read_cv_from\"]).exists():\n    with open(CONFIG[\"evaluation\"][\"read_cv_from\"], \"r\") as f:\n        cv_results = json.load(f)\n    print(\"Loaded CV from:\", CONFIG[\"evaluation\"][\"read_cv_from\"])\nelse:\n    cv_results = evaluate_candidate_via_cv(df, CONFIG)\n\nwith open(CONFIG[\"paths\"][\"cv_summary_path\"], \"w\") as f:\n    json.dump(cv_results, f, indent=2)\n\ncv_results"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2705 Quality Gates \u2014 **edit rules here**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def apply_quality_gates(cv_results, cfg):\n    target_recall = cfg[\"evaluation\"][\"target_recall\"]\n    stability_std_max = cfg[\"evaluation\"][\"stability_std_max\"]\n    min_fold_recall = cfg[\"evaluation\"][\"min_fold_recall\"]\n    min_pr_auc = cfg[\"evaluation\"].get(\"min_pr_auc\", None)\n\n    mean_rec = cv_results.get(\"mean_recall_at_target\", 0.0)\n    std_rec  = cv_results.get(\"std_recall_at_target\", 1.0)\n    fold_recalls = [f.get(\"recall_at_target\", 0.0) for f in cv_results.get(\"folds\", [])]\n    min_rec = float(min(fold_recalls)) if fold_recalls else 0.0\n\n    gates = {\n        \"gate_mean_recall\": mean_rec >= target_recall,                 # <- TODO \u270f\ufe0f adjust logic if needed\n        \"gate_stability\": std_rec <= stability_std_max,                # <- TODO \u270f\ufe0f\n        \"gate_min_fold\": min_rec >= min_fold_recall,                  # <- TODO \u270f\ufe0f\n    }\n    if min_pr_auc is not None and \"mean_pr_auc\" in cv_results:\n        gates[\"gate_min_pr_auc\"] = cv_results[\"mean_pr_auc\"] >= min_pr_auc  # <- TODO \u270f\ufe0f extra gate\n\n    decision = all(gates.values())\n\n    report = {\n        \"targets\": {\n            \"target_recall\": target_recall,\n            \"stability_std_max\": stability_std_max,\n            \"min_fold_recall\": min_fold_recall,\n            \"min_pr_auc\": min_pr_auc,\n        },\n        \"observed\": {\n            \"mean_recall_at_target\": mean_rec,\n            \"std_recall_at_target\": std_rec,\n            \"min_fold_recall\": min_rec,\n            \"mean_pr_auc\": cv_results.get(\"mean_pr_auc\"),\n            \"mean_roc_auc\": cv_results.get(\"mean_roc_auc\"),\n        },\n        \"gates\": gates,\n        \"quality_gates_pass\": decision\n    }\n    return report\n\nquality_report = apply_quality_gates(cv_results, CONFIG)\nquality_report"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83c\udfc6 Compare to Champion (Model Registry) \u2014 **choose metric**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def get_champion_metrics_from_registry(package_group, metric_key=\"mean_recall_at_target\"):\n    try:\n        import boto3\n        sm = boto3.client(\"sagemaker\")\n        res = sm.list_model_packages(\n            ModelPackageGroupName=package_group,\n            SortBy=\"CreationTime\",\n            SortOrder=\"Descending\",\n            MaxResults=20\n        )\n        for mp in res.get(\"ModelPackageSummaryList\", []):\n            if mp.get(\"ModelApprovalStatus\") == \"Approved\":\n                desc = sm.describe_model_package(ModelPackageName=mp[\"ModelPackageArn\"])\n                meta = desc.get(\"CustomerMetadataProperties\") or {}\n                if metric_key in meta:\n                    return {\"metric_key\": metric_key, \"value\": float(meta[metric_key]), \"arn\": mp[\"ModelPackageArn\"]}\n                return {\"metric_key\": metric_key, \"value\": None, \"arn\": mp[\"ModelPackageArn\"]}\n        return None\n    except Exception as e:\n        print(\"Registry lookup failed:\", e)\n        return None\n\nchampion = get_champion_metrics_from_registry(CONFIG[\"registry\"][\"package_group\"], CONFIG[\"evaluation\"][\"champion_metric_key\"])\nchampion"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udea6 Promotion Decision \u2014 **edit criteria here**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def decide_promotion(quality_report, cv_results, champion, cfg):\n    if not quality_report[\"quality_gates_pass\"]:\n        return {\"promote\": False, \"reason\": \"Failed quality gates\", \"compare\": None}\n\n    metric_key = cfg[\"evaluation\"][\"champion_metric_key\"]\n    margin = cfg[\"evaluation\"][\"better_than_champion_margin\"]\n    candidate_val = cv_results.get(metric_key, None)\n\n    # CUSTOM EXTRA RULES <- TODO \u270f\ufe0f add any additional promotion rules\n    extra_ok = True\n\n    if not extra_ok:\n        return {\"promote\": False, \"reason\": \"Failed extra promotion rules\", \"compare\": None}\n\n    if candidate_val is None:\n        return {\"promote\": True, \"reason\": \"Gates pass and candidate metric computed; no champion metric to compare\", \"compare\": None}\n\n    if not champion or champion.get(\"value\") is None:\n        return {\"promote\": True, \"reason\": \"Gates pass; champion metric missing\", \"compare\": None}\n\n    champ_val = champion[\"value\"]\n    better = (candidate_val >= champ_val + margin)   # <- TODO \u270f\ufe0f change comparator if lower-is-better metric\n    reason = f\"candidate {metric_key}={candidate_val:.4f} vs champion {champ_val:.4f} (margin {margin:.4f})\"\n\n    return {\"promote\": bool(better), \"reason\": reason, \"compare\": {\"metric\": metric_key, \"candidate\": candidate_val, \"champion\": champ_val, \"margin\": margin}}\n\npromotion = decide_promotion(quality_report, cv_results, champion, CONFIG)\npromotion"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce6 Artifact Checklist \u2014 **edit required list**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "REQUIRED = {\n    \"model_tar_path\": CONFIG[\"registry\"][\"candidate\"][\"model_tar_path\"],      # <- TODO \u270f\ufe0f must contain model + code\n    \"inference_script\": CONFIG[\"registry\"][\"candidate\"][\"inference_script\"],  # <- TODO \u270f\ufe0f entrypoint file name\n    \"requirements\": CONFIG[\"registry\"][\"candidate\"][\"requirements\"],          # <- TODO \u270f\ufe0f runtime deps\n    \"schema_json\": CONFIG[\"registry\"][\"candidate\"][\"schema_json\"],            # <- TODO \u270f\ufe0f input/output schema\n}\n\nmissing = {k:v for k,v in REQUIRED.items() if not Path(v).exists()}\nartifact_ok = len(missing) == 0\n\nartifact_report = {\"required\": REQUIRED, \"missing\": missing, \"artifact_ok\": artifact_ok}\nartifact_report"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfe Validation Report & Optional Registration"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "validation_report = {\n    \"run_id\": RUN_ID,\n    \"timestamp_utc\": RUN_TS,\n    \"quality_report\": quality_report,\n    \"cv_results\": cv_results,\n    \"champion\": champion,\n    \"promotion_decision\": promotion,\n    \"artifact_report\": artifact_report\n}\n\nwith open(CONFIG[\"paths\"][\"validation_report_path\"], \"w\") as f:\n    json.dump(validation_report, f, indent=2)\n\nprint(\"Saved report:\", CONFIG[\"paths\"][\"validation_report_path\"])\n\nshould_register = CONFIG[\"registry\"][\"register_if_pass\"] and promotion[\"promote\"] and artifact_report[\"artifact_ok\"]\nprint(\"\\nDecision:\")\nprint(\" Gates pass:      \", quality_report[\"quality_gates_pass\"])\nprint(\" Artifact ready:  \", artifact_report[\"artifact_ok\"])\nprint(\" Better than champ:\", promotion[\"promote\"])\nprint(\" Will register:   \", should_register)"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83e\udde9 Register candidate to SageMaker Model Registry \u2014 **optional & guarded**"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "if should_register:\n    try:\n        import boto3, sagemaker\n        sm = boto3.client(\"sagemaker\")\n        s3 = boto3.client(\"s3\")\n\n        s3_prefix = CONFIG[\"registry\"][\"s3_prefix\"]\n        assert s3_prefix.startswith(\"s3://\"), \"s3_prefix must be an s3:// URL\"\n        _, rest = s3_prefix.split(\"s3://\", 1)\n        bucket, key_prefix = rest.split(\"/\", 1)\n\n        uploads = {}\n        for key, local in CONFIG[\"registry\"][\"candidate\"].items():\n            if Path(local).exists():\n                dest_key = f\"{key_prefix}/{Path(local).name}\"\n                s3.upload_file(str(local), bucket, dest_key)\n                uploads[key] = f\"s3://{bucket}/{dest_key}\"\n\n        for local_path, name in [(CONFIG[\"paths\"][\"cv_summary_path\"], \"cv_summary.json\"), (CONFIG[\"paths\"][\"validation_report_path\"], \"validation_report.json\")]:\n            dest_key = f\"{key_prefix}/{name}\"\n            s3.upload_file(local_path, bucket, dest_key)\n            uploads[name] = f\"s3://{bucket}/{dest_key}\"\n\n        model_metrics = {\n            \"ModelQuality\": {\n                \"Statistics\": {\"S3Uri\": uploads[\"cv_summary.json\"], \"ContentType\": \"application/json\"},\n                \"Constraints\": {\"S3Uri\": uploads[\"validation_report.json\"], \"ContentType\": \"application/json\"}\n            }\n        }\n\n        image_uri = CONFIG[\"registry\"][\"container_image_uri\"] or sagemaker.image_uris.retrieve(\"pytorch\", boto3.Session().region_name, version=\"2.0\", image_scope=\"inference\")\n        primary_container = {\n            \"Image\": image_uri,\n            \"ModelDataUrl\": uploads.get(\"model_tar_path\"),\n            \"Environment\": {\n                \"SAGEMAKER_PROGRAM\": Path(CONFIG[\"registry\"][\"candidate\"][\"inference_script\"]).name,\n                \"SAGEMAKER_SUBMIT_DIRECTORY\": CONFIG[\"registry\"][\"candidate\"][\"model_tar_path\"],\n                \"SAGEMAKER_REQUIREMENTS\": Path(CONFIG[\"registry\"][\"candidate\"][\"requirements\"]).name,\n            }\n        }\n\n        try:\n            sm.describe_model_package_group(ModelPackageGroupName=CONFIG[\"registry\"][\"package_group\"])\n        except sm.exceptions.ClientError:\n            sm.create_model_package_group(\n                ModelPackageGroupName=CONFIG[\"registry\"][\"package_group\"],\n                ModelPackageGroupDescription=\"Model group created by validation exercise notebook\"\n            )\n\n        customer_meta = {\n            \"mean_recall_at_target\": str(cv_results.get(\"mean_recall_at_target\")),\n            \"std_recall_at_target\": str(cv_results.get(\"std_recall_at_target\")),\n            \"mean_pr_auc\": str(cv_results.get(\"mean_pr_auc\")),\n            \"mean_roc_auc\": str(cv_results.get(\"mean_roc_auc\")),\n            \"validation_report\": uploads[\"validation_report.json\"],\n            \"model_choice\": CONFIG[\"modeling\"][\"choice\"]\n        }\n\n        resp = sm.create_model_package(\n            ModelPackageGroupName=CONFIG[\"registry\"][\"package_group\"],\n            ModelPackageDescription=\"Registered via validation exercise notebook\",\n            InferenceSpecification={\n                \"Containers\": [primary_container],\n                \"SupportedContentTypes\": [\"application/json\", \"text/csv\"],\n                \"SupportedResponseMIMETypes\": [\"application/json\"]\n            },\n            ModelMetrics=model_metrics,\n            ModelApprovalStatus=CONFIG[\"registry\"][\"model_approval_status\"],\n            CustomerMetadataProperties=customer_meta\n        )\n        print(\"\u2705 Registered model package:\", resp[\"ModelPackageArn\"])\n    except Exception as e:\n        print(\"\u274c Registration failed:\", e)\nelse:\n    print(\"Skipping registration (conditions not met or disabled).\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddea Tips & Ready\u2011to\u2011use examples\n**CatBoost** quick start (set `CONFIG['modeling']['choice']='catboost'`):\n```python\nCONFIG['modeling']['catboost'].update({\n    'iterations': 1200,\n    'learning_rate': 0.06,\n    'depth': 6,\n    'l2_leaf_reg': 4.0,\n    'auto_class_weights': 'Balanced',\n    'verbose': False,\n})\n```\n**XGBoost** quick start (set `CONFIG['modeling']['choice']='xgboost'`):\n```python\nCONFIG['modeling']['xgboost'].update({\n    'n_estimators': 1200,\n    'learning_rate': 0.06,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 2.0,\n    'tree_method': 'hist',  # 'gpu_hist' if you have GPU\n})\n```\n> Remember to **uncomment** the pip install cell if your kernel doesn\u2019t have these packages.\n"}]}