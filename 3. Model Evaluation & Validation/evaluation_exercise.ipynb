{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\udcca Model Evaluation \u2014 **Exercise** (Add your changes where marked)\n\nWelcome! This notebook mirrors our MLOps evaluation workflow but leaves key choices to **you**.\n\n- Look for **`# <- TODO \u270f\ufe0f`** comments and edit those lines.\n- Keep runs **reproducible** and **config-driven**.\n- Save artifacts so they can be traced in CI/CD and in SageMaker Model Registry.\n\n### Why evaluation matters\n- **Ensures Production Readiness:** confirm performance meets SLAs before/after deployment.\n- **Drives Model Improvement:** pinpoint failure modes \u2192 guide feature and retraining work.\n- **Maintains Model Health:** watch for bias, data drift, and concept drift post-deployment.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddf0 Prereqs\n\nIf needed, install packages below (SageMaker kernels usually have most of these preinstalled)."}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "# %pip install pandas numpy scikit-learn mlflow catboost boto3 sagemaker s3fs pyarrow sqlalchemy redshift_connector"}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "# \u267b\ufe0f Reproducibility & environment capture\nimport os, sys, json, hashlib, random, platform\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\n\nSEED = 42   # <- TODO \u270f\ufe0f choose a single seed for reproducibility across all splits & models\nrandom.seed(SEED); np.random.seed(SEED)\n\nRUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\nRUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n\nARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/eval_{RUN_TS}_{RUN_ID}\")\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\nenv_info = {\n    \"python\": sys.version,\n    \"platform\": platform.platform(),\n    \"timestamp_utc\": RUN_TS,\n    \"seed\": SEED,\n}\nwith open(os.path.join(ARTIFACT_DIR, \"env_eval_info.json\"), \"w\") as f:\n    json.dump(env_info, f, indent=2)\n\nenv_info"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2699\ufe0f Configuration (edit here only)\n\nChoose **data source**, **target column**, **ID/leakage columns**, **model family**, and **CV strategy**. "}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "CONFIG = {\n    \"data\": {\n        # <- TODO \u270f\ufe0f choose: \"parquet\" or \"redshift\"\n        \"source\": os.environ.get(\"SOURCE\", \"parquet\"),\n        # <- TODO \u270f\ufe0f if parquet, point to your dataset (S3 or local path with wildcards)\n        \"parquet_uri\": os.environ.get(\"PARQUET_URI\", \"s3://YOUR-BUCKET/YOUR-PATH/*.parquet\"),\n        # <- TODO \u270f\ufe0f if redshift, provide a deterministic SQL query (ORDER BY for stability)\n        \"redshift_sql\": os.environ.get(\"SQL\", \"SELECT * FROM your_schema.your_table ORDER BY id\"),\n        # <- TODO \u270f\ufe0f set via env vars or instance role; change only if testing locally\n        \"redshift_kwargs\": {\n            \"host\": os.environ.get(\"REDSHIFT_HOST\", \"example.redshift.amazonaws.com\"),\n            \"database\": os.environ.get(\"REDSHIFT_DB\", \"dev\"),\n            \"user\": os.environ.get(\"REDSHIFT_USER\", \"username\"),\n            \"password\": os.environ.get(\"REDSHIFT_PASSWORD\", \"password\"),\n            \"port\": int(os.environ.get(\"REDSHIFT_PORT\", \"5439\")),\n        },\n    },\n    \"features\": {\n        # <- TODO \u270f\ufe0f list identifiers/leakage columns to DROP before training/eval\n        \"id_features\": [\"customer_id\",\"contract_id\",\"account_id\"]\n    },\n    \"model\": {\n        # <- TODO \u270f\ufe0f set your target column\n        \"target_col\": os.environ.get(\"TARGET\", \"churned\"),\n        \"random_seed\": SEED,\n        # Choose your model here (implementations below):\n        # options: \"catboost\", \"xgboost\", \"lightgbm\", \"sklearn_rf\", \"sklearn_logreg\"\n        \"family\": os.environ.get(\"MODEL_FAMILY\",\"catboost\"),  # <- TODO \u270f\ufe0f pick a model family\n\n        # Baseline hyperparameters (tune as you wish)\n        \"params\": {\n            # CatBoost-like defaults (ignored by other models unless mapped)\n            \"n_estimators\": 1200,   # <- TODO \u270f\ufe0f\n            \"learning_rate\": 0.08,  # <- TODO \u270f\ufe0f\n            \"depth\": 6,             # <- TODO \u270f\ufe0f\n            \"l2_leaf_reg\": 3.0,     # <- TODO \u270f\ufe0f\n            \"auto_class_weights\": \"Balanced\",\n        }\n    },\n    \"evaluation\": {\n        \"cv_folds\": 5,                     # <- TODO \u270f\ufe0f\n        \"cv_strategy\": \"stratified\",       # <- TODO \u270f\ufe0f stratified | timeseries\n        \"opt_target_recall\": 0.80,         # <- TODO \u270f\ufe0f threshold target\n        \"use_mlflow\": False,               # set True if you configured MLflow\n        # Optional: temporal column to force chronological splits in holdout/backtest\n        \"time_col\": \"signup_ts\",           # <- TODO \u270f\ufe0f set to None if not available\n    },\n    \"output\": {\n        \"artifact_dir\": ARTIFACT_DIR,\n        \"cv_summary_path\": os.path.join(ARTIFACT_DIR, \"cv_summary.json\"),\n        \"holdout_report_path\": os.path.join(ARTIFACT_DIR, \"holdout_report.json\"),\n    }\n}\n\nCONFIG"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce5 Load Data\n\nWe will try `data_io.load_data()` if available; otherwise we fall back to a synthetic dataset so you can run end\u2011to\u2011end now. "}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "# Try loading data via project helper\nload_data = None\ntry:\n    from data_io import load_data  # expects load_data(source, uri, sql, redshift_kwargs)\nexcept Exception as e:\n    print(\"\u2139\ufe0f data_io.load_data not found. Using synthetic demo. Error:\", repr(e))\n\ndef _demo_dataset(n=12000, seed=SEED):\n    rng = np.random.default_rng(seed)\n    df = pd.DataFrame({\n        \"customer_id\": np.arange(1, n+1),\n        \"age\": rng.integers(18, 85, size=n),\n        \"tenure_months\": rng.integers(0, 120, size=n),\n        \"monthly_charges\": rng.normal(45, 15, size=n).round(2),\n        \"contract_type\": rng.choice([\"month-to-month\",\"one-year\",\"two-year\"], size=n, p=[0.6,0.25,0.15]),\n        \"country\": rng.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n, p=[0.5,0.2,0.2,0.1]),\n        \"signup_ts\": pd.to_datetime(\"2022-01-01\") + pd.to_timedelta(rng.integers(0, 900, size=n), unit=\"D\"),\n        \"churned\": rng.choice([0,1], size=n, p=[0.78,0.22]).astype(int),\n    })\n    # anomalies to exercise cleaning\n    df.loc[rng.choice(df.index, 40, replace=False), \"monthly_charges\"] = -5.0\n    df.loc[rng.choice(df.index, 60, replace=False), \"age\"] = None\n    return df\n\nif load_data:\n    if CONFIG[\"data\"][\"source\"] == \"parquet\":\n        # <- TODO \u270f\ufe0f ensure parquet_uri points to your dataset\n        df = load_data(source=\"parquet\", uri=CONFIG[\"data\"][\"parquet_uri\"], sql=None, redshift_kwargs=None)\n    else:\n        # <- TODO \u270f\ufe0f ensure SQL/credentials resolve to your Redshift view/table\n        df = load_data(source=\"redshift\", uri=None, sql=CONFIG[\"data\"][\"redshift_sql\"], redshift_kwargs=CONFIG[\"data\"][\"redshift_kwargs\"])\nelse:\n    df = _demo_dataset()\n\nprint(\"Shape:\", df.shape)\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddea Utilities & Minimal Preprocessing\n\nWe\u2019ll try to import your project\u2019s `training_utils`. If unavailable, we define **simple fallbacks** you can customize. "}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "# Attempt to use your project helpers first\ntry:\n    from training_utils import (\n        setup_mlflow_tracking, preprocess_data, create_catboost_model, train_and_evaluate_model\n    )\n    HAVE_TRAINING_UTILS = True\n    print(\"Using project training_utils.\")\nexcept Exception as e:\n    print(\"training_utils not found. Using exercise fallbacks. Error:\", repr(e))\n    HAVE_TRAINING_UTILS = False\n\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score, precision_recall_curve,\n    precision_score, recall_score\n)\n\n# ---- TODO-friendly preprocessing ----\ndef preprocess_data_exercise(df, config):\n    df = df.copy()\n    target = config[\"model\"][\"target_col\"]\n\n    # <- TODO \u270f\ufe0f add deterministic cleaning rules for your dataset\n    # Example: fix invalid negatives\n    if \"monthly_charges\" in df.columns:\n        df.loc[df[\"monthly_charges\"] < 0, \"monthly_charges\"] = np.nan\n\n    # <- TODO \u270f\ufe0f choose your missing value policy\n    for c in df.columns:\n        if c == target: \n            continue\n        if df[c].dtype == object:\n            df[c] = df[c].fillna(\"__MISSING__\").astype(str)   # <- TODO \u270f\ufe0f categorical policy\n        elif pd.api.types.is_numeric_dtype(df[c]):\n            df[c] = df[c].fillna(df[c].median())              # <- TODO \u270f\ufe0f numerical imputer\n        elif str(df[c].dtype).startswith(\"datetime\"):\n            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n\n    # <- TODO \u270f\ufe0f add light feature engineering (must be deterministic)\n    if {\"tenure_months\",\"monthly_charges\"}.issubset(df.columns):\n        df[\"est_ltv\"] = (df[\"tenure_months\"] * df[\"monthly_charges\"]).round(2)\n\n    return df\n\ndef build_model_from_family(config):\n    fam = config[\"model\"][\"family\"]\n    params = config[\"model\"][\"params\"]\n\n    if fam == \"catboost\":\n        try:\n            from catboost import CatBoostClassifier\n            return CatBoostClassifier(\n                iterations=params.get(\"n_estimators\", 800),\n                learning_rate=params.get(\"learning_rate\", 0.08),\n                depth=params.get(\"depth\", 6),\n                l2_leaf_reg=params.get(\"l2_leaf_reg\", 3.0),\n                loss_function=\"Logloss\",\n                eval_metric=\"AUC\",\n                random_seed=config[\"model\"][\"random_seed\"],\n                auto_class_weights=params.get(\"auto_class_weights\", \"Balanced\"),\n                verbose=False\n            )\n        except Exception as e:\n            print(\"CatBoost not available, fallback to RandomForest.\", e)\n\n    if fam == \"xgboost\":\n        from xgboost import XGBClassifier\n        return XGBClassifier(\n            n_estimators=params.get(\"n_estimators\", 600),\n            learning_rate=params.get(\"learning_rate\", 0.1),\n            max_depth=params.get(\"depth\", 6),\n            subsample=0.8, colsample_bytree=0.8,\n            reg_lambda=params.get(\"l2_leaf_reg\", 1.0),\n            random_state=config[\"model\"][\"random_seed\"],\n            tree_method=\"hist\", eval_metric=\"auc\"\n        )\n\n    if fam == \"lightgbm\":\n        import lightgbm as lgb\n        return lgb.LGBMClassifier(\n            n_estimators=params.get(\"n_estimators\", 800),\n            learning_rate=params.get(\"learning_rate\", 0.08),\n            max_depth=params.get(\"depth\", -1),\n            reg_lambda=params.get(\"l2_leaf_reg\", 0.0),\n            class_weight=\"balanced\",\n            random_state=config[\"model\"][\"random_seed\"]\n        )\n\n    if fam == \"sklearn_logreg\":\n        from sklearn.linear_model import LogisticRegression\n        return LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=config[\"model\"][\"random_seed\"])\n\n    # default fallback\n    from sklearn.ensemble import RandomForestClassifier\n    return RandomForestClassifier(n_estimators=400, class_weight=\"balanced\", random_state=config[\"model\"][\"random_seed\"])\n\ndef train_and_eval_generic(model, X_tr, y_tr, X_va, y_va, target_recall):\n    # Simple encoding for object columns (joint mapping to avoid unseen category issues)\n    X_tr_enc, X_va_enc = X_tr.copy(), X_va.copy()\n    for c in X_tr_enc.columns:\n        if X_tr_enc[c].dtype == object:\n            vals = pd.concat([X_tr_enc[c], X_va_enc[c]], axis=0).astype(str)\n            mapping = {v:i for i, v in enumerate(pd.Series(vals).unique())}\n            X_tr_enc[c] = X_tr_enc[c].map(mapping).fillna(-1).astype(int)\n            X_va_enc[c] = X_va_enc[c].map(mapping).fillna(-1).astype(int)\n\n    model.fit(X_tr_enc, y_tr)\n    proba = model.predict_proba(X_va_enc)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(X_va_enc)\n\n    roc = roc_auc_score(y_va, proba)\n    pr_auc = average_precision_score(y_va, proba)\n    prec, rec, thr = precision_recall_curve(y_va, proba)\n\n    # best F1\n    f1s = (2*prec[:-1]*rec[:-1])/(prec[:-1]+rec[:-1]+1e-12)\n    i_best = int(np.argmax(f1s))\n    thr_best = float(thr[i_best])\n\n    # target recall\n    idx = np.where(rec[:-1] >= target_recall)[0]\n    i_t = int(idx[-1]) if len(idx) else 0\n    thr_rec = float(thr[i_t]) if len(idx) else 0.0\n\n    y_hat_best = (proba >= thr_best).astype(int)\n    y_hat_rec  = (proba >= thr_rec).astype(int)\n\n    metrics = {\n        \"roc_auc\": float(roc),\n        \"pr_auc\": float(pr_auc),\n        \"best_f1\": float((2*prec[i_best]*rec[i_best])/(prec[i_best]+rec[i_best]+1e-12)),\n        \"best_f1_threshold\": thr_best,\n        \"precision_at_best_f1\": float(precision_score(y_va, y_hat_best, zero_division=0)),\n        \"recall_at_best_f1\": float(recall_score(y_va, y_hat_best, zero_division=0)),\n        \"threshold_at_target_recall\": thr_rec,\n        \"precision_at_target_recall\": float(prec[i_t]),\n        \"recall_at_target_recall\": float(rec[i_t]),\n    }\n    return metrics, proba, {\"thr_best\": thr_best, \"thr_rec\": thr_rec}\n\n# Bind final functions depending on availability\nif HAVE_TRAINING_UTILS:\n    _preprocess = preprocess_data                    # <- uses your project logic\n    _make_model = lambda cfg: create_catboost_model(cfg)  # users can still change MODEL_FAMILY if they switch utils\n    _train_eval  = lambda m, Xtr, ytr, Xva, yva, tr: train_and_evaluate_model(m, None, None, CONFIG, Xtr, Xva, ytr, yva)\nelse:\n    _preprocess = preprocess_data_exercise           # <- TODO \u270f\ufe0f edit this function above\n    _make_model = build_model_from_family            # <- TODO \u270f\ufe0f choose model family in CONFIG\n    _train_eval  = lambda m, Xtr, ytr, Xva, yva, tr: train_and_eval_generic(m, Xtr, ytr, Xva, yva, tr)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udd01 Cross-Validation\n\nPick **StratifiedKFold** for classification or **TimeSeriesSplit** when chronology matters. Thresholds are set at your **target recall**. "}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n\ndef cross_validate_model(df, config):\n    TARGET_COL = config['model']['target_col']\n    id_features = config['features']['id_features']\n\n    # Drop identifiers/leakage\n    df_clean = df.drop(columns=[c for c in id_features if c in df.columns]).copy()\n\n    # Drop datetime columns for generic models (CatBoost supports special handling; we keep simple here)\n    dt_cols = [c for c in df_clean.columns if np.issubdtype(df_clean[c].dtype, np.datetime64)]\n    if dt_cols:\n        df_clean = df_clean.drop(columns=dt_cols)\n\n    # Preprocess\n    df_clean = _preprocess(df_clean, config)\n\n    # Target/Features\n    y = df_clean[TARGET_COL].astype(int).values\n    X = df_clean.drop(columns=[TARGET_COL])\n\n    # CV strategy\n    if config['evaluation']['cv_strategy'] == 'timeseries':\n        splitter = TimeSeriesSplit(n_splits=config['evaluation']['cv_folds'])\n    else:\n        splitter = StratifiedKFold(n_splits=config['evaluation']['cv_folds'], shuffle=True, random_state=config['model']['random_seed'])\n\n    results = {\"fold_results\": []}\n    print(f\"Starting {config['evaluation']['cv_folds']}-fold CV using {config['evaluation']['cv_strategy']} strategy\")\n\n    for fold, (tr_idx, va_idx) in enumerate(splitter.split(X, y), 1):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y[tr_idx], y[va_idx]\n\n        model = _make_model(config)\n        metrics, proba, thr = _train_eval(model, X_tr, y_tr, X_va, y_va, config['evaluation']['opt_target_recall'])\n\n        results[\"fold_results\"].append({\n            \"fold\": fold,\n            **metrics,\n            \"train_samples\": int(len(X_tr)),\n            \"val_samples\": int(len(X_va))\n        })\n\n        print(f\"Fold {fold}: ROC-AUC={metrics['roc_auc']:.4f} | F1={metrics['best_f1']:.4f} | \"\n              f\"Recall@target={metrics['recall_at_target_recall']:.4f} | \"\n              f\"Precision@target={metrics['precision_at_target_recall']:.4f}\")\n\n    # Aggregate\n    import numpy as np\n    agg = {}\n    for key in [\"roc_auc\",\"pr_auc\",\"best_f1\",\"precision_at_target_recall\",\"recall_at_target_recall\"]:\n        vals = [r[key] for r in results[\"fold_results\"]]\n        agg[f\"mean_{key}\"] = float(np.mean(vals))\n        agg[f\"std_{key}\"]  = float(np.std(vals))\n    results.update(agg)\n\n    return results, df_clean\n\ncv_results, df_clean = cross_validate_model(df, CONFIG)\ncv_results"}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "# Save CV summary\nimport json, os, pprint\npprint.pprint(cv_results)\nwith open(CONFIG[\"output\"][\"cv_summary_path\"], \"w\") as f:\n    json.dump(cv_results, f, indent=2)\nCONFIG[\"output\"][\"cv_summary_path\"]"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddea Holdout / Temporal Backtest (Optional)\n\nSimulate *future* data by a strict time split (or random, if you lack a timestamp). "}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "from sklearn.model_selection import train_test_split\n\nTARGET_COL = CONFIG[\"model\"][\"target_col\"]\ntime_col = CONFIG[\"evaluation\"][\"time_col\"]  # <- TODO \u270f\ufe0f set this field in CONFIG if you have a timestamp\n\nif time_col and time_col in df_clean.columns:\n    ts = pd.to_datetime(df_clean[time_col], errors=\"coerce\")\n    cutoff = ts.quantile(0.8)  # <- TODO \u270f\ufe0f adjust split policy if needed\n    tr = df_clean[ts < cutoff].copy()\n    te = df_clean[ts >= cutoff].copy()\nelse:\n    tr, te = train_test_split(df_clean, test_size=0.2, random_state=CONFIG[\"model\"][\"random_seed\"], stratify=df_clean[TARGET_COL])\n\nX_tr, y_tr = tr.drop(columns=[TARGET_COL]), tr[TARGET_COL].values\nX_te, y_te = te.drop(columns=[TARGET_COL]), te[TARGET_COL].values\n\nmdl = _make_model(CONFIG)\nmetrics_te, _, thr = _train_eval(mdl, X_tr, y_tr, X_te, y_te, CONFIG['evaluation']['opt_target_recall'])\n\nholdout_report = {\n    \"n_train\": int(len(tr)),\n    \"n_test\": int(len(te)),\n    **metrics_te\n}\n\nwith open(CONFIG[\"output\"][\"holdout_report_path\"], \"w\") as f:\n    json.dump(holdout_report, f, indent=2)\n\nholdout_report"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfe Browse SageMaker Experiments & Model Registry (Optional)\n\nUse this section to find **previous experiments, training jobs, and model packages** to compare with your results.\n\n> **NOTE:** Requires AWS credentials and permissions. Safe to run \u2014 cells will no-op without credentials. "}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": "# <- TODO \u270f\ufe0f optionally filter by names you use in your project\nEXPERIMENT_NAME_CONTAINS = \"\"   # e.g., \"churn\"\nMODEL_GROUP_CONTAINS = \"\"       # e.g., \"churn-model-group\"\n\ntry:\n    import boto3, sagemaker\n    sm = boto3.client(\"sagemaker\")\n    print(\"AWS Region:\", boto3.Session().region_name)\n\n    # Experiments\n    print(\"\\nRecent Experiments:\")\n    res = sm.list_experiments(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=10)\n    for e in res.get(\"ExperimentSummaries\", []):\n        if EXPERIMENT_NAME_CONTAINS.lower() in e[\"ExperimentName\"].lower():\n            print(\" -\", e[\"ExperimentName\"], \"| Created:\", e[\"CreationTime\"])\n\n    # Model Registry\n    print(\"\\nRecent Model Packages:\")\n    mres = sm.list_model_packages(ModelPackageGroupNameContains=MODEL_GROUP_CONTAINS, SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=10)\n    for mp in mres.get(\"ModelPackageSummaryList\", []):\n        print(\" - Group:\", mp.get(\"ModelPackageGroupName\"), \"| Status:\", mp.get(\"ModelApprovalStatus\"),\n              \"| Created:\", mp.get(\"CreationTime\"))\n\n    # Training Jobs\n    print(\"\\nRecent Training Jobs:\")\n    tres = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=10)\n    for tj in tres.get(\"TrainingJobSummaries\", []):\n        print(\" -\", tj[\"TrainingJobName\"], \"| Status:\", tj[\"TrainingJobStatus\"], \"| Created:\", tj[\"CreationTime\"])\n\nexcept Exception as e:\n    print(\"Skipping browse (missing AWS creds or permissions):\", e)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2705 Promotion Guidance\n\n- Set **quality gates** (e.g., `mean recall@target \u2265 0.80` and stable across folds).\n- Compare against **approved** model in Registry. Block promotion if worse.\n- If drift/bias suspected: open an issue, review data pipeline, schedule retraining.\n- Log and ship: `cv_summary.json`, `holdout_report.json`, model artifacts, and config to MLflow/Registry.\n"}]}