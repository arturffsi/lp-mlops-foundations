{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\udee1\ufe0f Model Validation \u2014 **Example** (SageMaker\u2011ready)\n\nPurpose: **decide if a candidate model is promotable**. This notebook validates a newly trained model against **quality gates**, compares it to the **approved champion** in the SageMaker **Model Registry**, and (optionally) **registers** the model if it passes and has all required artifacts.\n\n### Why validation?\n- **Ensures Production Readiness** \u2014 confirm predictive performance meets SLAs **before & after** deployment.\n- **Drives Model Improvement** \u2014 expose failure modes and gaps that guide **feature & retraining** work.\n- **Maintains Model Health** \u2014 detect **bias**, **data drift**, **concept drift**; block risky promotions.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddf0 Prerequisites\n\nUncomment to install any missing deps (Studio kernels usually have most of these):"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# %pip install pandas numpy scikit-learn boto3 sagemaker mlflow s3fs pyarrow catboost sqlalchemy redshift_connector"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udeaa Studio Bootstrap (safe to run locally too)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, boto3\ntry:\n    import sagemaker\n    sm_sess = sagemaker.Session()\n    _region = boto3.Session().region_name\n    try:\n        _role = sagemaker.get_execution_role()\n    except Exception:\n        _role = \"unknown-role\"\n    _bucket = sm_sess.default_bucket()\n    print(\"\u2705 SageMaker context\")\n    print(\" Region:\", _region)\n    print(\" Role:  \", _role)\n    print(\" Bucket:\", _bucket)\n    os.environ.setdefault(\"AWS_REGION\", _region or \"\")\n    os.environ.setdefault(\"SM_DEFAULT_BUCKET\", _bucket or \"\")\nexcept Exception as e:\n    print(\"\u2139\ufe0f Running without SageMaker context. Reason:\", e)"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u267b\ufe0f Reproducibility & Environment Capture"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import sys, json, hashlib, random, platform\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nRUN_TS = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\nRUN_ID = hashlib.sha1(f\"val-{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n\nARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/validation_{RUN_TS}_{RUN_ID}\")\nPath(ARTIFACT_DIR).mkdir(parents=True, exist_ok=True)\n\nenv_info = {\n    \"python\": sys.version,\n    \"platform\": platform.platform(),\n    \"timestamp_utc\": RUN_TS,\n    \"seed\": SEED,\n}\nwith open(Path(ARTIFACT_DIR)/\"env_validation_info.json\", \"w\") as f:\n    json.dump(env_info, f, indent=2)\n\nenv_info"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2699\ufe0f Configuration\n\nEdit this cell to point to your data, thresholds, and Registry settings. This notebook can compute fresh CV metrics or read precomputed results."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "CONFIG = {\n    \"data\": {\n        \"source\": os.environ.get(\"SOURCE\", \"parquet\"),  # \"parquet\" | \"redshift\"\n        \"parquet_uri\": os.environ.get(\"PARQUET_URI\", \"s3://YOUR-BUCKET/path/*.parquet\"),\n        \"redshift_sql\": os.environ.get(\"SQL\", \"SELECT * FROM your_schema.your_table ORDER BY id\"),\n        \"redshift_kwargs\": {\n            \"host\": os.environ.get(\"REDSHIFT_HOST\", \"example.redshift.amazonaws.com\"),\n            \"database\": os.environ.get(\"REDSHIFT_DB\", \"dev\"),\n            \"user\": os.environ.get(\"REDSHIFT_USER\", \"username\"),\n            \"password\": os.environ.get(\"REDSHIFT_PASSWORD\", \"password\"),\n            \"port\": int(os.environ.get(\"REDSHIFT_PORT\", \"5439\")),\n        },\n        \"target_col\": os.environ.get(\"TARGET\", \"churned\"),\n        \"id_features\": [\"customer_id\",\"contract_id\",\"account_id\"],\n        \"time_col\": os.environ.get(\"TIME_COL\", \"signup_ts\"),  # set to None if not available\n    },\n    \"evaluation\": {\n        \"cv_folds\": int(os.environ.get(\"CV_FOLDS\",\"5\")),\n        \"cv_strategy\": os.environ.get(\"CV_STRATEGY\",\"stratified\"),  # \"stratified\" | \"timeseries\"\n        \"target_recall\": float(os.environ.get(\"TARGET_RECALL\",\"0.80\")),\n        \"stability_std_max\": float(os.environ.get(\"STABILITY_STD_MAX\",\"0.03\")),  # std of recall@target\n        \"min_fold_recall\": float(os.environ.get(\"MIN_FOLD_RECALL\",\"0.75\")),      # safety floor per fold\n        \"better_than_champion_margin\": float(os.environ.get(\"BETTER_MARGIN\",\"0.0\")), # require strictly better by margin\n        \"read_cv_from\": os.environ.get(\"CV_JSON\",\"\"),   # optional: path to existing cv_summary.json\n    },\n    \"registry\": {\n        \"package_group\": os.environ.get(\"SM_MODEL_PACKAGE_GROUP\",\"churn-model-group\"),\n        \"candidate\": {\n            # local artifacts you plan to register\n            \"model_tar_path\": os.environ.get(\"MODEL_TAR\",\"model.tar.gz\"),    # created by training pipeline\n            \"inference_script\": os.environ.get(\"INFERENCE_SCRIPT\",\"inference.py\"),\n            \"requirements\": os.environ.get(\"REQUIREMENTS\",\"requirements.txt\"),\n            \"schema_json\": os.environ.get(\"SCHEMA_JSON\", str(Path(ARTIFACT_DIR)/\"feature_schema.json\")),\n            \"validation_report\": str(Path(ARTIFACT_DIR)/\"validation_report.json\"),\n        },\n        \"s3_prefix\": os.environ.get(\"ARTIFACTS_S3_PREFIX\", f\"s3://{os.environ.get('SM_DEFAULT_BUCKET','')}/model-validation/{RUN_TS}_{RUN_ID}\"),\n        \"register_if_pass\": os.environ.get(\"REGISTER_IF_PASS\",\"false\").lower() == \"true\",\n        \"model_approval_status\": os.environ.get(\"APPROVAL_STATUS\",\"PendingManualApproval\"),\n        \"container_image_uri\": os.environ.get(\"CONTAINER_IMAGE\",\"\"),  # optional: override\n    },\n    \"paths\": {\n        \"artifact_dir\": ARTIFACT_DIR,\n        \"cv_summary_path\": str(Path(ARTIFACT_DIR)/\"cv_summary.json\"),\n        \"validation_report_path\": str(Path(ARTIFACT_DIR)/\"validation_report.json\"),\n    }\n}\n\nCONFIG"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce5 Load Data (Redshift or S3 Parquet)\n\nTries `data_io.load_data()` first; falls back to a synthetic dataset so you can run end\u2011to\u2011end."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "load_data = None\ntry:\n    from data_io import load_data  # expects load_data(source, uri, sql, redshift_kwargs)\nexcept Exception as e:\n    print(\"\u2139\ufe0f data_io.load_data not found. Using synthetic demo. Error:\", repr(e))\n\ndef _demo_dataset(n=12000, seed=SEED):\n    rng = np.random.default_rng(seed)\n    df = pd.DataFrame({\n        \"customer_id\": np.arange(1, n+1),\n        \"age\": rng.integers(18, 85, size=n),\n        \"tenure_months\": rng.integers(0, 120, size=n),\n        \"monthly_charges\": rng.normal(45, 15, size=n).round(2),\n        \"contract_type\": rng.choice([\"month-to-month\",\"one-year\",\"two-year\"], size=n, p=[0.6,0.25,0.15]),\n        \"country\": rng.choice([\"PT\",\"ES\",\"FR\",\"DE\"], size=n, p=[0.5,0.2,0.2,0.1]),\n        \"signup_ts\": pd.to_datetime(\"2022-01-01\") + pd.to_timedelta(rng.integers(0, 900, size=n), unit=\"D\"),\n        \"churned\": rng.choice([0,1], size=n, p=[0.78,0.22]).astype(int),\n    })\n    # anomalies\n    df.loc[rng.choice(df.index, 40, replace=False), \"monthly_charges\"] = -5.0\n    df.loc[rng.choice(df.index, 60, replace=False), \"age\"] = None\n    return df\n\nif load_data:\n    if CONFIG[\"data\"][\"source\"] == \"parquet\":\n        df = load_data(source=\"parquet\", uri=CONFIG[\"data\"][\"parquet_uri\"], sql=None, redshift_kwargs=None)\n    else:\n        df = load_data(source=\"redshift\", uri=None, sql=CONFIG[\"data\"][\"redshift_sql\"], redshift_kwargs=CONFIG[\"data\"][\"redshift_kwargs\"])\nelse:\n    df = _demo_dataset()\n\nprint(\"Shape:\", df.shape)\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfc Minimal Deterministic Preprocessing (replace with your project helpers if you prefer)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.metrics import (\n    roc_auc_score, average_precision_score, precision_recall_curve,\n    precision_score, recall_score\n)\n\ndef preprocess_minimal(df, target):\n    df = df.copy()\n    if \"monthly_charges\" in df.columns:\n        df.loc[df[\"monthly_charges\"] < 0, \"monthly_charges\"] = np.nan\n    for c in df.columns:\n        if c == target: \n            continue\n        if df[c].dtype == object:\n            df[c] = df[c].fillna(\"__MISSING__\").astype(str)\n        elif pd.api.types.is_numeric_dtype(df[c]):\n            df[c] = df[c].fillna(df[c].median())\n        elif str(df[c].dtype).startswith(\"datetime\"):\n            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n    if {\"tenure_months\",\"monthly_charges\"}.issubset(df.columns):\n        df[\"est_ltv\"] = (df[\"tenure_months\"] * df[\"monthly_charges\"]).round(2)\n    return df"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udd01 Cross\u2011Validation (compute candidate metrics)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef encode_objects_joint(Xtr, Xva):\n    Xt, Xv = Xtr.copy(), Xva.copy()\n    for c in Xt.columns:\n        if Xt[c].dtype == object:\n            vals = pd.concat([Xt[c], Xv[c]], axis=0).astype(str)\n            mapping = {v:i for i,v in enumerate(pd.Series(vals).unique())}\n            Xt[c] = Xt[c].map(mapping).fillna(-1).astype(int)\n            Xv[c] = Xv[c].map(mapping).fillna(-1).astype(int)\n    return Xt, Xv\n\ndef evaluate_candidate_via_cv(df, cfg):\n    target = cfg[\"data\"][\"target_col\"]\n    ids = cfg[\"data\"][\"id_features\"]\n    time_col = cfg[\"data\"][\"time_col\"]\n    target_recall = cfg[\"evaluation\"][\"target_recall\"]\n    folds = cfg[\"evaluation\"][\"cv_folds\"]\n    strategy = cfg[\"evaluation\"][\"cv_strategy\"]\n\n    df = df.drop(columns=[c for c in ids if c in df.columns]).copy()\n    df = preprocess_minimal(df, target)\n\n    # Remove pure datetime columns for the simple baseline model here\n    dt_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n    if dt_cols:\n        df = df.drop(columns=dt_cols)\n\n    y = df[target].astype(int).values\n    X = df.drop(columns=[target])\n\n    if strategy == \"timeseries\" and time_col in df.columns:\n        splitter = TimeSeriesSplit(n_splits=folds)\n        split_iter = splitter.split(X, y)\n    else:\n        splitter = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(X, y)\n\n    fold_rows = []\n    for k, (tr_idx, va_idx) in enumerate(split_iter, 1):\n        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n        ytr, yva = y[tr_idx], y[va_idx]\n\n        Xtr_enc, Xva_enc = encode_objects_joint(Xtr, Xva)\n\n        model = RandomForestClassifier(\n            n_estimators=400, max_depth=None, random_state=SEED, class_weight=\"balanced\"\n        )\n        model.fit(Xtr_enc, ytr)\n        proba = model.predict_proba(Xva_enc)[:,1]\n\n        roc = roc_auc_score(yva, proba)\n        pr_auc = average_precision_score(yva, proba)\n        prec, rec, thr = precision_recall_curve(yva, proba)\n        # find highest threshold meeting recall target\n        idx = np.where(rec[:-1] >= target_recall)[0]\n        i = int(idx[-1]) if len(idx) else 0\n        thr_rec = float(thr[i]) if len(idx) else 0.0\n        yhat_rec = (proba >= thr_rec).astype(int)\n\n        row = {\n            \"fold\": k,\n            \"roc_auc\": float(roc),\n            \"pr_auc\": float(pr_auc),\n            \"recall_at_target\": float(recall_score(yva, yhat_rec, zero_division=0)),\n            \"precision_at_target\": float(precision_score(yva, yhat_rec, zero_division=0)),\n            \"threshold_at_target\": thr_rec,\n            \"n_train\": int(len(tr_idx)),\n            \"n_val\": int(len(va_idx)),\n        }\n        fold_rows.append(row)\n\n        print(f\"Fold {k}: ROC-AUC={row['roc_auc']:.4f} | PR-AUC={row['pr_auc']:.4f} | \"\n              f\"Recall@target={row['recall_at_target']:.4f} | Precision@target={row['precision_at_target']:.4f}\")\n\n    df_cv = pd.DataFrame(fold_rows)\n    agg = {\n        \"mean_recall_at_target\": float(df_cv[\"recall_at_target\"].mean()),\n        \"std_recall_at_target\": float(df_cv[\"recall_at_target\"].std()),\n        \"mean_pr_auc\": float(df_cv[\"pr_auc\"].mean()),\n        \"mean_roc_auc\": float(df_cv[\"roc_auc\"].mean()),\n    }\n    results = {\"folds\": fold_rows, **agg}\n    return results\n\n# Compute or read CV summary\nif CONFIG[\"evaluation\"][\"read_cv_from\"] and Path(CONFIG[\"evaluation\"][\"read_cv_from\"]).exists():\n    with open(CONFIG[\"evaluation\"][\"read_cv_from\"], \"r\") as f:\n        cv_results = json.load(f)\n    print(\"Loaded CV from:\", CONFIG[\"evaluation\"][\"read_cv_from\"])\nelse:\n    cv_results = evaluate_candidate_via_cv(df, CONFIG)\n\nwith open(CONFIG[\"paths\"][\"cv_summary_path\"], \"w\") as f:\n    json.dump(cv_results, f, indent=2)\n\ncv_results"}, {"cell_type": "markdown", "metadata": {}, "source": "## \u2705 Quality Gates\n\nWe gate on **mean recall@target**, **stability** (std across folds), and **per\u2011fold safety floor**. "}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def apply_quality_gates(cv_results, cfg):\n    target_recall = cfg[\"evaluation\"][\"target_recall\"]\n    stability_std_max = cfg[\"evaluation\"][\"stability_std_max\"]\n    min_fold_recall = cfg[\"evaluation\"][\"min_fold_recall\"]\n\n    mean_rec = cv_results[\"mean_recall_at_target\"]\n    std_rec  = cv_results[\"std_recall_at_target\"]\n    fold_recalls = [f[\"recall_at_target\"] for f in cv_results[\"folds\"]]\n    min_rec = float(min(fold_recalls)) if fold_recalls else 0.0\n\n    gates = {\n        \"gate_mean_recall\": mean_rec >= target_recall,\n        \"gate_stability\": std_rec <= stability_std_max,\n        \"gate_min_fold\": min_rec >= min_fold_recall,\n    }\n    decision = all(gates.values())\n\n    report = {\n        \"target_recall_threshold\": target_recall,\n        \"stability_std_max\": stability_std_max,\n        \"min_fold_recall\": min_fold_recall,\n        \"observed\": {\n            \"mean_recall_at_target\": mean_rec,\n            \"std_recall_at_target\": std_rec,\n            \"min_fold_recall\": min_rec\n        },\n        \"gates\": gates,\n        \"quality_gates_pass\": decision\n    }\n    return report\n\nquality_report = apply_quality_gates(cv_results, CONFIG)\nquality_report"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83c\udfc6 Compare to Champion in SageMaker Model Registry\n\nWe fetch the **latest Approved** model in the configured **Model Package Group** and read its stored metrics. If metrics are not present, we fall back to gates only."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def get_champion_metrics_from_registry(package_group, metric_key=\"mean_recall_at_target\"):\n    try:\n        import boto3\n        sm = boto3.client(\"sagemaker\")\n\n        # List model packages in the group (most recent first)\n        res = sm.list_model_packages(\n            ModelPackageGroupName=package_group,\n            SortBy=\"CreationTime\",\n            SortOrder=\"Descending\",\n            MaxResults=20\n        )\n        for mp in res.get(\"ModelPackageSummaryList\", []):\n            if mp.get(\"ModelApprovalStatus\") == \"Approved\":\n                desc = sm.describe_model_package(ModelPackageName=mp[\"ModelPackageArn\"])\n                # Prefer CustomerMetadataProperties for simple scalar metrics\n                meta = desc.get(\"CustomerMetadataProperties\") or {}\n                if metric_key in meta:\n                    return {\"metric_key\": metric_key, \"value\": float(meta[metric_key]), \"arn\": mp[\"ModelPackageArn\"]}\n                # Otherwise, try ModelMetrics (URIs to S3 JSON). We skip fetching blobs here.\n                mm = desc.get(\"ModelMetrics\") or {}\n                # If your pipeline writes a compact metrics JSON under ModelQuality -> Metrics, add S3 read here.\n                return {\"metric_key\": metric_key, \"value\": None, \"arn\": mp[\"ModelPackageArn\"]}\n        return None\n    except Exception as e:\n        print(\"Registry lookup failed:\", e)\n        return None\n\nchampion = get_champion_metrics_from_registry(CONFIG[\"registry\"][\"package_group\"])\nchampion"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udea6 Promotion Decision (candidate vs champion)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def decide_promotion(quality_report, cv_results, champion, cfg):\n    # Must pass quality gates\n    if not quality_report[\"quality_gates_pass\"]:\n        return {\"promote\": False, \"reason\": \"Failed quality gates\", \"compare\": None}\n\n    margin = cfg[\"evaluation\"][\"better_than_champion_margin\"]\n    candidate_rec = cv_results[\"mean_recall_at_target\"]\n\n    # If no champion or no metric found, promote based on gates alone\n    if not champion or champion.get(\"value\") is None:\n        return {\"promote\": True, \"reason\": \"Passed gates; no champion metric available\", \"compare\": None}\n\n    champion_rec = champion[\"value\"]\n    better = (candidate_rec >= champion_rec + margin)\n    reason = f\"candidate {candidate_rec:.4f} vs champion {champion_rec:.4f} (margin {margin:.4f})\"\n\n    return {\"promote\": bool(better), \"reason\": reason, \"compare\": {\"candidate\": candidate_rec, \"champion\": champion_rec, \"margin\": margin}}\n\npromotion = decide_promotion(quality_report, cv_results, champion, CONFIG)\npromotion"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udce6 Artifact Checklist\n\nBefore registering, ensure required files exist (and will be included in `model.tar.gz`)."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from pathlib import Path\n\nREQUIRED = {\n    \"model_tar_path\": CONFIG[\"registry\"][\"candidate\"][\"model_tar_path\"],\n    \"inference_script\": CONFIG[\"registry\"][\"candidate\"][\"inference_script\"],\n    \"requirements\": CONFIG[\"registry\"][\"candidate\"][\"requirements\"],\n    \"schema_json\": CONFIG[\"registry\"][\"candidate\"][\"schema_json\"],\n}\n\nmissing = {k:v for k,v in REQUIRED.items() if not Path(v).exists()}\nartifact_ok = len(missing) == 0\n\nartifact_report = {\n    \"required\": REQUIRED,\n    \"missing\": missing,\n    \"artifact_ok\": artifact_ok\n}\nartifact_report"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83e\uddfe Validation Report & (Optional) Registration\n\nIf **gates pass**, **artifacts are OK**, and **candidate beats champion**, we can register to the **Model Registry**. Registration is disabled by default; set `CONFIG['registry']['register_if_pass']=True` to enable."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Assemble validation report\nvalidation_report = {\n    \"run_id\": RUN_ID,\n    \"timestamp_utc\": RUN_TS,\n    \"quality_report\": quality_report,\n    \"cv_results\": cv_results,\n    \"champion\": champion,\n    \"promotion_decision\": promotion,\n    \"artifact_report\": artifact_report\n}\n\nwith open(CONFIG[\"paths\"][\"validation_report_path\"], \"w\") as f:\n    json.dump(validation_report, f, indent=2)\n\nprint(\"Saved report:\", CONFIG[\"paths\"][\"validation_report_path\"])\n\nshould_register = CONFIG[\"registry\"][\"register_if_pass\"] and promotion[\"promote\"] and artifact_report[\"artifact_ok\"]\n\nprint(\"\\nDecision:\")\nprint(\" Gates pass:      \", quality_report[\"quality_gates_pass\"])\nprint(\" Artifact ready:  \", artifact_report[\"artifact_ok\"])\nprint(\" Better than champ:\", promotion[\"promote\"])\nprint(\" Will register:   \", should_register)"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83e\udde9 Register candidate to SageMaker Model Registry (only if allowed by config)\n\n> This section uploads artifacts to S3 (under your prefix) and creates a **Model Package** entry in your **Model Package Group**, setting `CustomerMetadataProperties` for quick metric comparisons. Safe to run: guarded by `should_register` and AWS try/except."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "if should_register:\n    try:\n        import boto3, sagemaker, tarfile, io, s3fs, json as _json\n        sm = boto3.client(\"sagemaker\")\n        s3 = boto3.client(\"s3\")\n\n        # Parse S3 prefix\n        s3_prefix = CONFIG[\"registry\"][\"s3_prefix\"]\n        if not s3_prefix.startswith(\"s3://\"):\n            raise ValueError(\"s3_prefix must be an s3:// URL\")\n        _, s3_rest = s3_prefix.split(\"s3://\", 1)\n        s3_bucket, s3_key_prefix = s3_rest.split(\"/\", 1)\n\n        # Upload artifacts\n        uploads = {}\n        for key, local in CONFIG[\"registry\"][\"candidate\"].items():\n            if key == \"validation_report\":\n                # ensure latest\n                pass\n            if Path(local).exists():\n                dest_key = f\"{s3_key_prefix}/{Path(local).name}\"\n                s3.upload_file(str(local), s3_bucket, dest_key)\n                uploads[key] = f\"s3://{s3_bucket}/{dest_key}\"\n        # Also upload cv summary\n        cv_key = f\"{s3_key_prefix}/cv_summary.json\"\n        s3.upload_file(CONFIG[\"paths\"][\"cv_summary_path\"], s3_bucket, cv_key)\n        uploads[\"cv_summary\"] = f\"s3://{s3_bucket}/{cv_key}\"\n        # Upload validation report\n        vr_key = f\"{s3_key_prefix}/validation_report.json\"\n        s3.upload_file(CONFIG[\"paths\"][\"validation_report_path\"], s3_bucket, vr_key)\n        uploads[\"validation_report\"] = f\"s3://{s3_bucket}/{vr_key}\"\n\n        # Build ModelMetrics (optional pointers to S3 JSONs)\n        model_metrics = {\n            \"ModelQuality\": {\n                \"Statistics\": {\"S3Uri\": uploads[\"cv_summary\"], \"ContentType\": \"application/json\"},\n                \"Constraints\": {\"S3Uri\": uploads[\"validation_report\"], \"ContentType\": \"application/json\"}\n            }\n        }\n\n        # Inference spec (script mode or pre-built container). If you use a framework container, provide image.\n        image_uri = CONFIG[\"registry\"][\"container_image_uri\"] or sagemaker.image_uris.retrieve(\"pytorch\", boto3.Session().region_name, version=\"2.0\", image_scope=\"inference\")\n        primary_container = {\n            \"Image\": image_uri,\n            \"ModelDataUrl\": uploads.get(\"model_tar_path\"),\n            \"Environment\": {\n                \"SAGEMAKER_PROGRAM\": Path(CONFIG[\"registry\"][\"candidate\"][\"inference_script\"]).name,\n                \"SAGEMAKER_SUBMIT_DIRECTORY\": CONFIG[\"registry\"][\"candidate\"][\"model_tar_path\"],\n                \"SAGEMAKER_REQUIREMENTS\": Path(CONFIG[\"registry\"][\"candidate\"][\"requirements\"]).name,\n            }\n        }\n\n        # Ensure group exists (idempotent)\n        try:\n            sm.describe_model_package_group(ModelPackageGroupName=CONFIG[\"registry\"][\"package_group\"])\n        except sm.exceptions.ClientError:\n            sm.create_model_package_group(\n                ModelPackageGroupName=CONFIG[\"registry\"][\"package_group\"],\n                ModelPackageGroupDescription=\"Model group created by validation notebook\"\n            )\n\n        # Create Model Package\n        resp = sm.create_model_package(\n            ModelPackageGroupName=CONFIG[\"registry\"][\"package_group\"],\n            ModelPackageDescription=\"Registered via validation notebook\",\n            InferenceSpecification={\n                \"Containers\": [primary_container],\n                \"SupportedContentTypes\": [\"application/json\", \"text/csv\"],\n                \"SupportedResponseMIMETypes\": [\"application/json\"]\n            },\n            ModelMetrics=model_metrics,\n            ModelApprovalStatus=CONFIG[\"registry\"][\"model_approval_status\"],\n            CustomerMetadataProperties={\n                \"mean_recall_at_target\": str(cv_results[\"mean_recall_at_target\"]),\n                \"std_recall_at_target\": str(cv_results[\"std_recall_at_target\"]),\n                \"mean_pr_auc\": str(cv_results[\"mean_pr_auc\"]),\n                \"mean_roc_auc\": str(cv_results[\"mean_roc_auc\"]),\n                \"validation_report\": uploads[\"validation_report\"],\n            }\n        )\n        print(\"\u2705 Registered model package:\", resp[\"ModelPackageArn\"])\n    except Exception as e:\n        print(\"\u274c Registration failed:\", e)\nelse:\n    print(\"Skipping registration (conditions not met or disabled).\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83d\udccc Summary\n\n- **Quality gates:** evaluated and recorded in `validation_report.json`  \n- **Champion comparison:** used the latest **Approved** package in the **Model Package Group** (if any)  \n- **Artifacts check:** ensures `model.tar.gz`, `inference.py`, `requirements.txt`, and `feature_schema.json` exist before registration  \n- **Registration:** guarded by gates + champion comparison + artifacts presence\n\n> Tip: wire this notebook into your CI/CD as a **pre\u2011promotion** job. Keep thresholds versioned with infra-as-code (e.g., in your pipeline repo)."}]}