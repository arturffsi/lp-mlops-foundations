{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c4aca6",
   "metadata": {},
   "source": [
    "# 📊 Week 1 — Exploratory Data Analysis (EDA): Reading Guide\n",
    "\n",
    "**Learning Objectives (Week 1 – EDA)**  \n",
    "- Understand the motivation for MLOps and how EDA fits into a production ML lifecycle.  \n",
    "- Connect to Redshift and perform reproducible EDA.  \n",
    "- Document data quality issues and define target/feature schema.  \n",
    "- Prepare train/validation/test splits with leakage-aware methodology.  \n",
    "\n",
    "> **Context**: ZAP is targeting **MLOps Level 2**. Even EDA should be reproducible and versioned (data query, sampling, and preprocessing code committed).\n",
    "\n",
    "## 🔍 What is EDA and Why It Matters\n",
    "Exploratory Data Analysis (EDA) is the process of **exploring, visualizing, and validating datasets** before training models.  \n",
    "In **MLOps**, EDA is about much more than plots — it’s about **data reliability** and ensuring downstream pipelines are stable.\n",
    "\n",
    "**Why it matters for production:**\n",
    "- 🗑️ **Garbage in, garbage out** → poor data = poor models.  \n",
    "- ⚡ **Operational resilience** → detect defects early, before they hit production.  \n",
    "- 🔁 **Pipeline reliability** → schemas and checks from EDA become the foundation for automation.  \n",
    "\n",
    "\n",
    "## 📐 Data Quality Dimensions\n",
    "Checking data quality ensures your model won’t collapse when facing real-world inputs. Here are the key dimensions:\n",
    "\n",
    "| Dimension    | Question to Ask | Example Issue |\n",
    "|--------------|-----------------|---------------|\n",
    "| ✅ Completeness | Are required values present? | Missing customer age |\n",
    "| 🔄 Consistency | Do values follow expected formats/relations? | Country code \"PT\" inconsistently mapped |\n",
    "| 🎯 Accuracy | Are values correct? | Negative product price |\n",
    "| 🧩 Validity | Do values conform to rules/types? | Dates stored as free-text |\n",
    "| ⏱️ Timeliness | Is the data up to date? | Using last year’s sales for today’s forecast |\n",
    "\n",
    "\n",
    "## ⚠️ Leakage and Target Contamination\n",
    "- **Data leakage** → using information not available at prediction time.  \n",
    "- **Target contamination** → when the target leaks into features or data splits.  \n",
    "\n",
    "❌ Example leakage: Using \"credit approval status\" as a feature to predict loan approval.  \n",
    "❌ Example contamination: Randomly splitting time-series data, letting future events “leak” into training.\n",
    "\n",
    "➡️ Both lead to inflated metrics **during training** and catastrophic failures **in production**.\n",
    "\n",
    "\n",
    "## ♻️ Reproducibility\n",
    "Reproducibility = **same results given same inputs**. Essential for trust, debugging, and collaboration.\n",
    "\n",
    "Key practices:\n",
    "- 🎲 **Fixed seeds** → ensure reproducible sampling/splitting.  \n",
    "- 📑 **Deterministic queries** → e.g., always `ORDER BY id` in SQL.  \n",
    "- 🖥️ **Environment capture** → record Python & library versions, OS, hardware.  \n",
    "\n",
    "Without reproducibility → experiments can’t be compared, bugs can’t be traced.\n",
    "\n",
    "\n",
    "## 📦 Outputs That Feed the Pipeline\n",
    "EDA is not a one-off. Its **outputs become artifacts** for the ML pipeline:\n",
    "\n",
    "- 🗂️ **Feature schema** → defines types, ranges, categories, nullability.  \n",
    "- ✅ **Data checks** → rules like “no nulls in IDs” or “target is binary.”  \n",
    "- ✂️ **Split strategy** → deterministic, leakage-free train/val/test partitions.  \n",
    "\n",
    "These artifacts support:\n",
    "- Automation in CI/CD ✅  \n",
    "- Monitoring in production 📈  \n",
    "- MLOps Level 2 maturity ⚙️  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00348880",
   "metadata": {},
   "source": [
    "# 📝 Exercises - Build the Dataset\n",
    "\n",
    "You should choose any dataset existing on Redshift to practice EDA, and gather relevant information to train your model.  \n",
    "Dataset example should contain customer demographics, services, account info, etc.\n",
    "\n",
    "\n",
    "## 🔧 Setup\n",
    "Use the function `load_data()` provided in file `data_io.py` snippet to create a dataset from `parquet` on S3 bucket or directly from `redshift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0a7e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953585ae",
   "metadata": {},
   "source": [
    "## 1. Data Overview & Metadata\n",
    "Inspect the dataset:\n",
    "- Number of rows and columns.  \n",
    "- Data types of each column.  \n",
    "- Identify categorical, numerical, and target.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0401b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d4f1b",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks\n",
    "After identifying the tables you want to work on, a crucial step is to analyze their Data Quality using the following dimensions.\n",
    "Check the **5 quality dimensions** on this dataset:\n",
    "\n",
    "| Dimension    | Task |\n",
    "|--------------|------|\n",
    "| ✅ Completeness | Count missing/null values in each column. |\n",
    "| 🔄 Consistency | Look for inconsistent categories (e.g., “Male” vs. “male”). |\n",
    "| 🎯 Accuracy | Spot anomalies (e.g., negative charges). |\n",
    "| 🧩 Validity | Ensure logics are met. Ex: `TotalProfit ≈ n_units × unitary_profit`. |\n",
    "| ⏱️ Timeliness | Discuss whether tenure captures freshness of data. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66567d2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660f917",
   "metadata": {},
   "source": [
    "## 3. Target Variable Exploration\n",
    "- Plot the distribution of target table.  \n",
    "- Discuss if the dataset is **imbalanced** and what that implies for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109721e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3fd8e",
   "metadata": {},
   "source": [
    "## 4. Univariate Analysis\n",
    "- For numerical columns:  \n",
    "  - Plot histograms & boxplots.  \n",
    "  - Identify outliers and skewed distributions.  \n",
    "\n",
    "- For categorical columns:  \n",
    "  - Plot bar charts of category counts.  \n",
    "  - Check if categories have enough representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3383868",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e186cd",
   "metadata": {},
   "source": [
    "## 5. Bivariate Analysis\n",
    "- Compare target column across categorical columns :  \n",
    "\n",
    "- Compare target column across numerical features:  \n",
    "  - Ex: How does the values of a column grow proportinally and disproportionally in relation with another; Compare average column values for different target column values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecb075",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daf055",
   "metadata": {},
   "source": [
    "## 7. Reproducibility Practices\n",
    "- Set a **random seed** when sampling rows for inspection.  \n",
    "- Save an **EDA profile report** .  \n",
    "- Export a **feature schema JSON** with column names, types, and allowed ranges/categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f4643",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73137409",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split Strategy\n",
    "- Propose and implement a split strategy:  \n",
    "  - Random stratified split by target column.  \n",
    "  - Ensure reproducibility with a fixed random seed.  \n",
    "  - Document why stratification is necessary here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bf1e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a50131",
   "metadata": {},
   "source": [
    "# 🎯 Deliverables\n",
    "By the end of these exercises, you should have:\n",
    "1. A **data dictionary**.  \n",
    "2. Summary tables/plots of findings and key features.  \n",
    "3. A **feature schema JSON** with data types and constraints.  \n",
    "4. A **train/val/test split file** (e.g., `splits.json`) for reproducible downstream tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec284c",
   "metadata": {},
   "source": [
    "## Peer Validation\n",
    "  - Reproducible data loading (query or seed).  \n",
    "  - Clear schema with rationale per feature.  \n",
    "  - Split method documented and leakage-safe.  \n",
    "  - Artifacts present and versioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f736fde",
   "metadata": {},
   "source": [
    "# 📊 Week 1 — Exploratory Data Analysis (EDA): Reading Guide\n",
    "\n",
    "**Learning Objectives (Week 1 – EDA)**  \n",
    "- Understand the motivation for MLOps and how EDA fits into a production ML lifecycle.  \n",
    "- Connect to Redshift and perform reproducible EDA.  \n",
    "- Document data quality issues and define target/feature schema.  \n",
    "- Prepare train/validation/test splits with leakage-aware methodology.  \n",
    "\n",
    "> **Context**: ZAP is targeting **MLOps Level 2**. Even EDA should be reproducible and versioned (data query, sampling, and preprocessing code committed).\n",
    "\n",
    "## 🔍 What is EDA and Why It Matters\n",
    "Exploratory Data Analysis (EDA) is the process of **exploring, visualizing, and validating datasets** before training models.  \n",
    "In **MLOps**, EDA is about much more than plots — it’s about **data reliability** and ensuring downstream pipelines are stable.\n",
    "\n",
    "**Why it matters for production:**\n",
    "- 🗑️ **Garbage in, garbage out** → poor data = poor models.  \n",
    "- ⚡ **Operational resilience** → detect defects early, before they hit production.  \n",
    "- 🔁 **Pipeline reliability** → schemas and checks from EDA become the foundation for automation.  \n",
    "\n",
    "\n",
    "## 📐 Data Quality Dimensions\n",
    "Checking data quality ensures your model won’t collapse when facing real-world inputs. Here are the key dimensions:\n",
    "\n",
    "| Dimension    | Question to Ask | Example Issue |\n",
    "|--------------|-----------------|---------------|\n",
    "| ✅ Completeness | Are required values present? | Missing customer age |\n",
    "| 🔄 Consistency | Do values follow expected formats/relations? | Country code \"PT\" inconsistently mapped |\n",
    "| 🎯 Accuracy | Are values correct? | Negative product price |\n",
    "| 🧩 Validity | Do values conform to rules/types? | Dates stored as free-text |\n",
    "| ⏱️ Timeliness | Is the data up to date? | Using last year’s sales for today’s forecast |\n",
    "\n",
    "\n",
    "## ⚠️ Leakage and Target Contamination\n",
    "- **Data leakage** → using information not available at prediction time.  \n",
    "- **Target contamination** → when the target leaks into features or data splits.  \n",
    "\n",
    "❌ Example leakage: Using \"credit approval status\" as a feature to predict loan approval.  \n",
    "❌ Example contamination: Randomly splitting time-series data, letting future events “leak” into training.\n",
    "\n",
    "➡️ Both lead to inflated metrics **during training** and catastrophic failures **in production**.\n",
    "\n",
    "\n",
    "## ♻️ Reproducibility\n",
    "Reproducibility = **same results given same inputs**. Essential for trust, debugging, and collaboration.\n",
    "\n",
    "Key practices:\n",
    "- 🎲 **Fixed seeds** → ensure reproducible sampling/splitting.  \n",
    "- 📑 **Deterministic queries** → e.g., always `ORDER BY id` in SQL.  \n",
    "- 🖥️ **Environment capture** → record Python & library versions, OS, hardware.  \n",
    "\n",
    "Without reproducibility → experiments can’t be compared, bugs can’t be traced.\n",
    "\n",
    "\n",
    "## 📦 Outputs That Feed the Pipeline\n",
    "EDA is not a one-off. Its **outputs become artifacts** for the ML pipeline:\n",
    "\n",
    "- 🗂️ **Feature schema** → defines types, ranges, categories, nullability.  \n",
    "- ✅ **Data checks** → rules like “no nulls in IDs” or “target is binary.”  \n",
    "- ✂️ **Split strategy** → deterministic, leakage-free train/val/test partitions.  \n",
    "\n",
    "These artifacts support:\n",
    "- Automation in CI/CD ✅  \n",
    "- Monitoring in production 📈  \n",
    "- MLOps Level 2 maturity ⚙️  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daed676",
   "metadata": {},
   "source": [
    "# 📝 Exercises - Build the Dataset\n",
    "\n",
    "You should choose any dataset existing on Redshift to practice EDA, and gather relevant information to train your model.  \n",
    "Dataset example should contain customer demographics, services, account info, etc.\n",
    "\n",
    "\n",
    "## 🔧 Setup\n",
    "Use the function `load_data()` provided in file `data_io.py` snippet to create a dataset from `parquet` on S3 bucket or directly from `redshift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b9c94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73d89c",
   "metadata": {},
   "source": [
    "## 1. Data Overview & Metadata\n",
    "Inspect the dataset:\n",
    "- Number of rows and columns.  \n",
    "- Data types of each column.  \n",
    "- Identify categorical, numerical, and target.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1f4ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753d30f",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks\n",
    "After identifying the tables you want to work on, a crucial step is to analyze their Data Quality using the following dimensions.\n",
    "Check the **5 quality dimensions** on this dataset:\n",
    "\n",
    "| Dimension    | Task |\n",
    "|--------------|------|\n",
    "| ✅ Completeness | Count missing/null values in each column. |\n",
    "| 🔄 Consistency | Look for inconsistent categories (e.g., “Male” vs. “male”). |\n",
    "| 🎯 Accuracy | Spot anomalies (e.g., negative charges). |\n",
    "| 🧩 Validity | Ensure logics are met. Ex: `TotalProfit ≈ n_units × unitary_profit`. |\n",
    "| ⏱️ Timeliness | Discuss whether tenure captures freshness of data. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5328173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2e947",
   "metadata": {},
   "source": [
    "## 3. Target Variable Exploration\n",
    "- Plot the distribution of target table.  \n",
    "- Discuss if the dataset is **imbalanced** and what that implies for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0150d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014bbc8",
   "metadata": {},
   "source": [
    "## 4. Univariate Analysis\n",
    "- For numerical columns:  \n",
    "  - Plot histograms & boxplots.  \n",
    "  - Identify outliers and skewed distributions.  \n",
    "\n",
    "- For categorical columns:  \n",
    "  - Plot bar charts of category counts.  \n",
    "  - Check if categories have enough representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a832e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0066966",
   "metadata": {},
   "source": [
    "## 5. Bivariate Analysis\n",
    "- Compare target column across categorical columns :  \n",
    "\n",
    "- Compare target column across numerical features:  \n",
    "  - Ex: How does the values of a column grow proportinally and disproportionally in relation with another; Compare average column values for different target column values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497f01b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1a90d",
   "metadata": {},
   "source": [
    "## 7. Reproducibility Practices\n",
    "- Set a **random seed** when sampling rows for inspection.  \n",
    "- Save an **EDA profile report** .  \n",
    "- Export a **feature schema JSON** with column names, types, and allowed ranges/categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997c649",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b45b91",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split Strategy\n",
    "- Propose and implement a split strategy:  \n",
    "  - Random stratified split by target column.  \n",
    "  - Ensure reproducibility with a fixed random seed.  \n",
    "  - Document why stratification is necessary here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e4d96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b664b6",
   "metadata": {},
   "source": [
    "# 🎯 Deliverables\n",
    "By the end of these exercises, you should have:\n",
    "1. A **data dictionary**.  \n",
    "2. Summary tables/plots of findings and key features.  \n",
    "3. A **feature schema JSON** with data types and constraints.  \n",
    "4. A **train/val/test split file** (e.g., `splits.json`) for reproducible downstream tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929c796",
   "metadata": {},
   "source": [
    "## Peer Validation\n",
    "  - Reproducible data loading (query or seed).  \n",
    "  - Clear schema with rationale per feature.  \n",
    "  - Split method documented and leakage-safe.  \n",
    "  - Artifacts present and versioned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
