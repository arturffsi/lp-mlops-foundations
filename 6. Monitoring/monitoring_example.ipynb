{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f1d2bc3",
      "metadata": {},
      "source": [
        "# ðŸ›°ï¸ Deployment â€” Model & **Data Monitoring** (SageMaker)\n",
        "\n",
        "**Purpose:** Enable *production-grade monitoring* for the model deployed in `deployment_example.ipynb`:\n",
        "- Turn on **data capture** at the endpoint\n",
        "- Create **Model Monitor** schedules (Data Quality; optional: Model Quality)\n",
        "- Generate/attach **baselines** (statistics + constraints)\n",
        "- Inspect recent executions & **constraint violations**\n",
        "- Add/propagate **lineage tags** to the endpoint\n",
        "\n",
        "> Works in **SageMaker Studio â€“ Code Editor** or locally (with AWS creds). Uses the same endpoint/models (CatBoost/XGBoost) you deployed earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e4ed71",
      "metadata": {},
      "source": [
        "## ðŸ§° Prerequisites\n",
        "- You already ran your prior **deployment** notebook and have a live SageMaker **endpoint**.\n",
        "- You packaged your model with a valid **inference contract** (root `inference.py` & `requirements.txt`).\n",
        "- (Optional) You produced a **baseline dataset** CSV (header=True) from your training/validation data.\n",
        "\n",
        "If you haven't enabled data capture yet, this notebook will **update the endpoint config** to enable it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52766d7e",
      "metadata": {},
      "source": [
        "## â™»ï¸ Reproducibility & Environment Capture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7799ab1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, platform, random, hashlib\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "RUN_TS = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
        "RUN_ID = hashlib.sha1(f\"{RUN_TS}-{SEED}\".encode()).hexdigest()[:10]\n",
        "ARTIFACT_DIR = os.environ.get('ARTIFACT_DIR', f\"artifacts/monitoring_run_{RUN_TS}_{RUN_ID}\")\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "env_info = {\n",
        "    'python': sys.version,\n",
        "    'platform': platform.platform(),\n",
        "    'timestamp_utc': RUN_TS,\n",
        "    'seed': SEED,\n",
        "    'packages': {'pandas': pd.__version__, 'numpy': np.__version__}\n",
        "}\n",
        "with open(os.path.join(ARTIFACT_DIR, 'env_info.json'), 'w') as f:\n",
        "    json.dump(env_info, f, indent=2)\n",
        "env_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6bb19df",
      "metadata": {},
      "source": [
        "## âš™ï¸ Configuration\n",
        "Single source of truth for endpoint, capture, and monitor settings. **Adjust names/paths as needed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5093f2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "CONFIG = {\n",
        "    'aws': {\n",
        "        'region': os.environ.get('AWS_REGION', 'eu-west-1'),\n",
        "        'role': os.environ.get('SM_EXECUTION_ROLE', ''),  # if empty, we'll resolve from sagemaker\n",
        "        'default_bucket': os.environ.get('SM_DEFAULT_BUCKET', ''),\n",
        "    },\n",
        "    'deployment': {\n",
        "        # Endpoint deployed previously (Champion or Candidate)\n",
        "        'endpoint_name': os.environ.get('ENDPOINT_NAME', 'mlops-demo-endpoint'),\n",
        "        # Optional: tags propagated onto the endpoint\n",
        "        'lineage_tags': {\n",
        "            'candidate_run_id': os.environ.get('CANDIDATE_RUN_ID', ''),\n",
        "            'data_version': os.environ.get('DATA_VERSION', ''),\n",
        "            'model_family': os.environ.get('MODEL_FAMILY', ''),  # e.g., catboost|xgboost|sklearn\n",
        "        },\n",
        "        'data_capture': {\n",
        "            'enable': True,\n",
        "            'capture_percentage': 100,           # 0-100\n",
        "            'kms_key_id': os.environ.get('CAPTURE_KMS_KEY', ''),\n",
        "            's3_prefix': os.environ.get('CAPTURE_S3_PREFIX', f\"s3://{os.environ.get('SM_DEFAULT_BUCKET','')}/data-capture/{datetime.utcnow().strftime('%Y%m%d')}\")\n",
        "        }\n",
        "    },\n",
        "    'monitoring': {\n",
        "        'enable': True,\n",
        "        'instance_type': 'ml.m5.large',\n",
        "        'volume_size_gb': 30,\n",
        "        'max_runtime_seconds': 3600,\n",
        "        'schedule_cron': 'cron(0 */4 * * ? *)',   # every 4 hours\n",
        "        # Provide either a ready CSV (with header) or let this notebook export one from parquet\n",
        "        'baseline_dataset_uri': os.environ.get('BASELINE_DATASET_URI', ''),\n",
        "        'fallback_parquet_uri': os.environ.get('BASELINE_PARQUET_URI', ''),  # e.g., s3://.../processed/dataset.parquet\n",
        "        'baseline_sample_rows': 25000,\n",
        "        'problem_type': 'BinaryClassification',  # for ModelQuality monitor\n",
        "        'ground_truth_s3_uri': os.environ.get('GROUND_TRUTH_S3_URI', ''),    # CSV with ground-truth labels matching capture\n",
        "    }\n",
        "}\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c076883",
      "metadata": {},
      "source": [
        "## ðŸ”Œ AWS Session & Endpoint\n",
        "Resolve session, role, and sanity-check that the endpoint exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1d1fff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3, sagemaker\n",
        "from sagemaker import Session\n",
        "from sagemaker.session import production_variant\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "region = CONFIG['aws']['region']\n",
        "sm_sess = sagemaker.Session()\n",
        "sm_client = sm_sess.sagemaker_client\n",
        "runtime_sm = sm_sess.sagemaker_runtime_client\n",
        "role = CONFIG['aws']['role'] or sagemaker.get_execution_role()\n",
        "bucket = CONFIG['aws']['default_bucket'] or sm_sess.default_bucket()\n",
        "\n",
        "endpoint_name = CONFIG['deployment']['endpoint_name']\n",
        "print('Region:', region)\n",
        "print('Role  :', role)\n",
        "print('Bucket:', bucket)\n",
        "print('Endpoint:', endpoint_name)\n",
        "\n",
        "# Validate endpoint exists\n",
        "try:\n",
        "    desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
        "    print('Endpoint status:', desc['EndpointStatus'])\n",
        "except ClientError as e:\n",
        "    raise RuntimeError(f\"Endpoint '{endpoint_name}' not found or not accessible: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f969609",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Add/Update Lineage Tags on Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd7a40a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tags = [{ 'Key': k, 'Value': v } for k, v in CONFIG['deployment']['lineage_tags'].items() if v]\n",
        "if tags:\n",
        "    sm_client.add_tags(ResourceArn=desc['EndpointArn'], Tags=tags)\n",
        "    print('âœ… Added/updated lineage tags on endpoint:', {t['Key']: t['Value'] for t in tags})\n",
        "else:\n",
        "    print('No lineage tags provided; skipping.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306b5b8d",
      "metadata": {},
      "source": [
        "## ðŸ“¡ Ensure **Data Capture** Is Enabled\n",
        "If disabled, update the endpoint configuration to turn it on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8711b57b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model_monitor import DataCaptureConfig\n",
        "\n",
        "cap_cfg = CONFIG['deployment']['data_capture']\n",
        "capture_prefix = cap_cfg['s3_prefix']\n",
        "\n",
        "# Discover current config\n",
        "ep_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
        "epc_name = ep_desc['EndpointConfigName']\n",
        "epc_desc = sm_client.describe_endpoint_config(EndpointConfigName=epc_name)\n",
        "already_enabled = 'DataCaptureConfig' in epc_desc\n",
        "\n",
        "if already_enabled:\n",
        "    print('â„¹ï¸ Data capture already enabled on the endpoint config:', epc_name)\n",
        "else:\n",
        "    print('Enabling data capture on a new endpoint configâ€¦')\n",
        "    # Recreate endpoint config with capture\n",
        "    variants = epc_desc['ProductionVariants']\n",
        "    new_epc_name = f\"{epc_name}-cap-{RUN_ID}\"\n",
        "    \n",
        "    data_capture = {\n",
        "        'EnableCapture': cap_cfg['enable'],\n",
        "        'InitialSamplingPercentage': int(cap_cfg['capture_percentage']),\n",
        "        'DestinationS3Uri': capture_prefix,\n",
        "        'KmsKeyId': cap_cfg['kms_key_id'] or None,\n",
        "        'CaptureOptions': [{'CaptureMode': 'Input'}, {'CaptureMode': 'Output'}],\n",
        "        'CaptureContentTypeHeader': {'CsvContentTypes': ['text/csv'], 'JsonContentTypes': ['application/json']}\n",
        "    }\n",
        "    \n",
        "    sm_client.create_endpoint_config(\n",
        "        EndpointConfigName=new_epc_name,\n",
        "        ProductionVariants=variants,\n",
        "        DataCaptureConfig=data_capture\n",
        "    )\n",
        "    sm_client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=new_epc_name)\n",
        "    print('â³ Updating endpoint to new config with captureâ€¦ (check console for progress)')\n",
        "\n",
        "print('Capture S3 prefix:', capture_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1276cf3",
      "metadata": {},
      "source": [
        "## ðŸ§± Prepare / Locate a **Baseline Dataset**\n",
        "Model Monitor needs a representative CSV with header for **Data Quality** baselines.\n",
        "- If you already have one, set `CONFIG['monitoring']['baseline_dataset_uri']` to S3 path.\n",
        "- Otherwise, we can **export a CSV** from a processed parquet (sample)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8941d75e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import s3fs\n",
        "\n",
        "mon_cfg = CONFIG['monitoring']\n",
        "baseline_uri = mon_cfg['baseline_dataset_uri']\n",
        "\n",
        "if not baseline_uri and mon_cfg['fallback_parquet_uri']:\n",
        "    print('ðŸ” Building a baseline CSV from parquet sampleâ€¦')\n",
        "    df_parq = pd.read_parquet(mon_cfg['fallback_parquet_uri'])\n",
        "    # Simple sample; consider stratified/time-based sampling for your case\n",
        "    n = min(len(df_parq), mon_cfg['baseline_sample_rows'])\n",
        "    df_sample = df_parq.sample(n=n, random_state=SEED)\n",
        "    baseline_uri = f\"s3://{bucket}/monitoring/baseline/{RUN_TS}/baseline.csv\"\n",
        "    df_sample.to_csv(baseline_uri, index=False)\n",
        "    print('âœ… Baseline CSV written to:', baseline_uri)\n",
        "elif baseline_uri:\n",
        "    print('Using provided baseline CSV:', baseline_uri)\n",
        "else:\n",
        "    print('âš ï¸ No baseline provided and no parquet fallback set. You can still create a schedule without baselines, but it is recommended to provide them.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24f38a9",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ Create **Data Quality** Monitoring Schedule (DefaultModelMonitor)\n",
        "Generates baselines (if available) and creates a recurring schedule. This expands the snippet you provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce680fcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat\n",
        "\n",
        "if CONFIG['monitoring']['enable'] and CONFIG['deployment']['data_capture']['enable']:\n",
        "    try:\n",
        "        mon = DefaultModelMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG['monitoring']['instance_type'],\n",
        "            volume_size_in_gb=CONFIG['monitoring']['volume_size_gb'],\n",
        "            max_runtime_in_seconds=CONFIG['monitoring']['max_runtime_seconds'],\n",
        "            sagemaker_session=sm_sess,\n",
        "        )\n",
        "        stats_s3, cons_s3 = None, None\n",
        "        if baseline_uri:\n",
        "            print('Suggesting baselines from:', baseline_uri)\n",
        "            baseline_job = mon.suggest_baseline(\n",
        "                baseline_dataset=baseline_uri,\n",
        "                dataset_format=DatasetFormat.csv(header=True),\n",
        "                output_s3_uri=f\"s3://{bucket}/monitoring/baseline/{RUN_TS}\",\n",
        "                wait=False\n",
        "            )\n",
        "            print('Baseline suggestion started. Job name:', baseline_job.job_name)\n",
        "            # We won't block on baseline job; schedule can be created without explicit stats/constraints\n",
        "        schedule_name = f\"monitor-{CONFIG['deployment']['endpoint_name']}\"\n",
        "        mon.create_monitoring_schedule(\n",
        "            monitor_schedule_name=schedule_name,\n",
        "            endpoint_input=CONFIG['deployment']['endpoint_name'],\n",
        "            statistics=None,   # could point to baseline output s3 uris once completed\n",
        "            constraints=None,\n",
        "            schedule_cron_expression=CONFIG['monitoring']['schedule_cron']\n",
        "        )\n",
        "        print('âœ… Monitoring schedule created:', schedule_name)\n",
        "    except Exception as e:\n",
        "        print('âš ï¸ Model Monitor setup skipped/failed:', e)\n",
        "else:\n",
        "    print('Monitoring disabled or data capture off.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d74e89",
      "metadata": {},
      "source": [
        "## (Optional) ðŸ§ª Model Quality Monitor\n",
        "Requires **ground-truth labels** uploaded to S3. Only enable if you have a reliable feedback pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ca9e15",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model_monitor import ModelQualityMonitor\n",
        "\n",
        "gt_uri = CONFIG['monitoring']['ground_truth_s3_uri']\n",
        "if CONFIG['monitoring']['enable'] and gt_uri:\n",
        "    try:\n",
        "        mq = ModelQualityMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG['monitoring']['instance_type'],\n",
        "            volume_size_in_gb=CONFIG['monitoring']['volume_size_gb'],\n",
        "            max_runtime_in_seconds=CONFIG['monitoring']['max_runtime_seconds'],\n",
        "            sagemaker_session=sm_sess,\n",
        "            problem_type=CONFIG['monitoring']['problem_type']\n",
        "        )\n",
        "        mq_schedule = f\"model-quality-{CONFIG['deployment']['endpoint_name']}\"\n",
        "        mq.create_monitoring_schedule(\n",
        "            monitor_schedule_name=mq_schedule,\n",
        "            endpoint_input=CONFIG['deployment']['endpoint_name'],\n",
        "            ground_truth_input=gt_uri,\n",
        "            schedule_cron_expression=CONFIG['monitoring']['schedule_cron']\n",
        "        )\n",
        "        print('âœ… ModelQuality schedule created:', mq_schedule)\n",
        "    except Exception as e:\n",
        "        print('âš ï¸ ModelQuality setup skipped/failed:', e)\n",
        "else:\n",
        "    print('ModelQuality disabled or ground truth not provided.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c23924c5",
      "metadata": {},
      "source": [
        "## ðŸ” Inspect Schedules & Latest Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1622805",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_schedules(prefix: str):\n",
        "    res = sm_client.list_monitoring_schedules(NameContains=prefix)\n",
        "    return pd.DataFrame(res.get('MonitoringScheduleSummaries', []))\n",
        "\n",
        "sched_df = list_schedules(f\"monitor-{endpoint_name}\")\n",
        "sched_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ff6106",
      "metadata": {},
      "outputs": [],
      "source": [
        "def latest_exec(sched_name: str):\n",
        "    res = sm_client.describe_monitoring_schedule(MonitoringScheduleName=sched_name)\n",
        "    summary = res['MonitoringScheduleStatus']\n",
        "    last_exec = res.get('LastMonitoringExecutionSummary')\n",
        "    return summary, last_exec\n",
        "\n",
        "if not sched_df.empty:\n",
        "    sname = sched_df.iloc[0]['MonitoringScheduleName']\n",
        "    status, last = latest_exec(sname)\n",
        "    print('Schedule:', sname, '| Status:', status)\n",
        "    print('Last execution summary:')\n",
        "    print(json.dumps(last or {}, indent=2, default=str))\n",
        "else:\n",
        "    print('No schedules found (yet).')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831d230e",
      "metadata": {},
      "source": [
        "## ðŸ§¾ Read Constraint Violations (if any)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "804860bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, io, gzip\n",
        "import botocore\n",
        "\n",
        "def read_s3_json(s3_uri):\n",
        "    s3 = boto3.resource('s3')\n",
        "    m = re.match(r's3://([^/]+)/(.+)$', s3_uri)\n",
        "    if not m: return None\n",
        "    b, k = m.group(1), m.group(2)\n",
        "    obj = s3.Object(b, k).get()['Body'].read()\n",
        "    try:\n",
        "        return json.loads(obj)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return json.loads(gzip.decompress(obj))\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def show_latest_violations(sched_name: str):\n",
        "    res = sm_client.describe_monitoring_schedule(MonitoringScheduleName=sched_name)\n",
        "    last = res.get('LastMonitoringExecutionSummary', {})\n",
        "    rep = last.get('MonitoringOutput', {}).get('S3Output', {}).get('S3Uri')\n",
        "    if not rep:\n",
        "        print('No execution outputs yet.')\n",
        "        return\n",
        "    # Typical keys contain statistics.json / constraints.json\n",
        "    print('Execution output S3:', rep)\n",
        "    # Users can browse in S3 console; programmatic listing requires S3 ListObjects which we omit for brevity.\n",
        "\n",
        "if not sched_df.empty:\n",
        "    show_latest_violations(sched_df.iloc[0]['MonitoringScheduleName'])\n",
        "else:\n",
        "    print('No schedules to inspect.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af718c7b",
      "metadata": {},
      "source": [
        "## ðŸ§­ Peek at Captured Records (sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998c22e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import s3fs, glob\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "cap_uri = CONFIG['deployment']['data_capture']['s3_prefix']\n",
        "print('Capture prefix:', cap_uri)\n",
        "fs = s3fs.S3FileSystem()\n",
        "try:\n",
        "    # This lists objects, but the exact path includes endpoint name/date; we glob broadly\n",
        "    bucket_path = cap_uri.replace('s3://', '')\n",
        "    bucket, prefix = bucket_path.split('/', 1)\n",
        "    objs = fs.glob(f\"{bucket}/{prefix}/**/*.jsonl*\")\n",
        "    print('Found captured files:', len(objs))\n",
        "    if objs:\n",
        "        with fs.open(objs[0], 'rb') as f:\n",
        "            raw = f.read()\n",
        "        try:\n",
        "            txt = raw.decode('utf-8')\n",
        "        except Exception:\n",
        "            import gzip\n",
        "            txt = gzip.decompress(raw).decode('utf-8')\n",
        "        print('--- SAMPLE CAPTURED RECORD ---')\n",
        "        print('\\n'.join(txt.splitlines()[:3]))\n",
        "    else:\n",
        "        print('No capture files yet. Send traffic to the endpoint and re-run.')\n",
        "except Exception as e:\n",
        "    print('Could not list/read capture files:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb32f27e",
      "metadata": {},
      "source": [
        "## ðŸ§¹ (Optional) Clean Up\n",
        "Helpers to delete schedules. **Do not run in production without a plan.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c0d4a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_schedule(name: str):\n",
        "    try:\n",
        "        sm_client.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
        "        print('Deleted schedule:', name)\n",
        "    except ClientError as e:\n",
        "        print('Skip/delete failed:', e)\n",
        "\n",
        "# Example:\n",
        "# delete_schedule(f\"monitor-{endpoint_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9876dd5e",
      "metadata": {},
      "source": [
        "## âœ… Summary\n",
        "- Data capture **enabled** or verified\n",
        "- Data Quality **schedule** created (and Model Quality optional)\n",
        "- **Baselines** suggested (if dataset provided)\n",
        "- **Lineage tags** attached to endpoint\n",
        "- Utilities to **inspect executions** and **peek captures**\n",
        "\n",
        "Next steps: wire **Model Monitor** baselines & constraints into your **validation gates**, configure **bias/explainability** monitors, and connect CloudWatch alarms to operational SLOs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f24252",
      "metadata": {},
      "source": [
        "\n",
        "## ðŸ§  Bias & Explainability Monitors (SageMaker Clarify)\n",
        "\n",
        "Set up **Model Bias** and **Model Explainability** monitors using SageMaker Clarify.\n",
        "- **Bias monitor** compares baseline/captured data across selected facets (e.g., `gender`, `age_bucket`) and label.\n",
        "- **Explainability monitor** computes **SHAP** at scheduled intervals for drift in feature attributions.\n",
        "\n",
        "> Provide minimal config in `CONFIG['clarify']` or this section will skip with guidance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121f92e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Clarify configuration block\n",
        "CONFIG.setdefault('clarify', {\n",
        "    'enable_bias': True,\n",
        "    'enable_explainability': True,\n",
        "    # Label column name in *ground truth* for bias; for model quality/bias (post-training) you need ground truth.\n",
        "    'label': os.environ.get('LABEL_COL', ''),\n",
        "    # Facet (protected) columns to analyze fairness across (must be present in baseline/capture/ground-truth)\n",
        "    'facet_cols': [c for c in os.environ.get('FACET_COLS', 'sex,age_bucket').split(',') if c],\n",
        "    # CSV headers for inference payloads (if capturing CSV)\n",
        "    'headers': [h for h in os.environ.get('CLARIFY_HEADERS', '').split(',') if h],\n",
        "    # Inference config (content-type etc.); adjust to your endpoint contract\n",
        "    'predictor_config': {\n",
        "        'content_type': os.environ.get('PRED_CONTENT_TYPE', 'text/csv'),  # or 'application/json'\n",
        "        'accept_type': os.environ.get('PRED_ACCEPT', 'application/json'),\n",
        "        'probability_attribute': os.environ.get('PRED_PROBA_ATTR', ''),   # e.g., 'probabilities' for JSON\n",
        "        'label_headers': [os.environ.get('LABEL_HEADER', '')] if os.environ.get('LABEL_HEADER') else []\n",
        "    },\n",
        "    # SHAP baseline sample size\n",
        "    'explainability': {\n",
        "        'shap_baseline_rows': int(os.environ.get('SHAP_BASELINE_ROWS', '200')),\n",
        "        'seed': SEED\n",
        "    }\n",
        "})\n",
        "CONFIG['clarify']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee2f493",
      "metadata": {},
      "source": [
        "\n",
        "### Create Model **Bias** Monitoring Schedule\n",
        "\n",
        "Requires:\n",
        "- `CONFIG['monitoring']['baseline_dataset_uri']` (CSV w/ header) **and/or** a capture dataset with ground-truth mapping\n",
        "- `CONFIG['clarify']['label']` and `facet_cols`\n",
        "- Ground truth CSV in `CONFIG['monitoring']['ground_truth_s3_uri']`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e35051",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sagemaker.model_monitor import ModelBiasMonitor\n",
        "\n",
        "clar = CONFIG['clarify']\n",
        "gt_uri = CONFIG['monitoring']['ground_truth_s3_uri']\n",
        "bias_sched_name = f\"bias-{CONFIG['deployment']['endpoint_name']}\"\n",
        "\n",
        "if CONFIG['monitoring']['enable'] and clar.get('enable_bias') and gt_uri and clar.get('label'):\n",
        "    try:\n",
        "        bias_mon = ModelBiasMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG['monitoring']['instance_type'],\n",
        "            volume_size_in_gb=CONFIG['monitoring']['volume_size_gb'],\n",
        "            max_runtime_in_seconds=CONFIG['monitoring']['max_runtime_seconds'],\n",
        "            sagemaker_session=sm_sess\n",
        "        )\n",
        "        # Configure bias (post-training bias with ground truth)\n",
        "        from sagemaker.clarify import BiasConfig, ModelPredictedLabelConfig, ModelConfig\n",
        "\n",
        "        predicted_label_cfg = ModelPredictedLabelConfig(\n",
        "            label=clar['label'], probability=clar['predictor_config'].get('probability_attribute') or None\n",
        "        )\n",
        "        bias_cfg = BiasConfig(\n",
        "            label_values_or_threshold=[1],           # adjust for your positive class\n",
        "            facet_name=clar['facet_cols'][0] if clar['facet_cols'] else None,\n",
        "            facet_values_or_threshold=None,\n",
        "        )\n",
        "        model_cfg = ModelConfig(\n",
        "            model_name=None,                         # not needed for endpoint monitoring\n",
        "            instance_type=CONFIG['monitoring']['instance_type'],\n",
        "            instance_count=1,\n",
        "            accept_type=clar['predictor_config']['accept_type'],\n",
        "            content_type=clar['predictor_config']['content_type'],\n",
        "            endpoint_name=CONFIG['deployment']['endpoint_name']\n",
        "        )\n",
        "        bias_mon.create_monitoring_schedule(\n",
        "            monitor_schedule_name=bias_sched_name,\n",
        "            endpoint_input=CONFIG['deployment']['endpoint_name'],\n",
        "            ground_truth_input=gt_uri,\n",
        "            bias_config=bias_cfg,\n",
        "            model_config=model_cfg,\n",
        "            model_predicted_label_config=predicted_label_cfg,\n",
        "            schedule_cron_expression=CONFIG['monitoring']['schedule_cron']\n",
        "        )\n",
        "        print(\"âœ… Bias monitoring schedule created:\", bias_sched_name)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Bias monitor setup skipped/failed:\", e)\n",
        "else:\n",
        "    print(\"Bias monitor disabled or missing label/ground truth.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a72b635",
      "metadata": {},
      "source": [
        "\n",
        "### Create Model **Explainability** (SHAP) Monitoring Schedule\n",
        "\n",
        "Computes SHAP values on a sample of captured requests to track changes in feature importance patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "609ed129",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sagemaker.model_monitor import ModelExplainabilityMonitor\n",
        "from sagemaker.clarify import SHAPConfig, ModelConfig\n",
        "\n",
        "expl_sched_name = f\"explain-{CONFIG['deployment']['endpoint_name']}\"\n",
        "\n",
        "if CONFIG['monitoring']['enable'] and clar.get('enable_explainability'):\n",
        "    try:\n",
        "        shap_cfg = SHAPConfig(\n",
        "            baseline=None,  # when None, Clarify samples baseline from captured data\n",
        "            num_samples=clar['explainability']['shap_baseline_rows'],\n",
        "            agg_method='mean_abs',  # default aggregation\n",
        "            use_logit=False,\n",
        "            seed=clar['explainability']['seed']\n",
        "        )\n",
        "        model_cfg = ModelConfig(\n",
        "            model_name=None,\n",
        "            instance_type=CONFIG['monitoring']['instance_type'],\n",
        "            instance_count=1,\n",
        "            accept_type=clar['predictor_config']['accept_type'],\n",
        "            content_type=clar['predictor_config']['content_type'],\n",
        "            endpoint_name=CONFIG['deployment']['endpoint_name']\n",
        "        )\n",
        "        exp_mon = ModelExplainabilityMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG['monitoring']['instance_type'],\n",
        "            volume_size_in_gb=CONFIG['monitoring']['volume_size_gb'],\n",
        "            max_runtime_in_seconds=CONFIG['monitoring']['max_runtime_seconds'],\n",
        "            sagemaker_session=sm_sess\n",
        "        )\n",
        "        exp_mon.create_monitoring_schedule(\n",
        "            monitor_schedule_name=expl_sched_name,\n",
        "            endpoint_input=CONFIG['deployment']['endpoint_name'],\n",
        "            explainability_config=shap_cfg,\n",
        "            model_config=model_cfg,\n",
        "            schedule_cron_expression=CONFIG['monitoring']['schedule_cron']\n",
        "        )\n",
        "        print(\"âœ… Explainability monitoring schedule created:\", expl_sched_name)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Explainability monitor setup skipped/failed:\", e)\n",
        "else:\n",
        "    print(\"Explainability monitor disabled.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0936b5e9",
      "metadata": {},
      "source": [
        "\n",
        "## ðŸ“Š Timeâ€‘based Drift: **Temporal Distribution** Checks (from Baseline or Capture)\n",
        "\n",
        "If your **training (baseline) data** contain a timestamp column, we compute rolling metrics by time window and compare to recent **captured** data using **PSI** and **KS** tests. Results are saved to artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4ecd71a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import re, json, gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "def compute_psi(expected: np.ndarray, actual: np.ndarray, buckets=10) -> float:\n",
        "    expected = expected[~np.isnan(expected)]\n",
        "    actual = actual[~np.isnan(actual)]\n",
        "    if len(expected) < 10 or len(actual) < 10: \n",
        "        return np.nan\n",
        "    # quantile bins off expected\n",
        "    quantiles = np.linspace(0, 1, buckets+1)\n",
        "    cuts = np.unique(np.quantile(expected, quantiles))\n",
        "    expected_bins = np.digitize(expected, cuts[1:-1], right=False)\n",
        "    actual_bins = np.digitize(actual, cuts[1:-1], right=False)\n",
        "    e_counts = np.bincount(expected_bins, minlength=len(cuts)-1).astype(float)\n",
        "    a_counts = np.bincount(actual_bins, minlength=len(cuts)-1).astype(float)\n",
        "    e_prop = np.clip(e_counts / e_counts.sum(), 1e-6, None)\n",
        "    a_prop = np.clip(a_counts / a_counts.sum(), 1e-6, None)\n",
        "    psi = np.sum((a_prop - e_prop) * np.log(a_prop / e_prop))\n",
        "    return float(psi)\n",
        "\n",
        "def coerce_timestamp(s: pd.Series):\n",
        "    for cand in ['timestamp','event_time','ts','date','datetime','created_at']:\n",
        "        if cand in s.index or cand == s.name:\n",
        "            pass\n",
        "    # generic\n",
        "    if pd.api.types.is_datetime64_any_dtype(s):\n",
        "        return s\n",
        "    try:\n",
        "        return pd.to_datetime(s, errors='coerce')\n",
        "    except Exception:\n",
        "        return pd.to_datetime(pd.Series([None]*len(s)))\n",
        "\n",
        "def load_baseline_df(uri: str) -> pd.DataFrame:\n",
        "    if not uri: \n",
        "        return pd.DataFrame()\n",
        "    if uri.endswith('.csv'):\n",
        "        return pd.read_csv(uri)\n",
        "    if uri.endswith('.parquet') or '.parquet' in uri:\n",
        "        return pd.read_parquet(uri)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# 1) Load baseline and infer timestamp column\n",
        "baseline_uri = CONFIG['monitoring'].get('baseline_dataset_uri') or CONFIG['monitoring'].get('fallback_parquet_uri')\n",
        "df_base = load_baseline_df(baseline_uri)\n",
        "ts_col = None\n",
        "if not df_base.empty:\n",
        "    for c in df_base.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df_base[c]) or re.search(r'(time|date)', c, re.I):\n",
        "            ts_col = c; break\n",
        "\n",
        "if ts_col:\n",
        "    df_base[ts_col] = pd.to_datetime(df_base[ts_col], errors='coerce')\n",
        "    base_recent = df_base.dropna(subset=[ts_col]).copy()\n",
        "    # Choose a few numeric columns for drift checks\n",
        "    num_cols = [c for c in base_recent.columns if c != ts_col and pd.api.types.is_numeric_dtype(base_recent[c])]\n",
        "    num_cols = num_cols[:10]  # cap for brevity\n",
        "    print('Timestamp column:', ts_col, '| Features checked:', num_cols)\n",
        "\n",
        "    # 2) Load a small sample from data capture to compare\n",
        "    cap_prefix = CONFIG['deployment']['data_capture']['s3_prefix'].replace('s3://','')\n",
        "    if '/' in cap_prefix:\n",
        "        bucket, prefix = cap_prefix.split('/', 1)\n",
        "    else:\n",
        "        bucket, prefix = cap_prefix, ''\n",
        "\n",
        "    import s3fs\n",
        "    fs = s3fs.S3FileSystem()\n",
        "    keys = fs.glob(f\"{bucket}/{prefix}/**/*.jsonl*\")[:5]  # take a few recent capture files\n",
        "    frames = []\n",
        "    for k in keys:\n",
        "        with fs.open(k, 'rb') as f:\n",
        "            raw = f.read()\n",
        "        try:\n",
        "            txt = raw.decode('utf-8')\n",
        "        except Exception:\n",
        "            txt = gzip.decompress(raw).decode('utf-8')\n",
        "        lines = [json.loads(l) for l in txt.strip().splitlines() if l.strip()]\n",
        "        # Try to parse 'request'/'response' bodies. Adjust according to your inference contract.\n",
        "        for rec in lines:\n",
        "            # CSV request example\n",
        "            req = rec.get('request',{}).get('body','')\n",
        "            if isinstance(req, str) and ',' in req and '\\n' not in req and len(clar.get('headers',[]))>0:\n",
        "                try:\n",
        "                    row = pd.read_csv(pd.compat.StringIO(req), header=None).iloc[0].to_dict()\n",
        "                    row = {clar['headers'][i]: v for i, v in enumerate(row.values()) if i < len(clar['headers'])}\n",
        "                    frames.append(pd.DataFrame([row]))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            # JSON request example\n",
        "            elif isinstance(req, (dict,list)):\n",
        "                frames.append(pd.json_normalize(req))\n",
        "    df_cap = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "    if not df_cap.empty and ts_col in df_cap.columns:\n",
        "        df_cap[ts_col] = pd.to_datetime(df_cap[ts_col], errors='coerce')\n",
        "\n",
        "    # 3) Time-window summaries & drift\n",
        "    results = []\n",
        "    if not base_recent.empty and not df_cap.empty:\n",
        "        # align columns\n",
        "        common = [c for c in num_cols if c in df_cap.columns]\n",
        "        for c in common:\n",
        "            base_vals = base_recent[c].astype(float).values\n",
        "            cap_vals = df_cap[c].astype(float).values\n",
        "            psi = compute_psi(base_vals, cap_vals, buckets=10)\n",
        "            ks = ks_2samp(base_vals[~np.isnan(base_vals)], cap_vals[~np.isnan(cap_vals)]).statistic\n",
        "            results.append({'feature': c, 'psi': psi, 'ks_stat': float(ks)})\n",
        "    drift_df = pd.DataFrame(results)\n",
        "    out = Path(ARTIFACT_DIR) / 'temporal_drift_summary.csv'\n",
        "    if not drift_df.empty:\n",
        "        drift_df.to_csv(out, index=False)\n",
        "        print('âœ… Temporal drift summary saved to:', out)\n",
        "    else:\n",
        "        print('â„¹ï¸ Temporal drift summary not computed (insufficient capture or no common columns).')\n",
        "else:\n",
        "    print('No timestamp column detected in baseline; temporal drift check skipped.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
