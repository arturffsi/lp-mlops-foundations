{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üì° Monitoring & Explainability ‚Äî **Exercise Notebook** (SageMaker)\n",
        "\n",
        "**Goal:** Configure **data capture**, **data quality**, **bias**, **explainability (SHAP)**, and **temporal drift** checks for a deployed endpoint.\n",
        "\n",
        "> Fill in every `# <- TODO ‚úèÔ∏è` to customize this notebook for your endpoint and datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß∞ Prereqs (run once per kernel if needed)\n",
        "- SageMaker Studio **Code Editor** kernel (Python 3.9+)\n",
        "- IAM role with permissions for: SageMaker, S3, CloudWatch Logs, Model Monitor\n",
        "- Deployed endpoint from your `deployment_example.ipynb`\n",
        "\n",
        "Uncomment if packages are missing:\n",
        "```python\n",
        "# %pip install sagemaker boto3 awswrangler s3fs pandas pyarrow numpy scipy matplotlib\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ôªÔ∏è Reproducibility & Artifact Folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, platform, random, hashlib\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "RUN_TS = \"20251016T225937Z\"\n",
        "RUN_ID = \"286bf1016f\"\n",
        "ARTIFACT_DIR = os.environ.get(\"ARTIFACT_DIR\", f\"artifacts/monitoring_run_{RUN_TS}_{RUN_ID}\")\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "env_info = {\n",
        "    \"python\": sys.version,\n",
        "    \"platform\": platform.platform(),\n",
        "    \"timestamp_utc\": RUN_TS,\n",
        "    \"seed\": SEED,\n",
        "}\n",
        "with open(os.path.join(ARTIFACT_DIR, \"env_info.json\"), \"w\") as f:\n",
        "    json.dump(env_info, f, indent=2)\n",
        "\n",
        "ARTIFACT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configuration (edit in ONE place)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "CONFIG = {\n",
        "    \"deployment\": {\n",
        "        \"endpoint_name\": os.environ.get(\"ENDPOINT_NAME\", \"your-endpoint-name\"),  # <- TODO ‚úèÔ∏è set existing endpoint\n",
        "        \"data_capture\": {\n",
        "            \"enable\": True,\n",
        "            \"sampling_percentage\": 50,  # <- TODO ‚úèÔ∏è 1-100\n",
        "            \"s3_prefix\": os.environ.get(\"CAPTURE_PREFIX\", \"s3://your-bucket/data-capture/your-endpoint/\"),  # <- TODO ‚úèÔ∏è\n",
        "            \"content_type\": \"text/csv\",   # <- TODO ‚úèÔ∏è match your inference contract ('application/json' or 'text/csv')\n",
        "            \"kms_key_id\": None\n",
        "        }\n",
        "    },\n",
        "    \"monitoring\": {\n",
        "        \"enable\": True,\n",
        "        \"instance_type\": \"ml.m5.large\",     # <- TODO ‚úèÔ∏è adjust instance size for monitors\n",
        "        \"volume_size_gb\": 20,\n",
        "        \"max_runtime_seconds\": 3600,\n",
        "        \"schedule_cron\": \"cron(0 2 * * ? *)\",  # <- TODO ‚úèÔ∏è daily at 02:00 UTC; change as needed\n",
        "        \"baseline_dataset_uri\": os.environ.get(\"BASELINE_URI\", \"\"),  # <- TODO ‚úèÔ∏è CSV with header (recommended)\n",
        "        \"fallback_parquet_uri\": os.environ.get(\"FALLBACK_PARQUET\", \"\"),  # optional parquet for baseline build\n",
        "        \"ground_truth_s3_uri\": os.environ.get(\"GROUND_TRUTH_URI\", \"\"),  # <- TODO ‚úèÔ∏è required for bias/model quality\n",
        "    },\n",
        "    \"clarify\": {\n",
        "        \"enable_bias\": True,                # <- TODO ‚úèÔ∏è if you have ground truth\n",
        "        \"enable_explainability\": True,\n",
        "        \"label\": os.environ.get(\"LABEL_COL\", \"\"),  # <- TODO ‚úèÔ∏è target/label column in ground-truth\n",
        "        \"facet_cols\": [c for c in os.environ.get(\"FACET_COLS\", \"sex,age_bucket\").split(\",\") if c],  # <- TODO ‚úèÔ∏è protected attrs\n",
        "        \"headers\": [h for h in os.environ.get(\"CLARIFY_HEADERS\", \"\").split(\",\") if h],  # <- TODO ‚úèÔ∏è request header names for CSV capture\n",
        "        \"predictor_config\": {\n",
        "            \"content_type\": os.environ.get(\"PRED_CONTENT_TYPE\", \"text/csv\"),\n",
        "            \"accept_type\": os.environ.get(\"PRED_ACCEPT\", \"application/json\"),\n",
        "            \"probability_attribute\": os.environ.get(\"PRED_PROBA_ATTR\", \"\"),  # e.g., 'probabilities' (JSON)\n",
        "            \"label_headers\": [os.environ.get(\"LABEL_HEADER\", \"\")] if os.environ.get(\"LABEL_HEADER\") else []\n",
        "        },\n",
        "        \"explainability\": {\n",
        "            \"shap_baseline_rows\": int(os.environ.get(\"SHAP_BASELINE_ROWS\", \"200\")),  # <- TODO ‚úèÔ∏è\n",
        "            \"seed\": 42\n",
        "        },\n",
        "        \"positive_class\": 1  # <- TODO ‚úèÔ∏è set positive class value for bias metrics\n",
        "    }\n",
        "}\n",
        "\n",
        "CONFIG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîå AWS & SageMaker Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3, sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "\n",
        "sm_sess = sagemaker.Session()\n",
        "region = boto3.Session().region_name\n",
        "try:\n",
        "    role = get_execution_role()\n",
        "except Exception:\n",
        "    role = os.environ.get(\"SAGEMAKER_ROLE_ARN\", \"\")\n",
        "print(\"Region:\", region)\n",
        "print(\"Role:\", role or \"<unset>\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß≤ Data Capture ‚Äî Enable/Update on Endpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.session import ProductionVariant\n",
        "from sagemaker.model_monitor import DataCaptureConfig\n",
        "from sagemaker.predictor import Predictor\n",
        "\n",
        "ep = CONFIG[\"deployment\"][\"endpoint_name\"]\n",
        "cap_cfg = CONFIG[\"deployment\"][\"data_capture\"]\n",
        "print(\"Endpoint:\", ep)\n",
        "\n",
        "if cap_cfg[\"enable\"]:\n",
        "    try:\n",
        "        dcc = DataCaptureConfig(\n",
        "            enable_capture=True,\n",
        "            sampling_percentage=cap_cfg[\"sampling_percentage\"],\n",
        "            destination_s3_uri=cap_cfg[\"s3_prefix\"],\n",
        "            kms_key_id=cap_cfg[\"kms_key_id\"],\n",
        "            capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
        "            csv_content_types=[\"text/csv\"] if cap_cfg[\"content_type\"] == \"text/csv\" else None,\n",
        "            json_content_types=[\"application/json\"] if cap_cfg[\"content_type\"] == \"application/json\" else None,\n",
        "        )\n",
        "        sm_client = sm_sess.sagemaker_client\n",
        "        # Update endpoint capture config\n",
        "        sm_client.update_endpoint_data_capture(\n",
        "            EndpointName=ep,\n",
        "            CaptureOptions=[{\"CaptureMode\": \"Input\"}, {\"CaptureMode\": \"Output\"}],\n",
        "            DestinationS3Uri=cap_cfg[\"s3_prefix\"],\n",
        "            EnableCapture=True,\n",
        "            InitialSamplingPercentage=cap_cfg[\"sampling_percentage\"],\n",
        "            KmsKeyId=cap_cfg[\"kms_key_id\"] or \"\"\n",
        "        )\n",
        "        print(\"‚úÖ Data capture updated for:\", ep)\n",
        "        print(\"   Destination:\", cap_cfg[\"s3_prefix\"])\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Could not update data capture:\", e)\n",
        "else:\n",
        "    print(\"Data capture disabled in CONFIG.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé Data Quality Monitor (DefaultModelMonitor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat\n",
        "\n",
        "if CONFIG[\"monitoring\"][\"enable\"] and cap_cfg[\"enable\"]:\n",
        "    try:\n",
        "        mon = DefaultModelMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG[\"monitoring\"][\"instance_type\"],\n",
        "            volume_size_in_gb=CONFIG[\"monitoring\"][\"volume_size_gb\"],\n",
        "            max_runtime_in_seconds=CONFIG[\"monitoring\"][\"max_runtime_seconds\"],\n",
        "            sagemaker_session=sm_sess\n",
        "        )\n",
        "        baseline = CONFIG[\"monitoring\"][\"baseline_dataset_uri\"]\n",
        "        if baseline:\n",
        "            mon.suggest_baseline(\n",
        "                baseline_dataset=baseline,\n",
        "                dataset_format=DatasetFormat.csv(header=True),\n",
        "                output_s3_uri=f\"s3://{sm_sess.default_bucket()}/monitoring/baseline/{RUN_TS}\",\n",
        "                wait=False\n",
        "            )\n",
        "            print(\"‚ÑπÔ∏è Baseline suggestion started:\", baseline)\n",
        "        schedule_name = f\"data-quality-{ep}\"\n",
        "        mon.create_monitoring_schedule(\n",
        "            monitor_schedule_name=schedule_name,\n",
        "            endpoint_input=ep,\n",
        "            statistics=None,    # Use suggested statistics path if you persisted one\n",
        "            constraints=None,   # Use suggested constraints path if you persisted one\n",
        "            schedule_cron_expression=CONFIG[\"monitoring\"][\"schedule_cron\"]\n",
        "        )\n",
        "        print(\"‚úÖ Data Quality monitoring schedule created:\", schedule_name)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Data Quality monitor setup skipped/failed:\", e)\n",
        "else:\n",
        "    print(\"Monitoring disabled or data capture off.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è Bias Monitor (Clarify)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model_monitor import ModelBiasMonitor\n",
        "from sagemaker.clarify import BiasConfig, ModelPredictedLabelConfig, ModelConfig\n",
        "\n",
        "clar = CONFIG[\"clarify\"]\n",
        "gt_uri = CONFIG[\"monitoring\"][\"ground_truth_s3_uri\"]\n",
        "bias_sched_name = f\"bias-{ep}\"\n",
        "\n",
        "if CONFIG[\"monitoring\"][\"enable\"] and clar.get(\"enable_bias\") and gt_uri and clar.get(\"label\"):\n",
        "    try:\n",
        "        bias_mon = ModelBiasMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG[\"monitoring\"][\"instance_type\"],\n",
        "            volume_size_in_gb=CONFIG[\"monitoring\"][\"volume_size_gb\"],\n",
        "            max_runtime_in_seconds=CONFIG[\"monitoring\"][\"max_runtime_seconds\"],\n",
        "            sagemaker_session=sm_sess\n",
        "        )\n",
        "\n",
        "        predicted_label_cfg = ModelPredictedLabelConfig(\n",
        "            label=clar[\"label\"],\n",
        "            probability=clar[\"predictor_config\"].get(\"probability_attribute\") or None\n",
        "        )\n",
        "        bias_cfg = BiasConfig(\n",
        "            label_values_or_threshold=[clar.get(\"positive_class\", 1)],  # <- TODO ‚úèÔ∏è set your positive class if not 1\n",
        "            facet_name=clar[\"facet_cols\"][0] if clar[\"facet_cols\"] else None,\n",
        "            facet_values_or_threshold=None,\n",
        "        )\n",
        "        model_cfg = ModelConfig(\n",
        "            model_name=None,\n",
        "            instance_type=CONFIG[\"monitoring\"][\"instance_type\"],\n",
        "            instance_count=1,\n",
        "            accept_type=clar[\"predictor_config\"][\"accept_type\"],\n",
        "            content_type=clar[\"predictor_config\"][\"content_type\"],\n",
        "            endpoint_name=ep\n",
        "        )\n",
        "        bias_mon.create_monitoring_schedule(\n",
        "            monitor_schedule_name=bias_sched_name,\n",
        "            endpoint_input=ep,\n",
        "            ground_truth_input=gt_uri,\n",
        "            bias_config=bias_cfg,\n",
        "            model_config=model_cfg,\n",
        "            model_predicted_label_config=predicted_label_cfg,\n",
        "            schedule_cron_expression=CONFIG[\"monitoring\"][\"schedule_cron\"]\n",
        "        )\n",
        "        print(\"‚úÖ Bias monitoring schedule created:\", bias_sched_name)\n",
        "        print(\"   Ground truth:\", gt_uri)\n",
        "        print(\"   Facets:\", clar[\"facet_cols\"])\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Bias monitor setup skipped/failed:\", e)\n",
        "else:\n",
        "    print(\"Bias monitor disabled or missing label/ground truth.  # <- TODO ‚úèÔ∏è set CONFIG['clarify'] and ground truth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© Explainability Monitor (SHAP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model_monitor import ModelExplainabilityMonitor\n",
        "from sagemaker.clarify import SHAPConfig\n",
        "\n",
        "expl_sched_name = f\"explain-{ep}\"\n",
        "\n",
        "if CONFIG[\"monitoring\"][\"enable\"] and clar.get(\"enable_explainability\"):\n",
        "    try:\n",
        "        shap_cfg = SHAPConfig(\n",
        "            baseline=None,\n",
        "            num_samples=clar[\"explainability\"][\"shap_baseline_rows\"],\n",
        "            agg_method=\"mean_abs\",\n",
        "            use_logit=False,\n",
        "            seed=clar[\"explainability\"][\"seed\"]\n",
        "        )\n",
        "        model_cfg = ModelConfig(\n",
        "            model_name=None,\n",
        "            instance_type=CONFIG[\"monitoring\"][\"instance_type\"],\n",
        "            instance_count=1,\n",
        "            accept_type=clar[\"predictor_config\"][\"accept_type\"],\n",
        "            content_type=clar[\"predictor_config\"][\"content_type\"],\n",
        "            endpoint_name=ep\n",
        "        )\n",
        "        exp_mon = ModelExplainabilityMonitor(\n",
        "            role=role,\n",
        "            instance_count=1,\n",
        "            instance_type=CONFIG[\"monitoring\"][\"instance_type\"],\n",
        "            volume_size_in_gb=CONFIG[\"monitoring\"][\"volume_size_gb\"],\n",
        "            max_runtime_in_seconds=CONFIG[\"monitoring\"][\"max_runtime_seconds\"],\n",
        "            sagemaker_session=sm_sess\n",
        "        )\n",
        "        exp_mon.create_monitoring_schedule(\n",
        "            monitor_schedule_name=expl_sched_name,\n",
        "            endpoint_input=ep,\n",
        "            explainability_config=shap_cfg,\n",
        "            model_config=model_cfg,\n",
        "            schedule_cron_expression=CONFIG[\"monitoring\"][\"schedule_cron\"]\n",
        "        )\n",
        "        print(\"‚úÖ Explainability monitoring schedule created:\", expl_sched_name)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Explainability monitor setup skipped/failed:\", e)\n",
        "else:\n",
        "    print(\"Explainability monitor disabled.  # <- TODO ‚úèÔ∏è set CONFIG['clarify']['enable_explainability']=True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è±Ô∏è Temporal Drift Checks (PSI & KS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, json, gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "def compute_psi(expected: np.ndarray, actual: np.ndarray, buckets=10) -> float:\n",
        "    expected = expected[~np.isnan(expected)]\n",
        "    actual = actual[~np.isnan(actual)]\n",
        "    if len(expected) < 10 or len(actual) < 10:\n",
        "        return np.nan\n",
        "    quantiles = np.linspace(0, 1, buckets+1)\n",
        "    cuts = np.unique(np.quantile(expected, quantiles))\n",
        "    expected_bins = np.digitize(expected, cuts[1:-1], right=False)\n",
        "    actual_bins = np.digitize(actual, cuts[1:-1], right=False)\n",
        "    e_counts = np.bincount(expected_bins, minlength=len(cuts)-1).astype(float)\n",
        "    a_counts = np.bincount(actual_bins, minlength=len(cuts)-1).astype(float)\n",
        "    e_prop = np.clip(e_counts / e_counts.sum(), 1e-6, None)\n",
        "    a_prop = np.clip(a_counts / a_counts.sum(), 1e-6, None)\n",
        "    psi = np.sum((a_prop - e_prop) * np.log(a_prop / e_prop))\n",
        "    return float(psi)\n",
        "\n",
        "def load_df(uri: str) -> pd.DataFrame:\n",
        "    if not uri:\n",
        "        return pd.DataFrame()\n",
        "    if uri.endswith(\".csv\"):\n",
        "        return pd.read_csv(uri)\n",
        "    if uri.endswith(\".parquet\") or \".parquet\" in uri:\n",
        "        return pd.read_parquet(uri)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# 1) Load baseline\n",
        "baseline_uri = CONFIG[\"monitoring\"].get(\"baseline_dataset_uri\") or CONFIG[\"monitoring\"].get(\"fallback_parquet_uri\")\n",
        "df_base = load_df(baseline_uri)\n",
        "ts_col = None\n",
        "if not df_base.empty:\n",
        "    # look for time-like columns\n",
        "    for c in df_base.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df_base[c]) or re.search(r\"(time|date)\", c, flags=re.I):\n",
        "            ts_col = c; break\n",
        "\n",
        "if ts_col:\n",
        "    df_base[ts_col] = pd.to_datetime(df_base[ts_col], errors=\"coerce\")\n",
        "    base_recent = df_base.dropna(subset=[ts_col]).copy()\n",
        "    num_cols = [c for c in base_recent.columns if c != ts_col and pd.api.types.is_numeric_dtype(base_recent[c])][:10]\n",
        "    print(\"Timestamp column:\", ts_col)\n",
        "    print(\"Numeric features to check:\", num_cols)\n",
        "\n",
        "    # 2) Load captured sample (JSONL in S3)\n",
        "    import s3fs\n",
        "    cap_prefix = CONFIG[\"deployment\"][\"data_capture\"][\"s3_prefix\"].replace(\"s3://\",\"\")\n",
        "    bucket, prefix = (cap_prefix.split(\"/\",1)+[\"\"])[:2]\n",
        "    fs = s3fs.S3FileSystem()\n",
        "    keys = fs.glob(f\"{bucket}/{prefix}/**/*.jsonl*\")[-5:]  # recent few\n",
        "    frames = []\n",
        "    for k in keys:\n",
        "        with fs.open(k, \"rb\") as f:\n",
        "            raw = f.read()\n",
        "        try:\n",
        "            txt = raw.decode(\"utf-8\")\n",
        "        except Exception:\n",
        "            txt = gzip.decompress(raw).decode(\"utf-8\")\n",
        "        lines = [json.loads(l) for l in txt.strip().splitlines() if l.strip()]\n",
        "        for rec in lines:\n",
        "            req = rec.get(\"request\",{}).get(\"body\",\"\")\n",
        "            if isinstance(req, str) and \",\" in req and \"\\n\" not in req and len(CONFIG[\"clarify\"][\"headers\"])>0:\n",
        "                try:\n",
        "                    row = pd.read_csv(pd.compat.StringIO(req), header=None).iloc[0].to_dict()\n",
        "                    row = {CONFIG[\"clarify\"][\"headers\"][i]: v for i, v in enumerate(row.values()) if i < len(CONFIG[\"clarify\"][\"headers\"])}\n",
        "                    frames.append(pd.DataFrame([row]))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            elif isinstance(req, (dict,list)):\n",
        "                frames.append(pd.json_normalize(req))\n",
        "    df_cap = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "    if not df_cap.empty and ts_col in df_cap.columns:\n",
        "        df_cap[ts_col] = pd.to_datetime(df_cap[ts_col], errors=\"coerce\")\n",
        "\n",
        "    # 3) Drift metrics\n",
        "    results = []\n",
        "    if not base_recent.empty and not df_cap.empty:\n",
        "        common = [c for c in num_cols if c in df_cap.columns]\n",
        "        for c in common:\n",
        "            base_vals = base_recent[c].astype(float).values\n",
        "            cap_vals = df_cap[c].astype(float).values\n",
        "            psi = compute_psi(base_vals, cap_vals, buckets=10)\n",
        "            ks = ks_2samp(base_vals[~np.isnan(base_vals)], cap_vals[~np.isnan(cap_vals)]).statistic\n",
        "            results.append({\"feature\": c, \"psi\": psi, \"ks_stat\": float(ks)})\n",
        "    drift_df = pd.DataFrame(results)\n",
        "    out = Path(ARTIFACT_DIR) / \"temporal_drift_summary.csv\"\n",
        "    if not drift_df.empty:\n",
        "        drift_df.to_csv(out, index=False)\n",
        "        print(\"‚úÖ Temporal drift summary saved:\", out)\n",
        "        display(drift_df.sort_values(\"psi\", ascending=False).head(10))\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Temporal drift summary not computed (insufficient capture or columns mismatch).\")\n",
        "else:\n",
        "    print(\"No timestamp column detected in baseline; temporal drift check skipped.  # <- TODO ‚úèÔ∏è ensure baseline has a time column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÖ Schedules & Latest Executions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sm = sm_sess.sagemaker_client\n",
        "\n",
        "def list_schedules(prefix):\n",
        "    res = sm.list_monitoring_schedules(MonitoringScheduleNameContains=prefix)\n",
        "    return [s[\"MonitoringScheduleName\"] for s in res.get(\"MonitoringScheduleSummaries\", [])]\n",
        "\n",
        "for pfx in [\"data-quality-\", \"bias-\", \"explain-\"]:\n",
        "    names = list_schedules(pfx)\n",
        "    print(pfx, names)\n",
        "\n",
        "def last_execution(name):\n",
        "    res = sm.list_monitoring_executions(MonitoringScheduleName=name, SortBy=\"ScheduledTime\", SortOrder=\"Descending\", MaxResults=1)\n",
        "    return res.get(\"MonitoringExecutionSummaries\", [None])[0]\n",
        "\n",
        "for name in sum([list_schedules(pfx) for pfx in [\"data-quality-\", \"bias-\", \"explain-\"]], []):\n",
        "    ex = last_execution(name)\n",
        "    if ex:\n",
        "        print(f\"{name}: {ex['MonitoringExecutionStatus']} @ {ex['ScheduledTime']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ (Optional) Pause / Delete Schedules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è Use with care\n",
        "# from sagemaker.model_monitor import MonitoringSchedule\n",
        "# for name in [*list_schedules(\"data-quality-\"), *list_schedules(\"bias-\"), *list_schedules(\"explain-\")]:\n",
        "#     try:\n",
        "#         MonitoringSchedule(sagemaker_session=sm_sess, monitoring_schedule_name=name).delete_monitoring_schedule()\n",
        "#         print(\"Deleted:\", name)\n",
        "#     except Exception as e:\n",
        "#         print(\"Failed to delete\", name, e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Student Checklist\n",
        "- [ ] Set **endpoint name** in `CONFIG['deployment']['endpoint_name']`  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] Choose **data capture** prefix and **content type**  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] Provide **baseline dataset** (CSV with header)  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] (Bias) Provide **ground-truth S3** and **label/positive class**  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] (Bias) List **facet columns** to analyze  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] (Explainability) Set **SHAP sample size**  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] (Temporal drift) Ensure baseline has a **timestamp** & headers mapping  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] Adjust **cron schedule** & **instance type** for monitors  # <- TODO ‚úèÔ∏è  \n",
        "- [ ] Run cells in order; inspect created schedules and outputs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}